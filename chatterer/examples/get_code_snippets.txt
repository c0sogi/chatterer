```C:\Users\cosogi\chatterer\chatterer\interactive.py
from typing import TYPE_CHECKING, Any, Callable, Iterable, Optional, TypeVar
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage
from langchain_core.runnables import RunnableConfig
from pydantic import BaseModel, Field
from rich.console import Console
from rich.panel import Panel
from rich.prompt import Prompt
from .language_model import Chatterer
from .utils.code_agent import DEFAULT_CODE_GENERATION_PROMPT, DEFAULT_FUNCTION_REFERENCE_PREFIX_PROMPT, DEFAULT_FUNCTION_REFERENCE_SEPARATOR, CodeExecutionResult, FunctionSignature, augment_prompt_for_toolcall, get_default_repl_tool
if TYPE_CHECKING:
    from langchain_experimental.tools.python.tool import PythonAstREPLTool
T = TypeVar('T', bound=BaseModel)

class ThinkBeforeSpeak(BaseModel):
    """
    Analyze the user's request and formulate an initial plan.
    This involves understanding the core task and breaking it down into logical steps.
    """
    task: str = Field(description="A concise summary of the user's overall goal or question.")
    plans: list[str] = Field(description="A sequence of actionable steps required to address the user's task. Each step should be clear and logical. Indicate if a step likely requires code execution.")

class IsToolCallNeeded(BaseModel):
    """
    Determine if executing Python code is the necessary *next* action.
    Carefully review the most recent messages, especially the last code execution output and review (if any).
    """
    is_tool_call_needed: bool = Field(description='Set to True ONLY if the *next logical step* requires executing Python code AND the previous step (if it involved code) did not already attempt this exact action and fail or produce unusable results. If the last code execution failed to achieve its goal (e.g., wrong data, error), set to False unless you plan to execute *different* code to overcome the previous issue. Set to False if the next step is reasoning, asking questions, or formulating a response based on existing information (including failed tool attempts).')

class ReviewOnToolcall(BaseModel):
    """
    Evaluate the outcome of the Python code execution and decide the subsequent action.
    Critically assess if the execution achieved the intended goal and if the output is usable.
    """
    review_on_code_execution: str = Field(description='A critical analysis of the code execution result. Did it succeed technically? Did it produce the *expected and usable* output according to the plan? Explicitly mention any errors, unexpected values (like incorrect dates), or unusable results.')
    next_action: str = Field(description="Describe the *immediate next logical action* based on the review. **If the execution failed or yielded unusable/unexpected results, DO NOT suggest repeating the exact same code execution.** Instead, propose a different action, such as: 'Try a different code approach to get the time', 'Inform the user about the environmental issue with the date', 'Ask the user to verify the result', or 'Abandon this approach and try something else'. If the execution was successful and useful, describe the next step in the plan (e.g., 'Use the retrieved time to formulate the answer').")
    is_task_completed: bool = Field(description='Set to True ONLY IF the *overall user task* is now fully addressed OR if the *only remaining action* based on the review is to generate the final response/answer directly to the user (this includes informing the user about an unresolvable issue found during execution). Set to False if further *productive* intermediate steps (like trying different code, processing data further, asking for input) are needed before the final response.')

class Think(BaseModel):
    """
    Engage in reasoning when code execution is not the immediate next step.
    This could involve synthesizing information, preparing the final answer, or identifying missing information.
    """
    my_thinking: str = Field(description='Explain your reasoning process. Why is code execution not needed now? What information are you using from the context? How are you planning to formulate the response or proceed?')
    next_action: str = Field(description="Describe the *immediate next action* resulting from this thinking process. Examples: 'Formulate the final answer to the user', 'Ask the user a clarifying question', 'Summarize the findings so far'.")
    is_task_completed: bool = Field(description="Set this to True IF AND ONLY IF the 'next_action' you just described involves generating the final response, explanation, or answer directly for the user, based on the reasoning in 'my_thinking'. If the 'next_action' involves asking the user a question, planning *further* internal steps (beyond formulating the immediate response), or indicates the task cannot be completed yet, set this to False. **If the plan is simply to tell the user the answer now, set this to True.**")

def interactive_shell(chatterer: Chatterer, system_instruction: BaseMessage | Iterable[BaseMessage]=[SystemMessage('You are an AI assistant capable of answering questions and executing Python code to help users solve tasks.')], repl_tool: Optional['PythonAstREPLTool']=None, prompt_for_code_invoke: Optional[str]=DEFAULT_CODE_GENERATION_PROMPT, additional_callables: Optional[Callable[..., object] | Iterable[Callable[..., object]]]=None, function_reference_prefix: Optional[str]=DEFAULT_FUNCTION_REFERENCE_PREFIX_PROMPT, function_reference_seperator: str=DEFAULT_FUNCTION_REFERENCE_SEPARATOR, config: Optional[RunnableConfig]=None, stop: Optional[list[str]]=None, **kwargs: Any) -> None:
    try:
        console = Console()
        AI_STYLE = 'bold bright_blue'
        EXECUTED_CODE_STYLE = 'bold bright_yellow'
        OUTPUT_STYLE = 'bold bright_cyan'
        THINKING_STYLE = 'dim white'
    except ImportError:
        raise ImportError('Rich library not found. Please install it: pip install rich')
    if repl_tool is None:
        repl_tool = get_default_repl_tool()

    def set_locals(**kwargs: object) -> None:
        """Set local variables for the REPL tool."""
        if repl_tool.locals is None:
            repl_tool.locals = {}
        for key, value in kwargs.items():
            repl_tool.locals[key] = value

    def respond(messages: list[BaseMessage]) -> str:
        response = ''
        with console.status('[bold yellow]AI is thinking...'):
            response_panel = Panel('', title='AI Response', style=AI_STYLE, border_style='blue')
            current_content = ''
            for chunk in chatterer.generate_stream(messages=messages):
                current_content += chunk
                response_panel.renderable = current_content
            response = current_content
        console.print(Panel(response, title='AI Response', style=AI_STYLE))
        return response.strip()

    def complete_task(think_before_speak: ThinkBeforeSpeak) -> None:
        task_info = f'[bold]Task:[/bold] {think_before_speak.task}\n[bold]Plans:[/bold]\n- ' + '\n- '.join(think_before_speak.plans)
        console.print(Panel(task_info, title='Task Analysis & Plan', style='magenta'))
        session_messages: list[BaseMessage] = [AIMessage(content=f"Okay, I understand the task. Here's my plan:\n- Task Summary: {think_before_speak.task}\n- Steps:\n" + '\n'.join((f'  - {p}' for p in think_before_speak.plans)))]
        while True:
            current_context = context + session_messages
            is_tool_call_needed: IsToolCallNeeded = chatterer(augment_prompt_for_toolcall(function_signatures=function_signatures, messages=current_context, prompt_for_code_invoke=prompt_for_code_invoke, function_reference_prefix=function_reference_prefix, function_reference_seperator=function_reference_seperator), IsToolCallNeeded, config=config, stop=stop, **kwargs)
            if is_tool_call_needed.is_tool_call_needed:
                set_locals(__context__=context, __session__=session_messages)
                code_execution: CodeExecutionResult = chatterer.exec(messages=current_context, repl_tool=repl_tool, prompt_for_code_invoke=prompt_for_code_invoke, function_signatures=function_signatures, function_reference_prefix=function_reference_prefix, function_reference_seperator=function_reference_seperator, config=config, stop=stop, **kwargs)
                code_block_display = f'[bold]Executed Code:[/bold]\n```python\n{code_execution.code}\n```\n\n[bold]Output:[/bold]\n{code_execution.output}'
                console.print(Panel(code_block_display, title='Code Execution', style=EXECUTED_CODE_STYLE, border_style='yellow'))
                tool_call_message = AIMessage(content=f'I executed the following code:\n```python\n{code_execution.code}\n```\n**Output:**\n{code_execution.output}')
                session_messages.append(tool_call_message)
                current_context_after_exec = context + session_messages
                decision = chatterer(augment_prompt_for_toolcall(function_signatures=function_signatures, messages=current_context_after_exec, prompt_for_code_invoke=prompt_for_code_invoke, function_reference_prefix=function_reference_prefix, function_reference_seperator=function_reference_seperator), ReviewOnToolcall, config=config, stop=stop, **kwargs)
                review_text = f'[bold]Review:[/bold] {decision.review_on_code_execution.strip()}\n[bold]Next Action:[/bold] {decision.next_action.strip()}'
                console.print(Panel(review_text, title='Execution Review', style=OUTPUT_STYLE, border_style='cyan'))
                review_message = AIMessage(content=f'**Review of Execution:** {decision.review_on_code_execution.strip()}\n**Next Action:** {decision.next_action.strip()}')
                session_messages.append(review_message)
                if decision.is_task_completed:
                    console.print(Panel('[bold green]Task Completed![/bold green]', title='Status', border_style='green'))
                    break
            else:
                current_context_before_think = context + session_messages
                decision = chatterer(augment_prompt_for_toolcall(function_signatures=function_signatures, messages=current_context_before_think, prompt_for_code_invoke=prompt_for_code_invoke, function_reference_prefix=function_reference_prefix, function_reference_seperator=function_reference_seperator), Think, config=config, stop=stop, **kwargs)
                thinking_text = f'[dim]Reasoning:[/dim] {decision.my_thinking.strip()}\n[bold]Next Action:[/bold] {decision.next_action.strip()}'
                console.print(Panel(thinking_text, title='AI Thought Process (No Code)', style=THINKING_STYLE, border_style='white'))
                thinking_message = AIMessage(content=f'**My Reasoning (without code execution):** {decision.my_thinking.strip()}\n**Next Action:** {decision.next_action.strip()}')
                session_messages.append(thinking_message)
                if decision.is_task_completed:
                    console.print(Panel('[bold green]Task Completed![/bold green]', title='Status', border_style='green'))
                    break
        final_response_messages = context + session_messages
        response: str = respond(final_response_messages)
        context.append(AIMessage(content=response))
    if additional_callables:
        if callable(additional_callables):
            additional_callables = [additional_callables]
        function_signatures: list[FunctionSignature] = FunctionSignature.from_callable(list(additional_callables))
    else:
        function_signatures: list[FunctionSignature] = []
    context: list[BaseMessage] = []
    if system_instruction:
        if isinstance(system_instruction, BaseMessage):
            context.append(system_instruction)
        elif isinstance(system_instruction, str):
            context.append(SystemMessage(content=system_instruction))
        else:
            context.extend(list(system_instruction))
    console.print(Panel("Welcome to the Interactive Chatterer Shell!\nType 'quit' or 'exit' to end the conversation.", title='Welcome', style=AI_STYLE, border_style='blue'))
    while True:
        try:
            user_input = Prompt.ask('[bold green]You[/bold green]')
        except EOFError:
            user_input = 'exit'
        if user_input.strip().lower() in ['quit', 'exit']:
            console.print(Panel('Goodbye!', title='Exit', style=AI_STYLE, border_style='blue'))
            break
        context.append(HumanMessage(content=user_input.strip()))
        try:
            initial_plan_decision = chatterer(augment_prompt_for_toolcall(function_signatures=function_signatures, messages=context, prompt_for_code_invoke=prompt_for_code_invoke, function_reference_prefix=function_reference_prefix, function_reference_seperator=function_reference_seperator), ThinkBeforeSpeak, config=config, stop=stop, **kwargs)
            complete_task(initial_plan_decision)
        except Exception as e:
            import traceback
            console.print(Panel(f'[bold red]An error occurred:[/bold red]\n{e}\n\n[yellow]Traceback:[/yellow]\n{traceback.format_exc()}', title='Error', border_style='red'))
if __name__ == '__main__':
    interactive_shell(chatterer=Chatterer.openai())
```

```C:\Users\cosogi\chatterer\chatterer\language_model.py
import re
from typing import TYPE_CHECKING, Any, AsyncIterator, Callable, Iterable, Iterator, Literal, Optional, Self, Sequence, Type, TypeAlias, TypeVar, overload
from langchain_core.language_models.base import LanguageModelInput
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.runnables.base import Runnable
from langchain_core.runnables.config import RunnableConfig
from langchain_core.utils.utils import secret_from_env
from pydantic import BaseModel, Field, SecretStr
from .messages import AIMessage, BaseMessage, HumanMessage, UsageMetadata
from .utils.code_agent import CodeExecutionResult, FunctionSignature, augment_prompt_for_toolcall
if TYPE_CHECKING:
    from instructor import Partial
    from langchain_experimental.tools.python.tool import PythonAstREPLTool
PydanticModelT = TypeVar('PydanticModelT', bound=BaseModel)
StructuredOutputType: TypeAlias = dict[object, object] | BaseModel
DEFAULT_IMAGE_DESCRIPTION_INSTRUCTION = 'Provide a detailed description of all visible elements in the image, summarizing key details in a few clear sentences.'
DEFAULT_CODE_GENERATION_PROMPT = 'You are utilizing a Python code execution tool now.\nYour goal is to generate Python code that solves the task efficiently and appends both the code and its output to your context memory.\n\nTo optimize tool efficiency, follow these guidelines:\n- Write concise, efficient code that directly serves the intended purpose.\n- Avoid unnecessary operations (e.g., excessive loops, recursion, or heavy computations).\n- Handle potential errors gracefully (e.g., using try-except blocks).\n\nReturn your response strictly in the following JSON format:\n{\n  "code": "<your_python_code_here>"\n}\n\n'
DEFAULT_FUNCTION_REFERENCE_PREFIX_PROMPT = "Below functions are included in global scope and can be used in your code.\nDo not try to redefine the function(s).\nYou don't have to force yourself to use these tools - use them only when you need to.\n"
DEFAULT_FUNCTION_REFERENCE_SEPARATOR = '\n---\n'
PYTHON_CODE_PATTERN: re.Pattern[str] = re.compile('```(?:python\\s*\\n)?(.*?)```', re.DOTALL)

class Chatterer(BaseModel):
    """Language model for generating text from a given input."""
    client: BaseChatModel
    structured_output_kwargs: dict[str, Any] = Field(default_factory=dict)

    @classmethod
    def from_provider(cls, provider_and_model: str, structured_output_kwargs: Optional[dict[str, object]]={'strict': True}) -> Self:
        backend, model = provider_and_model.split(':', 1)
        backends = cls.get_backends()
        if (func := backends.get(backend)):
            return func(model, structured_output_kwargs)
        else:
            raise ValueError(f"Unsupported provider: {backend}. Supported providers are: {', '.join(backends.keys())}.")

    @classmethod
    def get_backends(cls) -> dict[str, Callable[[str, Optional[dict[str, object]]], Self]]:
        return {'openai': cls.openai, 'anthropic': cls.anthropic, 'google': cls.google, 'ollama': cls.ollama, 'openrouter': cls.open_router, 'xai': cls.xai}

    @classmethod
    def openai(cls, model: str='gpt-4.1', structured_output_kwargs: Optional[dict[str, object]]={'strict': True}, api_key: Optional[str]=None, **kwargs: Any) -> Self:
        from langchain_openai import ChatOpenAI
        return cls(client=ChatOpenAI(model=model, api_key=_get_api_key(api_key=api_key, env_key='OPENAI_API_KEY', raise_if_none=False), **kwargs), structured_output_kwargs=structured_output_kwargs or {})

    @classmethod
    def anthropic(cls, model_name: str='claude-3-7-sonnet-20250219', structured_output_kwargs: Optional[dict[str, object]]=None, api_key: Optional[str]=None, **kwargs: Any) -> Self:
        from langchain_anthropic import ChatAnthropic
        return cls(client=ChatAnthropic(model_name=model_name, api_key=_get_api_key(api_key=api_key, env_key='ANTHROPIC_API_KEY', raise_if_none=True), **kwargs), structured_output_kwargs=structured_output_kwargs or {})

    @classmethod
    def google(cls, model: str='gemini-2.5-flash-preview-04-17', structured_output_kwargs: Optional[dict[str, object]]=None, api_key: Optional[str]=None, **kwargs: Any) -> Self:
        from langchain_google_genai import ChatGoogleGenerativeAI
        return cls(client=ChatGoogleGenerativeAI(model=model, api_key=_get_api_key(api_key=api_key, env_key='GOOGLE_API_KEY', raise_if_none=False), **kwargs), structured_output_kwargs=structured_output_kwargs or {})

    @classmethod
    def ollama(cls, model: str='deepseek-r1:1.5b', structured_output_kwargs: Optional[dict[str, object]]=None, **kwargs: Any) -> Self:
        from langchain_ollama import ChatOllama
        return cls(client=ChatOllama(model=model, **kwargs), structured_output_kwargs=structured_output_kwargs or {})

    @classmethod
    def open_router(cls, model: str='openrouter/quasar-alpha', structured_output_kwargs: Optional[dict[str, object]]=None, api_key: Optional[str]=None, **kwargs: Any) -> Self:
        from langchain_openai import ChatOpenAI
        return cls(client=ChatOpenAI(model=model, base_url='https://openrouter.ai/api/v1', api_key=_get_api_key(api_key=api_key, env_key='OPENROUTER_API_KEY', raise_if_none=False), **kwargs), structured_output_kwargs=structured_output_kwargs or {})

    @classmethod
    def xai(cls, model: str='grok-3-mini', structured_output_kwargs: Optional[dict[str, object]]=None, base_url: str='https://api.x.ai/v1', api_key: Optional[str]=None, **kwargs: Any) -> Self:
        from langchain_openai import ChatOpenAI
        return cls(client=ChatOpenAI(model=model, base_url=base_url, api_key=_get_api_key(api_key=api_key, env_key='XAI_API_KEY', raise_if_none=False), **kwargs), structured_output_kwargs=structured_output_kwargs or {})

    @property
    def invoke(self):
        return self.client.invoke

    @property
    def ainvoke(self):
        return self.client.ainvoke

    @property
    def stream(self):
        return self.client.stream

    @property
    def astream(self):
        return self.client.astream

    @property
    def bind_tools(self):
        return self.client.bind_tools

    def __getattr__(self, name: str) -> Any:
        return getattr(self.client, name)

    @overload
    def __call__(self, messages: LanguageModelInput, response_model: Type[PydanticModelT], config: Optional[RunnableConfig]=None, stop: Optional[list[str]]=None, **kwargs: Any) -> PydanticModelT:
        ...

    @overload
    def __call__(self, messages: LanguageModelInput, response_model: None=None, config: Optional[RunnableConfig]=None, stop: Optional[list[str]]=None, **kwargs: Any) -> str:
        ...

    def __call__(self, messages: LanguageModelInput, response_model: Optional[Type[PydanticModelT]]=None, config: Optional[RunnableConfig]=None, stop: Optional[list[str]]=None, **kwargs: Any) -> str | PydanticModelT:
        if response_model:
            return self.generate_pydantic(response_model, messages, config, stop, **kwargs)
        return self.client.invoke(input=messages, config=config, stop=stop, **kwargs).text()

    def generate(self, messages: LanguageModelInput, config: Optional[RunnableConfig]=None, stop: Optional[list[str]]=None, **kwargs: Any) -> str:
        return self.client.invoke(input=messages, config=config, stop=stop, **kwargs).text()

    async def agenerate(self, messages: LanguageModelInput, config: Optional[RunnableConfig]=None, stop: Optional[list[str]]=None, **kwargs: Any) -> str:
        return (await self.client.ainvoke(input=messages, config=config, stop=stop, **kwargs)).text()

    def generate_stream(self, messages: LanguageModelInput, config: Optional[RunnableConfig]=None, stop: Optional[list[str]]=None, **kwargs: Any) -> Iterator[str]:
        for chunk in self.client.stream(input=messages, config=config, stop=stop, **kwargs):
            yield chunk.text()

    async def agenerate_stream(self, messages: LanguageModelInput, config: Optional[RunnableConfig]=None, stop: Optional[list[str]]=None, **kwargs: Any) -> AsyncIterator[str]:
        async for chunk in self.client.astream(input=messages, config=config, stop=stop, **kwargs):
            yield chunk.text()

    def generate_pydantic(self, response_model: Type[PydanticModelT], messages: LanguageModelInput, config: Optional[RunnableConfig]=None, stop: Optional[list[str]]=None, **kwargs: Any) -> PydanticModelT:
        result: StructuredOutputType = _with_structured_output(client=self.client, response_model=response_model, structured_output_kwargs=self.structured_output_kwargs).invoke(input=messages, config=config, stop=stop, **kwargs)
        if isinstance(result, response_model):
            return result
        else:
            return response_model.model_validate(result)

    async def agenerate_pydantic(self, response_model: Type[PydanticModelT], messages: LanguageModelInput, config: Optional[RunnableConfig]=None, stop: Optional[list[str]]=None, **kwargs: Any) -> PydanticModelT:
        result: StructuredOutputType = await _with_structured_output(client=self.client, response_model=response_model, structured_output_kwargs=self.structured_output_kwargs).ainvoke(input=messages, config=config, stop=stop, **kwargs)
        if isinstance(result, response_model):
            return result
        else:
            return response_model.model_validate(result)

    def generate_pydantic_stream(self, response_model: Type[PydanticModelT], messages: LanguageModelInput, config: Optional[RunnableConfig]=None, stop: Optional[list[str]]=None, **kwargs: Any) -> Iterator[PydanticModelT]:
        try:
            import instructor
        except ImportError:
            raise ImportError('Please install `instructor` with `pip install instructor` to use this feature.')
        partial_response_model = instructor.Partial[response_model]
        for chunk in _with_structured_output(client=self.client, response_model=partial_response_model, structured_output_kwargs=self.structured_output_kwargs).stream(input=messages, config=config, stop=stop, **kwargs):
            yield response_model.model_validate(chunk)

    async def agenerate_pydantic_stream(self, response_model: Type[PydanticModelT], messages: LanguageModelInput, config: Optional[RunnableConfig]=None, stop: Optional[list[str]]=None, **kwargs: Any) -> AsyncIterator[PydanticModelT]:
        try:
            import instructor
        except ImportError:
            raise ImportError('Please install `instructor` with `pip install instructor` to use this feature.')
        partial_response_model = instructor.Partial[response_model]
        async for chunk in _with_structured_output(client=self.client, response_model=partial_response_model, structured_output_kwargs=self.structured_output_kwargs).astream(input=messages, config=config, stop=stop, **kwargs):
            yield response_model.model_validate(chunk)

    def describe_image(self, image_url: str, instruction: str=DEFAULT_IMAGE_DESCRIPTION_INSTRUCTION) -> str:
        """
        Create a detailed description of an image using the Vision Language Model.
        - image_url: Image URL to describe
        """
        return self.generate([HumanMessage(content=[{'type': 'text', 'text': instruction}, {'type': 'image_url', 'image_url': {'url': image_url}}])])

    async def adescribe_image(self, image_url: str, instruction: str=DEFAULT_IMAGE_DESCRIPTION_INSTRUCTION) -> str:
        """
        Create a detailed description of an image using the Vision Language Model asynchronously.
        - image_url: Image URL to describe
        """
        return await self.agenerate([HumanMessage(content=[{'type': 'text', 'text': instruction}, {'type': 'image_url', 'image_url': {'url': image_url}}])])

    def get_approximate_token_count(self, message: BaseMessage) -> int:
        return self.client.get_num_tokens_from_messages([message])

    def get_usage_metadata(self, message: BaseMessage) -> UsageMetadata:
        if isinstance(message, AIMessage):
            usage_metadata = message.usage_metadata
            if usage_metadata is not None:
                input_tokens = usage_metadata['input_tokens']
                output_tokens = usage_metadata['output_tokens']
                return {'input_tokens': input_tokens, 'output_tokens': output_tokens, 'total_tokens': input_tokens + output_tokens}
            else:
                approx_tokens = self.get_approximate_token_count(message)
                return {'input_tokens': 0, 'output_tokens': approx_tokens, 'total_tokens': approx_tokens}
        else:
            approx_tokens = self.get_approximate_token_count(message)
            return {'input_tokens': approx_tokens, 'output_tokens': 0, 'total_tokens': approx_tokens}

    def exec(self, messages: LanguageModelInput, repl_tool: Optional['PythonAstREPLTool']=None, prompt_for_code_invoke: Optional[str]=DEFAULT_CODE_GENERATION_PROMPT, function_signatures: Optional[FunctionSignature | Iterable[FunctionSignature]]=None, function_reference_prefix: Optional[str]=DEFAULT_FUNCTION_REFERENCE_PREFIX_PROMPT, function_reference_seperator: str=DEFAULT_FUNCTION_REFERENCE_SEPARATOR, config: Optional[RunnableConfig]=None, stop: Optional[list[str]]=None, **kwargs: Any) -> CodeExecutionResult:
        if not function_signatures:
            function_signatures = []
        elif isinstance(function_signatures, FunctionSignature):
            function_signatures = [function_signatures]
        messages = augment_prompt_for_toolcall(function_signatures=function_signatures, messages=messages, prompt_for_code_invoke=prompt_for_code_invoke, function_reference_prefix=function_reference_prefix, function_reference_seperator=function_reference_seperator)
        code_obj: PythonCodeToExecute = self.generate_pydantic(response_model=PythonCodeToExecute, messages=messages, config=config, stop=stop, **kwargs)
        return CodeExecutionResult.from_code(code=code_obj.code, config=config, repl_tool=repl_tool, function_signatures=function_signatures, **kwargs)

    @property
    def invoke_code_execution(self) -> Callable[..., CodeExecutionResult]:
        """Alias for exec method for backward compatibility."""
        return self.exec

    async def aexec(self, messages: LanguageModelInput, repl_tool: Optional['PythonAstREPLTool']=None, prompt_for_code_invoke: Optional[str]=DEFAULT_CODE_GENERATION_PROMPT, additional_callables: Optional[Callable[..., object] | Sequence[Callable[..., object]]]=None, function_reference_prefix: Optional[str]=DEFAULT_FUNCTION_REFERENCE_PREFIX_PROMPT, function_reference_seperator: str=DEFAULT_FUNCTION_REFERENCE_SEPARATOR, config: Optional[RunnableConfig]=None, stop: Optional[list[str]]=None, **kwargs: Any) -> CodeExecutionResult:
        function_signatures: list[FunctionSignature] = FunctionSignature.from_callable(additional_callables)
        messages = augment_prompt_for_toolcall(function_signatures=function_signatures, messages=messages, prompt_for_code_invoke=prompt_for_code_invoke, function_reference_prefix=function_reference_prefix, function_reference_seperator=function_reference_seperator)
        code_obj: PythonCodeToExecute = await self.agenerate_pydantic(response_model=PythonCodeToExecute, messages=messages, config=config, stop=stop, **kwargs)
        return await CodeExecutionResult.afrom_code(code=code_obj.code, config=config, repl_tool=repl_tool, function_signatures=function_signatures, **kwargs)

    @property
    def ainvoke_code_execution(self):
        """Alias for aexec method for backward compatibility."""
        return self.aexec

class PythonCodeToExecute(BaseModel):
    code: str = Field(description='Python code to execute')

    def model_post_init(self, context: object) -> None:
        super().model_post_init(context)
        codes: list[str] = []
        for match in PYTHON_CODE_PATTERN.finditer(self.code):
            codes.append(match.group(1))
        if codes:
            self.code = '\n'.join(codes)

def _with_structured_output(client: BaseChatModel, response_model: Type['PydanticModelT | Partial[PydanticModelT]'], structured_output_kwargs: dict[str, Any]) -> Runnable[LanguageModelInput, dict[object, object] | BaseModel]:
    return client.with_structured_output(schema=response_model, **structured_output_kwargs)

@overload
def _get_api_key(api_key: Optional[str], env_key: str, raise_if_none: Literal[True]) -> SecretStr:
    ...

@overload
def _get_api_key(api_key: Optional[str], env_key: str, raise_if_none: Literal[False]) -> Optional[SecretStr]:
    ...

def _get_api_key(api_key: Optional[str], env_key: str, raise_if_none: bool) -> Optional[SecretStr]:
    if api_key is None:
        api_key_found: SecretStr | None = secret_from_env(env_key, default=None)()
        if raise_if_none and api_key_found is None:
            raise ValueError(f'Did not find API key, please add an environment variable `{env_key}` which contains it, or pass api_key as a named parameter.')
        return api_key_found
    else:
        return SecretStr(api_key)
```

```C:\Users\cosogi\chatterer\chatterer\messages.py
from langchain_core.language_models.base import LanguageModelInput
from langchain_core.messages import AIMessage, BaseMessage, BaseMessageChunk, FunctionMessage, HumanMessage, SystemMessage
from langchain_core.messages.ai import UsageMetadata
__all__ = ['AIMessage', 'BaseMessage', 'HumanMessage', 'SystemMessage', 'FunctionMessage', 'BaseMessageChunk', 'UsageMetadata', 'LanguageModelInput']
```

```C:\Users\cosogi\chatterer\chatterer\__init__.py
from .interactive import interactive_shell
from .language_model import Chatterer
from .messages import AIMessage, BaseMessage, BaseMessageChunk, FunctionMessage, HumanMessage, LanguageModelInput, SystemMessage, UsageMetadata
from .strategies import AoTPipeline, AoTPrompter, AoTStrategy, BaseStrategy
from .tools import CodeSnippets, MarkdownLink, PdfToMarkdown, PlayWrightBot, PlaywrightLaunchOptions, PlaywrightOptions, PlaywrightPersistencyOptions, UpstageDocumentParseParser, acaption_markdown_images, anything_to_markdown, caption_markdown_images, citation_chunker, extract_text_from_pdf, get_default_html_to_markdown_options, get_default_playwright_launch_options, get_youtube_video_details, get_youtube_video_subtitle, html_to_markdown, open_pdf, pdf_to_text, pyscripts_to_snippets, render_pdf_as_image
from .utils import Base64Image, CodeExecutionResult, FunctionSignature, get_default_repl_tool, insert_callables_into_global
__all__ = ['BaseStrategy', 'Chatterer', 'AoTStrategy', 'AoTPipeline', 'AoTPrompter', 'html_to_markdown', 'anything_to_markdown', 'pdf_to_text', 'get_default_html_to_markdown_options', 'pyscripts_to_snippets', 'citation_chunker', 'BaseMessage', 'HumanMessage', 'SystemMessage', 'AIMessage', 'FunctionMessage', 'Base64Image', 'FunctionSignature', 'CodeExecutionResult', 'get_default_repl_tool', 'insert_callables_into_global', 'get_youtube_video_subtitle', 'get_youtube_video_details', 'interactive_shell', 'UpstageDocumentParseParser', 'BaseMessageChunk', 'CodeSnippets', 'LanguageModelInput', 'UsageMetadata', 'PlayWrightBot', 'PlaywrightLaunchOptions', 'PlaywrightOptions', 'PlaywrightPersistencyOptions', 'get_default_playwright_launch_options', 'acaption_markdown_images', 'caption_markdown_images', 'MarkdownLink', 'PdfToMarkdown', 'extract_text_from_pdf', 'open_pdf', 'render_pdf_as_image']
```

```C:\Users\cosogi\chatterer\chatterer\common_types\io.py
import os
from io import BufferedReader, BufferedWriter, BytesIO, StringIO, TextIOWrapper
from typing import TypeAlias
FileDescriptorOrPath: TypeAlias = int | str | bytes | os.PathLike[str] | os.PathLike[bytes]
BytesReadable: TypeAlias = BytesIO | BufferedReader
BytesWritable: TypeAlias = BytesIO | BufferedWriter
StringReadable: TypeAlias = StringIO | TextIOWrapper
StringWritable: TypeAlias = StringIO | TextIOWrapper
Readable: TypeAlias = BytesReadable | StringReadable
Writable: TypeAlias = BytesWritable | StringWritable
PathOrReadable: TypeAlias = FileDescriptorOrPath | Readable
```

```C:\Users\cosogi\chatterer\chatterer\common_types\__init__.py
from .io import BytesReadable, BytesWritable, FileDescriptorOrPath, PathOrReadable, Readable, StringReadable, StringWritable, Writable
__all__ = ['BytesReadable', 'BytesWritable', 'FileDescriptorOrPath', 'PathOrReadable', 'Readable', 'StringReadable', 'StringWritable', 'Writable']
```

```C:\Users\cosogi\chatterer\chatterer\examples\anything_to_markdown.py
def resolve_import_path_and_get_logger():
    import logging
    import sys
    if __name__ == '__main__' and '.' not in sys.path:
        sys.path.append('.')
    logger = logging.getLogger(__name__)
    return logger
logger = resolve_import_path_and_get_logger()
from pathlib import Path
from typing import Optional, TypedDict
import openai
from spargear import ArgumentSpec, BaseArguments
from chatterer import anything_to_markdown

class AnythingToMarkdownReturns(TypedDict):
    input: str
    output: Optional[str]
    out_text: str

class AnythingToMarkdownArguments(BaseArguments):
    """Command line arguments for converting various file types to markdown."""
    input: ArgumentSpec[str] = ArgumentSpec(['input'], help='Input file to convert to markdown')
    output: Optional[str] = None
    "Output path for the converted markdown file. If not provided, the input file's suffix is replaced with .md"
    model: Optional[str] = None
    'OpenAI Model to use for conversion'
    api_key: Optional[str] = None
    'API key for OpenAI API'
    base_url: Optional[str] = None
    'Base URL for OpenAI API'
    style_map: Optional[str] = None
    'Output style map'
    exiftool_path: Optional[str] = None
    '"Path to exiftool for metadata extraction'
    docintel_endpoint: Optional[str] = None
    'Document Intelligence API endpoint'
    prevent_save_file: bool = False
    'Prevent saving the converted file to disk.'
    encoding: str = 'utf-8'
    'Encoding for the output file.'

    def run(self) -> AnythingToMarkdownReturns:
        input = self.input.unwrap()
        if not self.prevent_save_file:
            if not self.output:
                output = Path(input).with_suffix('.md')
            else:
                output = Path(self.output)
        else:
            output = None
        if self.model:
            llm_client = openai.OpenAI(api_key=self.api_key, base_url=self.base_url)
            llm_model = self.model
        else:
            llm_client = None
            llm_model = None
        text: str = anything_to_markdown(input, llm_client=llm_client, llm_model=llm_model, style_map=self.style_map, exiftool_path=self.exiftool_path, docintel_endpoint=self.docintel_endpoint)
        if output:
            output.parent.mkdir(parents=True, exist_ok=True)
            output.write_text(text, encoding=self.encoding)
            logger.info(f'Converted `{input}` to markdown and saved to `{output}`.')
        else:
            logger.info(f'Converted `{input}` to markdown.')
        return {'input': input, 'output': str(output) if output is not None else None, 'out_text': text}
if __name__ == '__main__':
    AnythingToMarkdownArguments().run()
```

```C:\Users\cosogi\chatterer\chatterer\examples\login_with_playwright.py
def resolve_import_path():
    import sys
    from pathlib import Path
    parent = Path(__file__).resolve().parent.parent
    if str(parent) not in sys.path:
        sys.path.append(str(parent))
resolve_import_path()
import argparse
import json
from pathlib import Path
from chatterer import PlayWrightBot

def save_session(url: str, jsonpath: str | Path) -> None:
    """
    Launches a non-headless browser and navigates to the login_url.
    The user can manually log in, then press Enter in the console
    to store the current session state into a JSON file.
    """
    print(f'[*] Launching browser and navigating to {url} ... Please log in manually.')
    with PlayWrightBot(playwright_launch_options={'headless': False}) as bot:
        bot.get_page(url)
        print('[*] After completing the login in the browser, press Enter here to save the session.')
        input('    >> Press Enter when ready: ')
        context = bot.get_sync_browser()
        print(f'[*] Saving storage state to {jsonpath} ...')
        context.storage_state(path=jsonpath)
    print('[*] Done! Browser is now closed.')

def load_session(url: str, jsonpath: str | Path) -> None:
    """
    Loads the session state from the specified JSON file, then navigates
    to a protected_url that normally requires login. If the stored session
    is valid, it should open without re-entering credentials.
    """
    print(f'[*] Loading session from {jsonpath} and navigating to {url} ...')
    with open(jsonpath, 'r') as f:
        storage_state = json.load(f)
    with PlayWrightBot(playwright_launch_options={'headless': False}, playwright_persistency_options={'storage_state': storage_state}) as bot:
        bot.get_page(url)
        print("[*] Press Enter in the console when you're done checking the protected page.")
        input('    >> Press Enter to exit: ')
    print('[*] Done! Browser is now closed.')

def main() -> None:
    parser = argparse.ArgumentParser(description='A simple CLI tool for saving and using Playwright sessions via storage_state.')
    subparsers = parser.add_subparsers(dest='command', required=True)
    parser_save = subparsers.add_parser('save', help='Save a new session by manually logging in.')
    parser_save.add_argument('url', help='URL to manually log in.')
    parser_save.add_argument('--jsonpath', default=Path(__file__).with_suffix('.json'), help='Path to save the session JSON file.')
    parser_check = subparsers.add_parser('load', help='Use a saved session to view a protected page.')
    parser_check.add_argument('url', help='URL that requires login.')
    parser_check.add_argument('--jsonpath', default=Path(__file__).with_suffix('.json'), help='Path to the session JSON file to load.')
    args = parser.parse_args()
    if args.command == 'save':
        save_session(url=args.url, jsonpath=args.jsonpath)
    elif args.command == 'load':
        load_session(url=args.url, jsonpath=args.jsonpath)
if __name__ == '__main__':
    main()
```

```C:\Users\cosogi\chatterer\chatterer\examples\make_ppt.py
def resolve_import_path():
    """Adds the parent directory to sys.path for local module imports."""
    import sys
    from pathlib import Path
    parent = Path(__file__).resolve().parent.parent
    if str(parent) not in sys.path:
        sys.path.append(str(parent))
resolve_import_path()
import re
import sys
from pathlib import Path
from typing import NotRequired, TypedDict
from spargear import BaseArguments
from chatterer import BaseMessage, Chatterer, HumanMessage, SystemMessage
DEFAULT_ROLE_PROMPT = '# Prompt Content\n\n## Role Prompt: AI for Generating Presentation Slides and Scripts\n\nYou are a professional AI assistant that generates presentation slides and corresponding speech scripts based on user-provided content. Your primary task is to create a visually appealing and highly informative single presentation slide using HTML/CSS, along with a natural and well-structured speech script to accompany the slide. You must pay close attention to avoid layout breakage due to unnecessary whitespace or line breaks in text content or code blocks.\n\n[Core Features]\n\n1.  **Slide Generation (HTML/CSS):**\n    *   Analyze the user\'s input, extract key information, and structure it logically.\n    *   **Text Normalization:** Before inserting text into HTML elements, remove unintended line breaks and excessive spacing caused by OCR or formatting errors. Ensure that sentences and list items flow naturally. (e.g., "Easy gram\\nmar:" → "Easy grammar:")\n    *   Design the slide based on a 16:9 aspect ratio (1280x720 pixels). Use the `.slide` class to explicitly define this size.\n    *   Use Tailwind CSS utilities (via CDN: `<link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">`)—such as Grid, Flexbox, Padding, and Margin—to structure the layout. Prevent layout issues caused by long text lines or unwanted white space.\n    *   Enhance visuals with Font Awesome icons. Include the CDN in the `<head>` section (`<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css">`) and choose relevant icons appropriately.\n    *   Use Korean Google Fonts: `<link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@300;400;500;700&display=swap" rel="stylesheet">`\n    *   **Code Block Handling:**\n        *   Use `<pre><code>` tags for code samples. (e.g., `<div class="code-block bg-gray-800 text-white p-4 rounded mt-2"><pre><code>...code...</code></pre></div>`)\n        *   Ensure that code lines inside `<code>` tags start without leading spaces to avoid rendering issues caused by HTML indentation.\n        *   Optionally use `<span>` tags and classes like `comment`, `keyword`, or `string` for syntax highlighting.\n    *   Carefully choose color, spacing, and typography to create a clean, professional layout.\n    *   The generated HTML code must be fully self-contained, including all style information (Tailwind classes and optionally custom CSS via `<style>`) and CDN links. It should be enclosed in ```html ... ``` markdown blocks.\n\n2.  **Speech Script Generation:**\n    *   Write a clear and effective speech script to accompany the generated slide.\n    *   Use a professional yet natural tone to ensure easy understanding for the audience.\n    *   Explain each component of the slide (title, main points, code samples, etc.) in order, and include transitional or explanatory remarks where appropriate.\n    *   Keep the speech concise but informative, covering all key points.\n    *   Present the script clearly, outside of the HTML block.\n\n[Workflow]\n\n1.  Receive the user\'s input (topic or content for the presentation slide).\n2.  Identify the key message, normalize the text, and structure it to fit into a single slide.\n3.  Design and generate a 1280x720 slide using Tailwind CSS and Font Awesome. Pay special attention to code block formatting. Enclose the *complete* HTML for the slide within ```html ... ```.\n4.  Write a natural speech script based on the slide\'s content. Place the script *after* the HTML block.\n5.  Clearly separate the HTML code (within ```html ... ```) and the speech script.\n\n[Guidelines]\n\n*   Always aim to generate a single, self-contained slide.\n*   Maintain high quality and structure similar to provided examples, but optimize the layout and content according to the input.\n*   Pay close attention to whitespace and line break handling in both source HTML and rendered output, especially for plain text and code blocks.\n*   Always use CDN links for external libraries (Tailwind, Font Awesome, Google Fonts).\n*   Write the code with readability and reusability in mind.\n*   The speech script must be tightly aligned with the slide content.\n*   **Output Format:** First provide the complete HTML code block: ```html <!DOCTYPE html>...</html> ```. Immediately following the HTML block, provide the speech script as plain text.\n\nIncorrect Code Block HTML Example (To Avoid):\n\n```html\n<!-- This renders unintended indentation due to <pre> preserving HTML source indentation -->\n<div class="code-block">\n    <pre><code>\n        score = 85\n        if score >= 60:\n            print("Passed!")\n        else:\n            print("Failed.")\n    </code></pre>\n</div>\n````\n\nCorrect Code Block HTML Example (Recommended):\n\n```html\n<div class="code-block bg-gray-800 text-white p-4 rounded">\n<pre><code>score = 85\nif score >= 60:\n    print("Passed!") # Keep indentation within code block only\nelse:\n    print("Failed.")</code></pre>\n</div>\n```'
DEFAULT_JOB_PROMPT = "Objective: Analyze the content of the provided research material and create a detailed presentation slide plan in Markdown format. Save this plan to a file named 'plan.md'.\n\nDetailed Guidelines:\n\n1.  **Content Analysis:** Thoroughly analyze the provided research content to identify the core topic, main arguments, supporting data, conclusions, and other critical points.\n2.  **Slide Structure:** Based on the analysis, organize the overall slide structure with a logical flow (e.g., Introduction – Main Body – Conclusion, or Problem – Solution). Aim for a reasonable number of slides to cover the material effectively without being overwhelming.\n3.  **Slide-by-Slide Planning:** For each slide, define the content in detail. Each slide plan must include the following elements:\n    *   **Slide Number:** Use the format `# Slide N`.\n    *   **Topic:** Clearly state the central theme of the slide. (`Topic: ...`)\n    *   **Summary:** Provide a concise summary of the key message, main content, relevant data, and possible visual ideas (like charts, key points, code snippets if applicable) to be included in the slide. (`Summary: ...`)\n4.  **Output:** Generate *only* the Markdown content for the slide plan, following the format example below. Do not include any other explanatory text before or after the plan.\n\nOutput Format Example:\n\n```markdown\n# Slide 1\n\nTopic: Background and Rationale\nSummary: Explain why the [research topic] is important and what current issues it addresses. Briefly reference relevant statistics or previous studies to highlight the need for this research. Consider using a compelling opening statistic or image.\n\n# Slide 2\n\nTopic: Research Objectives and Questions\nSummary: Clearly state the specific objectives of the research (e.g., To investigate X, To develop Y). Present 1–2 concise research questions that the study aims to answer. Use bullet points for clarity.\n\n# Slide 3\n\nTopic: Methodology\nSummary: Describe the research methods used (e.g., surveys, literature review, experiments, data analysis techniques). Summarize the data collection process and key parameters. Maybe include a simple flowchart icon.\n\n... (and so on for all necessary slides) ...\n\n# Slide N\n\nTopic: Conclusion and Recommendations\nSummary: Summarize the key research findings and present the main conclusions clearly. Mention any significant limitations. Suggest directions for future research or practical recommendations based on the findings. End with a clear take-away message.\n```\n\n--- START OF RESEARCH MATERIAL ---\n{research_content}\n--- END OF RESEARCH MATERIAL ---\n\nNow, generate the slide plan based *only* on the provided research material.\n"
DEFAULT_ORGANIZATION_PROMPT = 'Objective: Create a final `presentation.html` file using impress.js (loaded via CDN) to structure the provided individual HTML slide contents into a cohesive presentation.\n\nGuidelines:\n\n1.  **Structure:** Use the standard impress.js HTML structure.\n    *   Include the impress.js library and its default CSS via CDN in the `<head>`.\n    *   Each slide\'s HTML content should be placed inside a `<div class="step">` element within the `<div id="impress">` container.\n    *   Assign appropriate `data-x`, `data-y`, `data-scale`, `data-rotate`, etc., attributes to the `step` divs to create a logical flow and visual transitions between slides. A simple linear flow (increasing `data-x` for each step) is acceptable, but feel free to add minor variations if it enhances the presentation flow.\n2.  **Content Integration:** Embed the *full* HTML content provided for each slide within its corresponding `<div class="step">`. Ensure the slide content (including its own `<head>` elements like Tailwind CSS links if they were generated per-slide) is correctly placed *inside* the `step` div. It might be better to consolidate CSS/Font links into the main HTML `<head>`. Let\'s aim to consolidate the CSS/Font links in the main `<head>` and put only the `<body>` content of each slide inside the `<div class="step">`.\n3.  **impress.js Initialization:** Include the `impress().init();` script at the end of the `<body>`.\n4.  **Output:** Generate *only* the complete HTML code for the `presentation.html` file. Do not include any other explanatory text.\n\n--- START OF SLIDE HTML CONTENTS ---\n\n{all_slides_html}\n\n--- END OF SLIDE HTML CONTENTS ---\n\nNow, generate the final `presentation.html` file using impress.js and the provided slide contents.\n'

class MakePptArguments(BaseArguments):
    """
    Arguments for the presentation generation process.
    """
    input_file: str = 'research.md'
    'Path to the input research file'
    plan_file: str = 'plan.md'
    'Path to the slide plan file'
    output_file: str = 'presentation.html'
    'Path to the output presentation file'
    slides_dir: str = 'slides'
    'Directory to save individual slide files'
    role_prompt: str = DEFAULT_ROLE_PROMPT
    'Role prompt for the AI assistant'
    job_prompt: str = DEFAULT_JOB_PROMPT
    'Job prompt for content analysis and slide planning'
    organization_prompt: str = DEFAULT_ORGANIZATION_PROMPT
    'Prompt for organizing slides into a presentation script'
    provider: str = 'openai:gpt-4.1'
    'Name of the language model to use (provider:model_name)'
    verbose: bool = True
    'Flag for verbose output during processing'

def parse_plan(plan_content: str) -> list[dict[str, str]]:
    """Parses the Markdown plan content into a list of slide dictionaries."""
    slides: list[dict[str, str]] = []
    slide_pattern = re.compile('# Slide (?P<number>\\d+)\\s*\\n+Topic:\\s*(?P<topic>.*?)\\s*\\n+Summary:\\s*(?P<summary>.*?)(?=\\n+# Slide|\\Z)', re.DOTALL | re.IGNORECASE)
    matches = slide_pattern.finditer(plan_content)
    for match in matches:
        slide_data = match.groupdict()
        slides.append({'number': slide_data['number'].strip(), 'topic': slide_data['topic'].strip(), 'summary': slide_data['summary'].strip()})
    return slides

def extract_html_script(response: str) -> tuple[str | None, str | None]:
    """Extracts HTML block and the following script from the AI response."""
    html_match = re.search('```html\\s*(.*?)\\s*```', response, re.DOTALL)
    if not html_match:
        html_match = re.search('<!DOCTYPE html>.*?</html>', response, re.DOTALL | re.IGNORECASE)
        if not html_match:
            return (None, response)
    html_content = html_match.group(1) if len(html_match.groups()) > 0 else html_match.group(0)
    html_content = html_content.strip()
    script_content = response[html_match.end():].strip()
    return (html_content, script_content if script_content else None)

def extract_slide_body(html_content: str) -> str:
    """Extracts the content within the <body> tags of a slide's HTML."""
    body_match = re.search('<body.*?>(.*?)</body>', html_content, re.DOTALL | re.IGNORECASE)
    if body_match:
        return body_match.group(1).strip()
    else:
        html_content = re.sub('<head>.*?</head>', '', html_content, flags=re.DOTALL | re.IGNORECASE)
        html_content = re.sub('<style>.*?</style>', '', html_content, flags=re.DOTALL | re.IGNORECASE)
        return html_content.strip()

class GeneratedSlide(TypedDict):
    number: str
    html: str
    script: NotRequired[str]

def run_presentation_agent(args: MakePptArguments):
    """Executes the presentation generation agent loop."""
    if args.verbose:
        print(f'Initializing Presentation Agent with model: {args.provider}')
        print(f'Input file: {args.input_file}')
        print(f'Plan file: {args.plan_file}')
        print(f'Output file: {args.output_file}')
        print(f'Slides dir: {args.slides_dir}')
    try:
        chatterer = Chatterer.from_provider(args.provider)
    except ValueError as e:
        print(f'Error initializing model: {e}')
        sys.exit(1)
    except Exception as e:
        print(f'An unexpected error occurred during model initialization: {e}')
        sys.exit(1)
    input_path = Path(args.input_file)
    if not input_path.is_file():
        print(f"Error: Input file not found at '{args.input_file}'")
        sys.exit(1)
    try:
        research_content = input_path.read_text(encoding='utf-8')
        if args.verbose:
            print(f'Successfully read input file: {args.input_file}')
    except Exception as e:
        print(f"Error reading input file '{args.input_file}': {e}")
        sys.exit(1)
    plan_path = Path(args.plan_file)
    if args.verbose:
        print('\n--- Generating Presentation Plan ---')
    plan_prompt = args.job_prompt.format(research_content=research_content)
    messages = [HumanMessage(content=plan_prompt)]
    try:
        if args.verbose:
            print('Sending request to LLM for plan generation...')
        plan_content = chatterer(messages)
        if args.verbose:
            print('Received plan from LLM.')
        plan_path.parent.mkdir(parents=True, exist_ok=True)
        plan_path.write_text(plan_content, encoding='utf-8')
        if args.verbose:
            print(f'Presentation plan saved to: {args.plan_file}')
    except Exception as e:
        print(f'Error during plan generation or saving: {e}')
        sys.exit(1)
    if args.verbose:
        print('\n--- Parsing Presentation Plan ---')
    try:
        slides_plan = parse_plan(plan_content)
        if not slides_plan:
            print('Error: Could not parse any slides from the generated plan. Check plan.md format.')
            sys.exit(1)
        if args.verbose:
            print(f'Successfully parsed {len(slides_plan)} slides from the plan.')
    except Exception as e:
        print(f"Error parsing plan file '{args.plan_file}': {e}")
        sys.exit(1)
    if args.verbose:
        print('\n--- Generating Individual Slides & Scripts ---')
    slides_dir_path = Path(args.slides_dir)
    slides_dir_path.mkdir(parents=True, exist_ok=True)
    generated_slides_data: list[GeneratedSlide] = []
    system_message = SystemMessage(content=args.role_prompt)
    for i, slide_info in enumerate(slides_plan):
        slide_num = slide_info.get('number', str(i + 1))
        topic = slide_info.get('topic', 'N/A')
        summary = slide_info.get('summary', 'N/A')
        if args.verbose:
            print(f'\nGenerating Slide {slide_num}: {topic}')
        slide_gen_prompt = f'Generate the HTML/CSS slide and the speech script for the following slide based on the plan:\n\nSlide Number: {slide_num}\nTopic: {topic}\nSummary/Content Instructions:\n{summary}\n\nRemember to follow all instructions in the role prompt, especially regarding HTML structure (1280x720 .slide class, Tailwind, Font Awesome, Google Fonts via CDN, correct code block formatting) and output format (```html ... ``` block followed by the script).\n'
        messages: list[BaseMessage] = [system_message, HumanMessage(content=slide_gen_prompt)]
        try:
            if args.verbose:
                print('Sending request to LLM for slide generation...')
            response = chatterer(messages)
            if args.verbose:
                print('Received slide content from LLM.')
            html_content, script_content = extract_html_script(response)
            if not html_content:
                print(f'Warning: Could not extract HTML block for slide {slide_num}. Skipping.')
                continue
            slide_html_path = slides_dir_path / f'slide_{slide_num}.html'
            slide_script_path = slides_dir_path / f'slide_{slide_num}_script.txt'
            slide_html_path.write_text(html_content, encoding='utf-8')
            if args.verbose:
                print(f'Saved HTML for slide {slide_num} to: {slide_html_path}')
            if script_content:
                slide_script_path.write_text(script_content, encoding='utf-8')
                if args.verbose:
                    print(f'Saved script for slide {slide_num} to: {slide_script_path}')
            elif args.verbose:
                print(f'Warning: No script content found for slide {slide_num}.')
            generated_slides_data.append({'number': slide_num, 'html': html_content, 'script': script_content or ''})
        except Exception as e:
            print(f'Error generating slide {slide_num}: {e}')
            sys.exit(1)
    if not generated_slides_data:
        print('Error: No slides were successfully generated.')
        sys.exit(1)
    if args.verbose:
        print('\n--- Organizing Slides into Final Presentation ---')
    output_path = Path(args.output_file)
    all_slides_body_html = ''
    for slide_data in generated_slides_data:
        slide_body = extract_slide_body(slide_data['html'])
        all_slides_body_html += f"<!-- Slide {slide_data['number']} Content -->\n"
        all_slides_body_html += f"<div class='step' data-x='{int(slide_data['number']) * 1500}' data-y='0' data-scale='1'>\n"
        all_slides_body_html += slide_body
        all_slides_body_html += '\n</div>\n\n'
    organization_formatted_prompt = args.organization_prompt.format(all_slides_html=all_slides_body_html)
    messages = [HumanMessage(content=organization_formatted_prompt)]
    try:
        if args.verbose:
            print('Sending request to LLM for final presentation generation...')
        final_presentation_html = chatterer(messages)
        final_presentation_html = re.sub('^```html\\s*', '', final_presentation_html, flags=re.IGNORECASE)
        final_presentation_html = re.sub('\\s*```$', '', final_presentation_html)
        if args.verbose:
            print('Received final presentation HTML from LLM.')
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text(final_presentation_html, encoding='utf-8')
        if args.verbose:
            print(f'Final presentation saved to: {args.output_file}')
    except Exception as e:
        print(f'Error during final presentation generation or saving: {e}')
        sys.exit(1)
    print('\n--- Presentation Generation Complete! ---')

def main(args: MakePptArguments) -> None:
    if not Path(args.input_file).exists():
        print(f'Creating dummy input file: {args.input_file}')
        Path(args.input_file).write_text('# Research Paper: The Future of AI Assistants\n\n## Introduction\nThe field of Artificial Intelligence (AI) has seen exponential growth. AI assistants are becoming integrated into daily life. This research explores future trends.\n\n## Current State\nCurrent assistants (Siri, Alexa, Google Assistant) primarily handle simple commands, Q&A, and basic task automation. They rely heavily on predefined scripts and cloud connectivity. NLP has improved significantly, but true contextual understanding remains a challenge.\n\n## Key Trends\n1.  **Proactive Assistance:** Assistants will anticipate user needs.\n2.  **Hyper-Personalization:** Tailoring responses and actions based on deep user understanding.\n3.  **Multimodal Interaction:** Seamlessly integrating voice, text, vision, and gestures.\n4.  **On-Device Processing:** Enhancing privacy and speed by reducing cloud dependency. Example: `model.run_locally()`\n5.  **Emotional Intelligence:** Recognizing and responding appropriately to user emotions.\n\n## Challenges\n*   Data Privacy and Security\n*   Algorithmic Bias\n*   Computational Cost\n*   Maintaining User Trust\n\n## Conclusion\nThe future of AI assistants points towards highly personalized, proactive, and emotionally intelligent companions. Overcoming challenges related to privacy and bias is crucial for widespread adoption. Python code example:\n```python\ndef greet(user):\n    # Simple greeting\n    print(f"Hello, {user}! How can I assist today?")\n\ngreet("Developer")\n```\n', encoding='utf-8')
    run_presentation_agent(args)
if __name__ == '__main__':
    main(MakePptArguments())
```

```C:\Users\cosogi\chatterer\chatterer\examples\pdf_to_markdown.py
def resolve_import_path():
    import sys
    from pathlib import Path
    parent = Path(__file__).resolve().parent.parent
    if str(parent) not in sys.path:
        sys.path.append(str(parent))
resolve_import_path()
import sys
from pathlib import Path
from spargear import ArgumentSpec, BaseArguments
from chatterer import Chatterer, PdfToMarkdown

class PdfToMarkdownArgs(BaseArguments):
    input: ArgumentSpec[Path] = ArgumentSpec(['input'], help='Path to the input PDF file or a directory containing PDF files.')
    chatterer: ArgumentSpec[Chatterer] = ArgumentSpec(['--chatterer'], default=None, help='Chatterer instance for communication.', type=Chatterer.from_provider, required=True)
    pages: ArgumentSpec[str] = ArgumentSpec(['--pages'], default=None, help="Page indices to convert (e.g., '1,3,5-9').")
    out: ArgumentSpec[Path] = ArgumentSpec(['--out'], default=None, help='Output path. For a file, path to the output markdown file. For a directory, output directory for .md files.')
    recursive: ArgumentSpec[bool] = ArgumentSpec(['--recursive'], action='store_true', default=False, help='If input is a directory, search for PDFs recursively.')

def parse_page_indices(pages_str: str) -> list[int] | None:
    if not pages_str:
        return None
    indices: set[int] = set()
    for part in pages_str.split(','):
        part = part.strip()
        if not part:
            continue
        if '-' in part:
            start_str, end_str = part.split('-', 1)
            start = int(start_str.strip())
            end = int(end_str.strip())
            if start > end:
                raise ValueError
            indices.update(range(start, end + 1))
        else:
            indices.add(int(part))
    if not indices:
        raise ValueError
    return sorted(indices)

def main(args: PdfToMarkdownArgs) -> list[dict[str, str]]:
    src_pinputth = args.input.unwrap().resolve()
    pages_arg = args.pages.value
    page_indices = parse_page_indices(pages_arg) if pages_arg else None
    pdf_files: list[Path] = []
    is_dir = False
    if src_pinputth.is_file():
        if src_pinputth.suffix.lower() != '.pdf':
            sys.exit(1)
        pdf_files.append(src_pinputth)
    elif src_pinputth.is_dir():
        is_dir = True
        pattern = '*.pdf'
        pdf_files = sorted([f for f in (src_pinputth.rglob(pattern) if args.recursive.value else src_pinputth.glob(pattern)) if f.is_file()])
        if not pdf_files:
            sys.exit(0)
    else:
        sys.exit(1)
    out_base = args.out.value if args.out.value else src_pinputth if is_dir else src_pinputth.with_suffix('.md')
    if not out_base.exists():
        out_base.mkdir(parents=True, exist_ok=True) if is_dir else out_base.parent.mkdir(parents=True, exist_ok=True)
    converter = PdfToMarkdown(chatterer=args.chatterer.unwrap())
    results: list[dict[str, str]] = []
    for pdf in pdf_files:
        output = out_base / (pdf.stem + '.md') if is_dir else out_base
        md = converter.convert(str(pdf), page_indices)
        output.parent.mkdir(parents=True, exist_ok=True)
        output.write_text(md, encoding='utf-8')
        results.append({'input': pdf.as_posix(), 'output': output.as_posix(), 'result': md})
    print(f'[*] Converted {len(pdf_files)} PDF(s) to markdown and saved to `{out_base}`.')
    return results
if __name__ == '__main__':
    main(PdfToMarkdownArgs())
```

```C:\Users\cosogi\chatterer\chatterer\examples\pdf_to_text.py
def resolve_import_path():
    import sys
    from pathlib import Path
    parent = Path(__file__).resolve().parent.parent
    if str(parent) not in sys.path:
        sys.path.append(str(parent))
resolve_import_path()
import sys
from pathlib import Path
from spargear import ArgumentSpec, BaseArguments
from chatterer.tools.convert_to_text import pdf_to_text

class PdfToTextArgs(BaseArguments):
    input: ArgumentSpec[Path] = ArgumentSpec(['input'], help='Path to the PDF file.')
    pages: ArgumentSpec[str] = ArgumentSpec(['--pages'], default=None, help="Page indices to extract, e.g. '1,3,5-9'.")
    out: ArgumentSpec[Path] = ArgumentSpec(['--out'], default=None, help='Output file path.')

def parse_page_indices(pages_str: str) -> list[int]:
    indices: set[int] = set()
    for part in pages_str.split(','):
        part = part.strip()
        if '-' in part:
            start_str, end_str = part.split('-', 1)
            start = int(start_str)
            end = int(end_str)
            if start > end:
                raise ValueError
            indices.update(range(start, end + 1))
        else:
            indices.add(int(part))
    return sorted(indices)

def main(args: PdfToTextArgs) -> None:
    input = args.input.unwrap().resolve()
    out = args.out.value or input.with_suffix('.txt')
    if not input.is_file():
        sys.exit(1)
    out.write_text(pdf_to_text(input, parse_page_indices(pages_arg) if (pages_arg := args.pages.value) else None), encoding='utf-8')
    print(f'[*] Extracted text from `{input}` to `{out}`')
if __name__ == '__main__':
    main(PdfToTextArgs())
```

```C:\Users\cosogi\chatterer\chatterer\examples\transcription_api.py
from io import BytesIO
from pathlib import Path
from typing import cast
from openai import OpenAI
from pydub import AudioSegment
from spargear import ArgumentSpec, BaseArguments
MAX_CHUNK_DURATION = 600

class TranscriptionArguments(BaseArguments):
    audio_file = ArgumentSpec(['audio-file'], type=Path, help='The audio file to transcribe.')
    output_path = ArgumentSpec(['--output-path'], type=Path, default=None, help='Path to save the transcription output.')
    model: ArgumentSpec[str] = ArgumentSpec(['--model'], default='gpt-4o-transcribe', help='The model to use for transcription.')
    api_key: ArgumentSpec[str] = ArgumentSpec(['--api-key'], default=None, help='The API key for authentication.')
    base_url: ArgumentSpec[str] = ArgumentSpec(['--base-url'], default='https://api.openai.com/v1', help='The base URL for the API.')

def load_audio_segment(file_path: Path) -> AudioSegment:
    """
    Load an audio file as an AudioSegment. Convert to mp3 format in-memory if needed.
    """
    ext = file_path.suffix.lower()[1:]
    audio = AudioSegment.from_file(file_path.as_posix(), format=ext if ext != 'mp3' else None)
    if ext != 'mp3':
        buffer = BytesIO()
        audio.export(buffer, format='mp3')
        buffer.seek(0)
        audio = AudioSegment.from_file(buffer, format='mp3')
    return audio

def split_audio(audio: AudioSegment, max_duration_s: int) -> list[AudioSegment]:
    """
    Split the AudioSegment into chunks no longer than max_duration_s seconds.
    """
    chunk_length_ms = (max_duration_s - 1) * 1000
    duration_ms = len(audio)
    segments: list[AudioSegment] = []
    segment_idx: int = 0
    for start_ms in range(0, duration_ms, chunk_length_ms):
        end_ms = min(start_ms + chunk_length_ms, duration_ms)
        segment = cast(AudioSegment, audio[start_ms:end_ms])
        segments.append(segment)
        segment_idx += 1
    return segments

def transcribe_segment(segment: AudioSegment, client: OpenAI, model: str) -> str:
    """
    Transcribe a single AudioSegment chunk and return its text.
    """
    buffer = BytesIO()
    segment.export(buffer, format='mp3')
    buffer.seek(0)
    mp3_bytes = buffer.read()
    response = client.audio.transcriptions.create(model=model, prompt='Transcribe whole text from audio.', file=('audio.mp3', mp3_bytes), response_format='text', stream=True)
    for res in response:
        if res.type == 'transcript.text.delta':
            print(res.delta, end='', flush=True)
        if res.type == 'transcript.text.done':
            print()
            return res.text
    else:
        raise RuntimeError('No transcription result found.')

def main(args: TranscriptionArguments) -> None:
    audio_path = args.audio_file.unwrap()
    model = args.model.unwrap()
    client = OpenAI(api_key=args.api_key.value, base_url=args.base_url.value)
    audio = load_audio_segment(audio_path)
    segments = split_audio(audio, MAX_CHUNK_DURATION)
    print(f'[i] Audio duration: {len(audio) / 1000:.1f}s; splitting into {len(segments)} segment(s)')
    transcripts: list[str] = []
    for idx, seg in enumerate(segments, start=1):
        print(f'[i] Transcribing segment {idx}/{len(segments)}...')
        transcripts.append(transcribe_segment(seg, client, model))
    full_transcript = '\n\n'.join(transcripts)
    output_path: Path = args.output_path.value or audio_path.with_suffix('.txt')
    output_path.write_text(full_transcript, encoding='utf-8')
    print(f'[✓] Transcription saved to: {output_path}')
if __name__ == '__main__':
    main(TranscriptionArguments())
```

```C:\Users\cosogi\chatterer\chatterer\examples\upstage_parser.py
def resolve_import_path():
    import sys
    from pathlib import Path
    parent = Path(__file__).resolve().parent.parent
    if str(parent) not in sys.path:
        sys.path.append(str(parent))
resolve_import_path()
from pathlib import Path
from chatterer import Chatterer, UpstageDocumentParseParser
from chatterer.tools.upstage_document_parser import DEFAULT_IMAGE_DIR, DOCUMENT_PARSE_BASE_URL, DOCUMENT_PARSE_DEFAULT_MODEL, OCR, Category, OutputFormat, SplitType
from langchain_core.documents.base import Blob
from spargear import ArgumentSpec, BaseArguments

class Arguments(BaseArguments):
    input: ArgumentSpec[Path] = ArgumentSpec(['input'], help='Path to the input file.')
    out: ArgumentSpec[Path] = ArgumentSpec(['--out'], default=None, help='Output file path.')
    api_key: ArgumentSpec[str] = ArgumentSpec(['--api-key'], default=None, help='API key for the Upstage API.')
    base_url: ArgumentSpec[str] = ArgumentSpec(['--base-url'], default=DOCUMENT_PARSE_BASE_URL, help='Base URL for the Upstage API.')
    model: ArgumentSpec[str] = ArgumentSpec(['--model'], default=DOCUMENT_PARSE_DEFAULT_MODEL, help='Model to use for parsing.')
    split: ArgumentSpec[SplitType] = ArgumentSpec(['--split'], default='none', help='Split type for parsing.')
    ocr: ArgumentSpec[OCR] = ArgumentSpec(['--ocr'], default='auto', help='OCR type for parsing.')
    output_format: ArgumentSpec[OutputFormat] = ArgumentSpec(['--output-format'], default='markdown', help='Output format.')
    coordinates: ArgumentSpec[bool] = ArgumentSpec(['--coordinates'], action='store_true', help='Include coordinates.')
    base64_encoding: ArgumentSpec[list[Category]] = ArgumentSpec(['--base64-encoding'], default=['figure'], help='Base64 encoding for specific categories.')
    image_description_instruction: ArgumentSpec[str] = ArgumentSpec(['--image-description-instruction'], default='Describe the image in detail.', help='Instruction for image description.')
    image_dir: ArgumentSpec[str] = ArgumentSpec(['--image-dir'], default=DEFAULT_IMAGE_DIR, help='Directory for image paths.')
    chatterer: ArgumentSpec[Chatterer] = ArgumentSpec(['--chatterer'], default=None, help='Chatterer instance for communication.', type=Chatterer.from_provider)
if __name__ == '__main__':
    Arguments.load()
    input = Arguments.input.unwrap().resolve()
    out = Arguments.out.value or input.with_suffix('.md')
    parser = UpstageDocumentParseParser(api_key=Arguments.api_key.value, base_url=Arguments.base_url.unwrap(), model=Arguments.model.unwrap(), split=Arguments.split.unwrap(), ocr=Arguments.ocr.unwrap(), output_format=Arguments.output_format.unwrap(), coordinates=Arguments.coordinates.unwrap(), base64_encoding=Arguments.base64_encoding.unwrap(), image_description_instruction=Arguments.image_description_instruction.unwrap(), image_dir=Arguments.image_dir.value, chatterer=Arguments.chatterer.value)
    docs = parser.parse(Blob.from_path(input))
    if (image_dir := Arguments.image_dir.value):
        for path, image in parser.image_data.items():
            (path := Path(path)).parent.mkdir(parents=True, exist_ok=True)
            path.write_bytes(image)
            print(f'[*] Saved image to `{path}`')
    markdown: str = '\n\n'.join((f'<!--- page {i} -->\n{doc.page_content}' for i, doc in enumerate(docs, 1)))
    out.write_text(markdown, encoding='utf-8')
    print(f'[*] Parsed `{input}` to `{out}`')
```

```C:\Users\cosogi\chatterer\chatterer\examples\webpage_to_markdown.py
def resolve_import_path():
    import sys
    from pathlib import Path
    parent = Path(__file__).resolve().parent.parent
    if str(parent) not in sys.path:
        sys.path.append(str(parent))
resolve_import_path()
import asyncio
import sys
from pathlib import Path
from typing import Literal, Optional
from chatterer import Chatterer, MarkdownLink, PlayWrightBot
from spargear import ArgumentSpec, BaseArguments

class WebpageToMarkdownArgs(BaseArguments):
    url: ArgumentSpec[str] = ArgumentSpec(['url'], help='The URL to crawl.')
    out: ArgumentSpec[Path] = ArgumentSpec(['--out'], default=None, help='The output file path.')
    sync: ArgumentSpec[bool] = ArgumentSpec(['--sync'], action='store_true', default=False, help='Run the script synchronously.')
    llm: ArgumentSpec[str] = ArgumentSpec(['--llm'], default=None, help='The LLM backend and model to use for filtering the markdown.')
    engine: ArgumentSpec[Literal['firefox', 'chromium', 'webkit']] = ArgumentSpec(['--engine'], default='firefox', help='The browser engine to use.', choices=['firefox', 'chromium', 'webkit'])

def truncate_string(s: str) -> str:
    return s[:50] + '...' if len(s) > 50 else s

def main_sync(url: str, out: Path, ch: Optional[Chatterer], engine: Literal['chromium', 'firefox', 'webkit']) -> None:
    with PlayWrightBot(chatterer=ch, engine=engine) as bot:
        md = bot.url_to_md(url.strip())
        out.write_text(md, encoding='utf-8')
        if ch:
            md_llm = bot.url_to_md_with_llm(url.strip())
            out.write_text(md_llm, encoding='utf-8')
        links = MarkdownLink.from_markdown(md, referer_url=url)
        for link in links:
            if link.type == 'link':
                print(f'- [{truncate_string(link.url)}] {truncate_string(link.inline_text)} ({truncate_string(link.inline_title)})')
            elif link.type == 'image':
                print(f'- ![{truncate_string(link.url)}] ({truncate_string(link.inline_text)})')

async def main_async(url: str, out: Path, ch: Optional[Chatterer], engine: Literal['chromium', 'firefox', 'webkit']) -> None:
    async with PlayWrightBot(chatterer=ch, engine=engine) as bot:
        md = await bot.aurl_to_md(url.strip())
        out.write_text(md, encoding='utf-8')
        if ch:
            md_llm = await bot.aurl_to_md_with_llm(url.strip())
            out.write_text(md_llm, encoding='utf-8')
        links = MarkdownLink.from_markdown(md, referer_url=url)
        for link in links:
            if link.type == 'link':
                print(f'- [{truncate_string(link.url)}] {truncate_string(link.inline_text)} ({truncate_string(link.inline_title)})')
            elif link.type == 'image':
                print(f'- ![{truncate_string(link.url)}] ({truncate_string(link.inline_text)})')

def main() -> None:
    WebpageToMarkdownArgs.load()
    output = WebpageToMarkdownArgs.out.value or Path(__file__).with_suffix('.md')
    ch = Chatterer.from_provider(WebpageToMarkdownArgs.llm.value) if WebpageToMarkdownArgs.llm.value else None
    if WebpageToMarkdownArgs.sync.value:
        main_sync(WebpageToMarkdownArgs.url.unwrap(), output, ch, WebpageToMarkdownArgs.engine.unwrap())
    else:
        asyncio.run(main_async(WebpageToMarkdownArgs.url.unwrap(), output, ch, WebpageToMarkdownArgs.engine.unwrap()))
    sys.exit(0)
if __name__ == '__main__':
    main()
```

```C:\Users\cosogi\chatterer\chatterer\strategies\atom_of_thoughts.py
from __future__ import annotations
import asyncio
import logging
from dataclasses import dataclass, field
from enum import StrEnum
from typing import Optional, Type, TypeVar
from pydantic import BaseModel, Field, ValidationError
from ..language_model import Chatterer, LanguageModelInput
from ..messages import AIMessage, BaseMessage, HumanMessage
from .base import BaseStrategy
QA_TEMPLATE = 'Q: {question}\nA: {answer}'
MAX_DEPTH_REACHED = 'Max depth reached in recursive decomposition.'
UNKNOWN = 'Unknown'

class SubQuestionNode(BaseModel):
    """A single sub-question node in a decomposition tree."""
    question: str = Field(description='A sub-question string that arises from decomposition.')
    answer: Optional[str] = Field(description='Answer for this sub-question, if resolved.')
    depend: list[int] = Field(description='Indices of sub-questions that this node depends on.')

class RecursiveDecomposeResponse(BaseModel):
    """The result of a recursive decomposition step."""
    thought: str = Field(description='Reasoning about decomposition.')
    final_answer: str = Field(description='Best answer to the main question.')
    sub_questions: list[SubQuestionNode] = Field(description='Root-level sub-questions.')

class ContractQuestionResponse(BaseModel):
    """The result of contracting (simplifying) a question."""
    thought: str = Field(description='Reasoning on how the question was compressed.')
    question: str = Field(description='New, simplified, self-contained question.')

class EnsembleResponse(BaseModel):
    """The ensemble process result."""
    thought: str = Field(description='Explanation for choosing the final answer.')
    answer: str = Field(description='Best final answer after ensemble.')
    confidence: float = Field(description='Confidence score in [0, 1].')

    def model_post_init(self, __context: object) -> None:
        self.confidence = max(0.0, min(1.0, self.confidence))

class LabelResponse(BaseModel):
    """Used to refine and reorder the sub-questions with corrected dependencies."""
    thought: str = Field(description='Explanation or reasoning about labeling.')
    sub_questions: list[SubQuestionNode] = Field(description='Refined list of sub-questions with corrected dependencies.')

class CritiqueResponse(BaseModel):
    """A response used for LLM to self-critique or question its own correctness."""
    thought: str = Field(description='Critical reflection on correctness.')
    self_assessment: float = Field(description='Self-assessed confidence in the approach/answer. A float in [0,1].')

class DevilsAdvocateResponse(BaseModel):
    """
    A response for a 'devil's advocate' pass.
    We consider an alternative viewpoint or contradictory answer.
    """
    thought: str = Field(description='Reasoning behind the contradictory viewpoint.')
    final_answer: str = Field(description='Alternative or conflicting answer to challenge the main one.')
    sub_questions: list[SubQuestionNode] = Field(description='Any additional sub-questions from the contrarian perspective.')

class AoTPrompter:
    """Generic base prompter that defines the required prompt methods."""

    def recursive_decompose_prompt(self, messages: list[BaseMessage], question: str, sub_answers: list[tuple[str, str]]) -> list[BaseMessage]:
        """
        Prompt for main decomposition.
        Encourages step-by-step reasoning and listing sub-questions as JSON.
        """
        decompose_instructions = 'First, restate the main question.\nDecide if sub-questions are needed. If so, list them.\nIn the \'thought\' field, show your chain-of-thought.\nReturn valid JSON:\n{\n  "thought": "...",\n  "final_answer": "...",\n  "sub_questions": [\n    {"question": "...", "answer": null, "depend": []},\n    ...\n  ]\n}\n'
        content_sub_answers = '\n'.join((f'Sub-answer so far: Q={q}, A={a}' for q, a in sub_answers))
        return messages + [HumanMessage(content=f'Main question:\n{question}'), AIMessage(content=content_sub_answers), AIMessage(content=decompose_instructions)]

    def label_prompt(self, messages: list[BaseMessage], question: str, decompose_response: RecursiveDecomposeResponse) -> list[BaseMessage]:
        """
        Prompt for refining the sub-questions and dependencies.
        """
        label_instructions = 'Review each sub-question to ensure correctness and proper ordering.\nReturn valid JSON in the form:\n{\n  "thought": "...",\n  "sub_questions": [\n    {"question": "...", "answer": "...", "depend": [...]},\n    ...\n  ]\n}\n'
        return messages + [AIMessage(content=f'Question: {question}'), AIMessage(content=f'Current sub-questions:\n{decompose_response.sub_questions}'), AIMessage(content=label_instructions)]

    def contract_prompt(self, messages: list[BaseMessage], sub_answers: list[tuple[str, str]]) -> list[BaseMessage]:
        """
        Prompt for merging sub-answers into one self-contained question.
        """
        contract_instructions = 'Please merge sub-answers into a single short question that is fully self-contained.\nIn \'thought\', show how you unify the information.\nThen produce JSON:\n{\n  "thought": "...",\n  "question": "a short but self-contained question"\n}\n'
        sub_q_content = '\n'.join((f'Q: {q}\nA: {a}' for q, a in sub_answers))
        return messages + [AIMessage(content='We have these sub-questions and answers:'), AIMessage(content=sub_q_content), AIMessage(content=contract_instructions)]

    def contract_direct_prompt(self, messages: list[BaseMessage], contracted_question: str) -> list[BaseMessage]:
        """
        Prompt for directly answering the contracted question thoroughly.
        """
        direct_instructions = 'Answer the simplified question thoroughly. Show your chain-of-thought in \'thought\'.\nReturn JSON:\n{\n  "thought": "...",\n  "final_answer": "..."\n}\n'
        return messages + [HumanMessage(content=f'Simplified question: {contracted_question}'), AIMessage(content=direct_instructions)]

    def critique_prompt(self, messages: list[BaseMessage], thought: str, answer: str) -> list[BaseMessage]:
        """
        Prompt for self-critique.
        """
        critique_instructions = 'Critique your own approach. Identify possible errors or leaps.\nReturn JSON:\n{\n  "thought": "...",\n  "self_assessment": <float in [0,1]>\n}\n'
        return messages + [AIMessage(content=f'Your previous THOUGHT:\n{thought}'), AIMessage(content=f'Your previous ANSWER:\n{answer}'), AIMessage(content=critique_instructions)]

    def ensemble_prompt(self, messages: list[BaseMessage], possible_thought_and_answers: list[tuple[str, str]]) -> list[BaseMessage]:
        """
        Show multiple candidate solutions and pick the best final answer with confidence.
        """
        instructions = 'You have multiple candidate solutions. Compare carefully and pick the best.\nReturn JSON:\n{\n  "thought": "why you chose this final answer",\n  "answer": "the best consolidated answer",\n  "confidence": 0.0 ~ 1.0\n}\n'
        reasonings: list[BaseMessage] = []
        for idx, (thought, ans) in enumerate(possible_thought_and_answers):
            reasonings.append(AIMessage(content=f'[Candidate {idx}] Thought:\n{thought}\nAnswer:\n{ans}\n---'))
        return messages + reasonings + [AIMessage(content=instructions)]

    def devils_advocate_prompt(self, messages: list[BaseMessage], question: str, existing_answer: str) -> list[BaseMessage]:
        """
        Prompt for a devil's advocate approach to contradict or provide an alternative viewpoint.
        """
        instructions = 'Act as a devil\'s advocate. Suppose the existing answer is incomplete or incorrect.\nChallenge it, find alternative ways or details. Provide a new \'final_answer\' (even if contradictory).\nReturn JSON in the same shape as RecursiveDecomposeResponse OR a dedicated structure.\nBut here, let\'s keep it in a new dedicated structure:\n{\n  "thought": "...",\n  "final_answer": "...",\n  "sub_questions": [\n    {"question": "...", "answer": null, "depend": []},\n    ...\n  ]\n}\n'
        return messages + [AIMessage(content=f'Current question: {question}\nExisting answer to challenge: {existing_answer}\n'), AIMessage(content=instructions)]

class StepName(StrEnum):
    """Enum for step names in the pipeline."""
    DOMAIN_DETECTION = 'DomainDetection'
    DECOMPOSITION = 'Decomposition'
    DECOMPOSITION_CRITIQUE = 'DecompositionCritique'
    CONTRACTED_QUESTION = 'ContractedQuestion'
    CONTRACTED_DIRECT_ANSWER = 'ContractedDirectAnswer'
    CONTRACT_CRITIQUE = 'ContractCritique'
    BEST_APPROACH_DECISION = 'BestApproachDecision'
    ENSEMBLE = 'Ensemble'
    FINAL_ANSWER = 'FinalAnswer'
    DEVILS_ADVOCATE = 'DevilsAdvocate'
    DEVILS_ADVOCATE_CRITIQUE = 'DevilsAdvocateCritique'

class StepRelation(StrEnum):
    """Enum for relationship types in the reasoning graph."""
    CRITIQUES = 'CRITIQUES'
    SELECTS = 'SELECTS'
    RESULT_OF = 'RESULT_OF'
    SPLIT_INTO = 'SPLIT_INTO'
    DEPEND_ON = 'DEPEND_ON'
    PRECEDES = 'PRECEDES'
    DECOMPOSED_BY = 'DECOMPOSED_BY'

class StepRecord(BaseModel):
    """A typed record for each pipeline step."""
    step_name: StepName
    domain: Optional[str] = None
    score: Optional[float] = None
    used: Optional[StepName] = None
    sub_questions: Optional[list[SubQuestionNode]] = None
    parent_decomp_step_idx: Optional[int] = None
    parent_subq_idx: Optional[int] = None
    question: Optional[str] = None
    thought: Optional[str] = None
    answer: Optional[str] = None

    def as_properties(self) -> dict[str, str | float | int | None]:
        """Converts the StepRecord to a dictionary of properties."""
        result: dict[str, str | float | int | None] = {}
        if self.score is not None:
            result['score'] = self.score
        if self.domain:
            result['domain'] = self.domain
        if self.question:
            result['question'] = self.question
        if self.thought:
            result['thought'] = self.thought
        if self.answer:
            result['answer'] = self.answer
        return result

class SimpleColorFormatter(logging.Formatter):
    """Simple color-coded logging formatter for console output using ANSI escape codes."""
    BLUE = '\x1b[94m'
    GREEN = '\x1b[92m'
    YELLOW = '\x1b[93m'
    RED = '\x1b[91m'
    RESET = '\x1b[0m'
    LEVEL_COLORS = {logging.DEBUG: BLUE, logging.INFO: GREEN, logging.WARNING: YELLOW, logging.ERROR: RED, logging.CRITICAL: RED}

    def format(self, record: logging.LogRecord) -> str:
        log_color = self.LEVEL_COLORS.get(record.levelno, self.RESET)
        message = super().format(record)
        return f'{log_color}{message}{self.RESET}'
logger = logging.getLogger('AoT')
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
handler.setFormatter(SimpleColorFormatter('%(levelname)s: %(message)s'))
logger.handlers = [handler]
logger.propagate = False
T = TypeVar('T', bound=EnsembleResponse | ContractQuestionResponse | LabelResponse | CritiqueResponse | RecursiveDecomposeResponse | DevilsAdvocateResponse)

@dataclass
class AoTPipeline:
    """
    The pipeline orchestrates:
      1) Recursive decomposition
      2) For each sub-question, it tries a main approach + a devil's advocate approach
      3) Merges sub-answers using an ensemble
      4) Contracts the question
      5) Possibly does a direct approach on the contracted question
      6) Ensembling the final answers
    """
    chatterer: Chatterer
    max_depth: int = 2
    max_retries: int = 2
    steps_history: list[StepRecord] = field(default_factory=list)
    prompter: AoTPrompter = field(default_factory=AoTPrompter)

    async def _ainvoke_pydantic(self, messages: list[BaseMessage], model_cls: Type[T], fallback: str='<None>') -> T:
        """
        Attempts up to max_retries to parse the model_cls from LLM output as JSON.
        """
        for attempt in range(1, self.max_retries + 1):
            try:
                return await self.chatterer.agenerate_pydantic(response_model=model_cls, messages=messages)
            except ValidationError as e:
                logger.warning(f'ValidationError on attempt {attempt} for {model_cls.__name__}: {e}')
                if attempt == self.max_retries:
                    if issubclass(model_cls, EnsembleResponse):
                        return model_cls(thought=fallback, answer=fallback, confidence=0.0)
                    elif issubclass(model_cls, ContractQuestionResponse):
                        return model_cls(thought=fallback, question=fallback)
                    elif issubclass(model_cls, LabelResponse):
                        return model_cls(thought=fallback, sub_questions=[])
                    elif issubclass(model_cls, CritiqueResponse):
                        return model_cls(thought=fallback, self_assessment=0.0)
                    elif issubclass(model_cls, DevilsAdvocateResponse):
                        return model_cls(thought=fallback, final_answer=fallback, sub_questions=[])
                    else:
                        return model_cls(thought=fallback, final_answer=fallback, sub_questions=[])
        raise RuntimeError('Unexpected error in _ainvoke_pydantic')

    async def _ainvoke_critique(self, messages: list[BaseMessage], thought: str, answer: str) -> CritiqueResponse:
        """
        Instructs the LLM to critique the given thought & answer, returning CritiqueResponse.
        """
        return await self._ainvoke_pydantic(messages=self.prompter.critique_prompt(messages=messages, thought=thought, answer=answer), model_cls=CritiqueResponse)

    async def _ainvoke_devils_advocate(self, messages: list[BaseMessage], question: str, existing_answer: str) -> DevilsAdvocateResponse:
        """
        Instructs the LLM to challenge an existing answer with a devil's advocate approach.
        """
        return await self._ainvoke_pydantic(messages=self.prompter.devils_advocate_prompt(messages, question=question, existing_answer=existing_answer), model_cls=DevilsAdvocateResponse)

    async def _arecursive_decompose_question(self, messages: list[BaseMessage], question: str, depth: int, parent_decomp_step_idx: Optional[int]=None, parent_subq_idx: Optional[int]=None) -> RecursiveDecomposeResponse:
        """
        Recursively decompose the given question. For each sub-question:
          1) Recursively decompose that sub-question if we still have depth left
          2) After getting a main sub-answer, do a devil's advocate pass
          3) Combine main sub-answer + devil's advocate alternative via an ensemble
        """
        if depth < 0:
            logger.info('Max depth reached, returning unknown.')
            return RecursiveDecomposeResponse(thought=MAX_DEPTH_REACHED, final_answer=UNKNOWN, sub_questions=[])
        decompose_resp: RecursiveDecomposeResponse = await self._ainvoke_pydantic(messages=self.prompter.recursive_decompose_prompt(messages=messages, question=question, sub_answers=[]), model_cls=RecursiveDecomposeResponse)
        if decompose_resp.sub_questions:
            label_resp: LabelResponse = await self._ainvoke_pydantic(messages=self.prompter.label_prompt(messages, question, decompose_resp), model_cls=LabelResponse)
            decompose_resp.sub_questions = label_resp.sub_questions
        current_decomp_step_idx = self._record_decomposition_step(question=question, final_answer=decompose_resp.final_answer, sub_questions=decompose_resp.sub_questions, parent_decomp_step_idx=parent_decomp_step_idx, parent_subq_idx=parent_subq_idx)
        if depth > 0 and decompose_resp.sub_questions:
            solved_subs: list[SubQuestionNode] = await self._aresolve_sub_questions(messages=messages, sub_questions=decompose_resp.sub_questions, depth=depth, parent_decomp_step_idx=current_decomp_step_idx)
            refined_prompt = self.prompter.recursive_decompose_prompt(messages=messages, question=question, sub_answers=[(sq.question, sq.answer or UNKNOWN) for sq in solved_subs])
            refined_resp: RecursiveDecomposeResponse = await self._ainvoke_pydantic(refined_prompt, RecursiveDecomposeResponse)
            decompose_resp.final_answer = refined_resp.final_answer
            decompose_resp.sub_questions = solved_subs
            self.steps_history[current_decomp_step_idx].answer = refined_resp.final_answer
            self.steps_history[current_decomp_step_idx].sub_questions = solved_subs
        return decompose_resp

    def _record_decomposition_step(self, question: str, final_answer: str, sub_questions: list[SubQuestionNode], parent_decomp_step_idx: Optional[int], parent_subq_idx: Optional[int]) -> int:
        """
        Save the decomposition step in steps_history, returning the index.
        """
        step_record = StepRecord(step_name=StepName.DECOMPOSITION, question=question, answer=final_answer, sub_questions=sub_questions, parent_decomp_step_idx=parent_decomp_step_idx, parent_subq_idx=parent_subq_idx)
        self.steps_history.append(step_record)
        return len(self.steps_history) - 1

    async def _aresolve_sub_questions(self, messages: list[BaseMessage], sub_questions: list[SubQuestionNode], depth: int, parent_decomp_step_idx: Optional[int]) -> list[SubQuestionNode]:
        """
        Resolve sub-questions in topological order.
        For each sub-question:
          1) Recursively decompose (main approach).
          2) Acquire a devil's advocate alternative.
          3) Critique or ensemble if needed.
          4) Finalize sub-question answer.
        """
        n = len(sub_questions)
        in_degree = [0] * n
        graph: list[list[int]] = [[] for _ in range(n)]
        for i, sq in enumerate(sub_questions):
            for dep in sq.depend:
                if 0 <= dep < n:
                    in_degree[i] += 1
                    graph[dep].append(i)
        queue = [i for i in range(n) if in_degree[i] == 0]
        topo_order: list[int] = []
        while queue:
            node = queue.pop(0)
            topo_order.append(node)
            for nxt in graph[node]:
                in_degree[nxt] -= 1
                if in_degree[nxt] == 0:
                    queue.append(nxt)
        final_subs: dict[int, SubQuestionNode] = {}

        async def _resolve_one_subq(idx: int):
            sq = sub_questions[idx]
            main_resp = await self._arecursive_decompose_question(messages=messages, question=sq.question, depth=depth - 1, parent_decomp_step_idx=parent_decomp_step_idx, parent_subq_idx=idx)
            main_answer = main_resp.final_answer
            devils_resp = await self._ainvoke_devils_advocate(messages=messages, question=sq.question, existing_answer=main_answer)
            ensemble_sub = await self._ainvoke_pydantic(self.prompter.ensemble_prompt(messages=messages, possible_thought_and_answers=[(main_resp.thought, main_answer), (devils_resp.thought, devils_resp.final_answer)]), EnsembleResponse)
            sub_best_answer = ensemble_sub.answer
            sq.answer = sub_best_answer
            final_subs[idx] = sq
            self.steps_history.append(StepRecord(step_name=StepName.DEVILS_ADVOCATE, question=sq.question, answer=devils_resp.final_answer, thought=devils_resp.thought, sub_questions=devils_resp.sub_questions))
            dev_adv_crit = await self._ainvoke_critique(messages=messages, thought=devils_resp.thought, answer=devils_resp.final_answer)
            self.steps_history.append(StepRecord(step_name=StepName.DEVILS_ADVOCATE_CRITIQUE, thought=dev_adv_crit.thought, score=dev_adv_crit.self_assessment))
        tasks = [_resolve_one_subq(i) for i in topo_order]
        await asyncio.gather(*tasks, return_exceptions=False)
        return [final_subs[i] for i in range(n)]

    async def arun_pipeline(self, messages: list[BaseMessage]) -> str:
        """
        Execute the pipeline:
          1) Decompose the main question (recursively).
          2) Self-critique.
          3) Provide a devil's advocate approach on the entire main result.
          4) Contract sub-answers (optional).
          5) Directly solve the contracted question.
          6) Self-critique again.
          7) Final ensemble across main vs devil's vs contracted direct answer.
          8) Return final answer.
        """
        self.steps_history.clear()
        original_question: str = messages[-1].text()
        decomp_resp = await self._arecursive_decompose_question(messages=messages, question=original_question, depth=self.max_depth)
        logger.info(f'[Main Decomposition] final_answer={decomp_resp.final_answer}')
        decomp_critique = await self._ainvoke_critique(messages=messages, thought=decomp_resp.thought, answer=decomp_resp.final_answer)
        self.steps_history.append(StepRecord(step_name=StepName.DECOMPOSITION_CRITIQUE, thought=decomp_critique.thought, score=decomp_critique.self_assessment))
        devils_on_main = await self._ainvoke_devils_advocate(messages=messages, question=original_question, existing_answer=decomp_resp.final_answer)
        self.steps_history.append(StepRecord(step_name=StepName.DEVILS_ADVOCATE, question=original_question, answer=devils_on_main.final_answer, thought=devils_on_main.thought, sub_questions=devils_on_main.sub_questions))
        devils_crit_main = await self._ainvoke_critique(messages=messages, thought=devils_on_main.thought, answer=devils_on_main.final_answer)
        self.steps_history.append(StepRecord(step_name=StepName.DEVILS_ADVOCATE_CRITIQUE, thought=devils_crit_main.thought, score=devils_crit_main.self_assessment))
        top_decomp_record: Optional[StepRecord] = next((s for s in reversed(self.steps_history) if s.step_name == StepName.DECOMPOSITION and s.parent_decomp_step_idx is None), None)
        if top_decomp_record and top_decomp_record.sub_questions:
            sub_answers = [(sq.question, sq.answer or UNKNOWN) for sq in top_decomp_record.sub_questions]
        else:
            sub_answers = []
        contract_resp = await self._ainvoke_pydantic(messages=self.prompter.contract_prompt(messages, sub_answers), model_cls=ContractQuestionResponse)
        contracted_question = contract_resp.question
        self.steps_history.append(StepRecord(step_name=StepName.CONTRACTED_QUESTION, question=contracted_question, thought=contract_resp.thought))
        contracted_direct = await self._ainvoke_pydantic(messages=self.prompter.contract_direct_prompt(messages, contracted_question), model_cls=RecursiveDecomposeResponse, fallback='No Contracted Direct Answer')
        self.steps_history.append(StepRecord(step_name=StepName.CONTRACTED_DIRECT_ANSWER, answer=contracted_direct.final_answer, thought=contracted_direct.thought))
        logger.info(f'[Contracted Direct] final_answer={contracted_direct.final_answer}')
        contract_critique = await self._ainvoke_critique(messages=messages, thought=contracted_direct.thought, answer=contracted_direct.final_answer)
        self.steps_history.append(StepRecord(step_name=StepName.CONTRACT_CRITIQUE, thought=contract_critique.thought, score=contract_critique.self_assessment))
        ensemble_resp = await self._ainvoke_pydantic(self.prompter.ensemble_prompt(messages=messages, possible_thought_and_answers=[(decomp_resp.thought, decomp_resp.final_answer), (devils_on_main.thought, devils_on_main.final_answer), (contracted_direct.thought, contracted_direct.final_answer)]), EnsembleResponse)
        best_approach_answer = ensemble_resp.answer
        approach_used = StepName.ENSEMBLE
        self.steps_history.append(StepRecord(step_name=StepName.BEST_APPROACH_DECISION, used=approach_used))
        logger.info(f'[Best Approach Decision] => {approach_used}')
        self.steps_history.append(StepRecord(step_name=StepName.FINAL_ANSWER, answer=best_approach_answer, score=ensemble_resp.confidence))
        logger.info(f'[Final Answer] => {best_approach_answer}')
        return best_approach_answer

    def run_pipeline(self, messages: list[BaseMessage]) -> str:
        """Synchronous wrapper around arun_pipeline."""
        return asyncio.run(self.arun_pipeline(messages))

    def get_reasoning_graph(self, global_id_prefix: str='AoT'):
        """
        Constructs a Graph object (from hypothetical `neo4j_extension`)
        capturing the pipeline steps, including devil's advocate steps.
        """
        from neo4j_extension import Graph, Node, Relationship
        g = Graph()
        step_nodes: dict[int, Node] = {}
        subq_nodes: dict[str, Node] = {}
        for i, record in enumerate(self.steps_history):
            step_node = Node(properties=record.as_properties(), labels={record.step_name}, globalId=f'{global_id_prefix}_step_{i}')
            g.add_node(step_node)
            step_nodes[i] = step_node
        all_sub_questions: dict[str, tuple[int, int, SubQuestionNode]] = {}
        for i, record in enumerate(self.steps_history):
            if record.sub_questions:
                for sq_idx, sq in enumerate(record.sub_questions):
                    sq_id = f'{global_id_prefix}_decomp_{i}_sub_{sq_idx}'
                    all_sub_questions[sq_id] = (i, sq_idx, sq)
        for sq_id, (i, sq_idx, sq) in all_sub_questions.items():
            n_subq = Node(properties={'question': sq.question, 'answer': sq.answer or ''}, labels={'SubQuestion'}, globalId=sq_id)
            g.add_node(n_subq)
            subq_nodes[sq_id] = n_subq
        for i, record in enumerate(self.steps_history):
            if record.sub_questions:
                start_node = step_nodes[i]
                for sq_idx, sq in enumerate(record.sub_questions):
                    sq_id = f'{global_id_prefix}_decomp_{i}_sub_{sq_idx}'
                    end_node = subq_nodes[sq_id]
                    rel = Relationship(properties={}, rel_type=StepRelation.SPLIT_INTO, start_node=start_node, end_node=end_node, globalId=f'{global_id_prefix}_split_{i}_{sq_idx}')
                    g.add_relationship(rel)
                    for dep in sq.depend:
                        if 0 <= dep < len(record.sub_questions):
                            dep_id = f'{global_id_prefix}_decomp_{i}_sub_{dep}'
                            if dep_id in subq_nodes:
                                dep_node = subq_nodes[dep_id]
                                rel_dep = Relationship(properties={}, rel_type=StepRelation.DEPEND_ON, start_node=end_node, end_node=dep_node, globalId=f'{global_id_prefix}_dep_{i}_q_{sq_idx}_on_{dep}')
                                g.add_relationship(rel_dep)
        for i in range(len(self.steps_history) - 1):
            start_node = step_nodes[i]
            end_node = step_nodes[i + 1]
            rel = Relationship(properties={}, rel_type=StepRelation.PRECEDES, start_node=start_node, end_node=end_node, globalId=f'{global_id_prefix}_precede_{i}_to_{i + 1}')
            g.add_relationship(rel)
        for i, record in enumerate(self.steps_history):
            if 'CRITIQUE' in record.step_name:
                if i > 0:
                    start_node = step_nodes[i]
                    end_node = step_nodes[i - 1]
                    rel = Relationship(properties={}, rel_type=StepRelation.CRITIQUES, start_node=start_node, end_node=end_node, globalId=f'{global_id_prefix}_crit_{i}')
                    g.add_relationship(rel)
        best_decision_idx = None
        used_step_idx = None
        for i, record in enumerate(self.steps_history):
            if record.step_name == StepName.BEST_APPROACH_DECISION and record.used:
                best_decision_idx = i
                used_step_idx = next((j for j in step_nodes if self.steps_history[j].step_name == record.used), None)
                if used_step_idx is not None:
                    rel = Relationship(properties={}, rel_type=StepRelation.SELECTS, start_node=step_nodes[i], end_node=step_nodes[used_step_idx], globalId=f'{global_id_prefix}_select_{i}_use_{used_step_idx}')
                    g.add_relationship(rel)
        final_answer_idx = next((i for i, r in enumerate(self.steps_history) if r.step_name == StepName.FINAL_ANSWER), None)
        if final_answer_idx is not None and best_decision_idx is not None:
            rel = Relationship(properties={}, rel_type=StepRelation.RESULT_OF, start_node=step_nodes[final_answer_idx], end_node=step_nodes[best_decision_idx], globalId=f'{global_id_prefix}_final_{final_answer_idx}_resultof_{best_decision_idx}')
            g.add_relationship(rel)
        return g

@dataclass
class AoTStrategy(BaseStrategy):
    """
    Strategy using AoTPipeline with a reasoning graph and deeper devil's advocate.
    """
    pipeline: AoTPipeline

    async def ainvoke(self, messages: LanguageModelInput) -> str:
        """Asynchronously run the pipeline with the given messages."""
        msgs = self.pipeline.chatterer.client._convert_input(messages).to_messages()
        return await self.pipeline.arun_pipeline(msgs)

    def invoke(self, messages: LanguageModelInput) -> str:
        """Synchronously run the pipeline with the given messages."""
        msgs = self.pipeline.chatterer.client._convert_input(messages).to_messages()
        return self.pipeline.run_pipeline(msgs)

    def get_reasoning_graph(self):
        """Return the AoT reasoning graph from the pipeline’s steps history."""
        return self.pipeline.get_reasoning_graph(global_id_prefix='AoT')
if __name__ == '__main__':
    from neo4j_extension import Neo4jConnection
    chatterer = Chatterer.openai()
    pipeline = AoTPipeline(chatterer=chatterer, max_depth=3)
    strategy = AoTStrategy(pipeline=pipeline)
    question = 'Solve 5.9 = 5.11 - x. Also compare 9.11 and 9.9.'
    answer = strategy.invoke(question)
    print('Final Answer:', answer)
    graph = strategy.get_reasoning_graph()
    print(f'\nGraph has {len(graph.nodes)} nodes and {len(graph.relationships)} relationships.')
    with Neo4jConnection() as conn:
        conn.clear_all()
        conn.upsert_graph(graph)
        print('Graph stored in Neo4j.')
```

```C:\Users\cosogi\chatterer\chatterer\strategies\base.py
from abc import ABC, abstractmethod
from ..language_model import LanguageModelInput

class BaseStrategy(ABC):

    @abstractmethod
    def invoke(self, messages: LanguageModelInput) -> str:
        """
        Invoke the strategy with the given messages.

        messages: List of messages to be passed to the strategy.
            e.g. [{"role": "user", "content": "What is the meaning of life?"}]
        """
```

```C:\Users\cosogi\chatterer\chatterer\strategies\__init__.py
from .atom_of_thoughts import AoTPipeline, AoTStrategy, AoTPrompter
from .base import BaseStrategy
__all__ = ['BaseStrategy', 'AoTPipeline', 'AoTPrompter', 'AoTStrategy']
```

```C:\Users\cosogi\chatterer\chatterer\tools\caption_markdown_images.py
import os.path
import re
from asyncio import gather
from traceback import format_exception_only, print_exc
from typing import Awaitable, Callable, ClassVar, Literal, NamedTuple, NewType, Optional, Self, TypeGuard, cast
from urllib.parse import urljoin, urlparse
from chatterer.language_model import Chatterer
from ..utils.base64_image import Base64Image, ImageProcessingConfig

class MarkdownLink(NamedTuple):
    type: Literal['link', 'image']
    url: str
    text: str
    title: Optional[str]
    pos: int
    end_pos: int

    @classmethod
    def from_markdown(cls, markdown_text: str, referer_url: Optional[str]) -> list[Self]:
        """
        The main function that returns the list of MarkdownLink for the input text.
        For simplicity, we do a "pure inline parse" of the entire text
        instead of letting the block parser break it up. That ensures that
        link tokens cover the global positions of the entire input.
        """
        from mistune import InlineParser, InlineState, Markdown

        class _TrackingInlineState(InlineState):
            meta_offset: int = 0

            def copy(self) -> Self:
                new_state = self.__class__(self.env)
                new_state.src = self.src
                new_state.tokens = []
                new_state.in_image = self.in_image
                new_state.in_link = self.in_link
                new_state.in_emphasis = self.in_emphasis
                new_state.in_strong = self.in_strong
                new_state.meta_offset = self.meta_offset
                return new_state

        class _TrackingInlineParser(InlineParser):
            state_cls: ClassVar = _TrackingInlineState

            def parse_link(self, m: re.Match[str], state: _TrackingInlineState) -> Optional[int]:
                """
                Mistune calls parse_link with a match object for the link syntax
                and the current inline state. If we successfully parse the link,
                super().parse_link(...) returns the new position *within self.src*.
                We add that to state.meta_offset for the global position.

                Because parse_link in mistune might return None or an int, we only
                record positions if we get an int back (meaning success).
                """
                offset = state.meta_offset
                new_pos: int | None = super().parse_link(m, state)
                if new_pos is not None:
                    if state.tokens:
                        token = state.tokens[-1]
                        token['global_pos'] = (offset + m.start(), offset + new_pos)
                return new_pos
        md = Markdown(inline=_TrackingInlineParser())
        state = _TrackingInlineState({})
        state.src = markdown_text
        md.inline.parse(state)
        return cls._extract_links(tokens=state.tokens, referer_url=referer_url)

    @property
    def inline_text(self) -> str:
        return self.text.replace('\n', ' ').strip()

    @property
    def inline_title(self) -> str:
        return self.title.replace('\n', ' ').strip() if self.title else ''

    @property
    def link_markdown(self) -> str:
        if self.title:
            return f'[{self.inline_text}]({self.url} "{self.inline_title}")'
        return f'[{self.inline_text}]({self.url})'

    @classmethod
    def replace(cls, text: str, replacements: list[tuple[Self, str]]) -> str:
        for self, replacement in sorted(replacements, key=lambda x: x[0].pos, reverse=True):
            text = text[:self.pos] + replacement + text[self.end_pos:]
        return text

    @classmethod
    def _extract_links(cls, tokens: list[dict[str, object]], referer_url: Optional[str]) -> list[Self]:
        results: list[Self] = []
        for token in tokens:
            if (type := token.get('type')) in ('link', 'image') and 'global_pos' in token and ('attrs' in token) and _attrs_typeguard((attrs := token['attrs'])) and ('url' in attrs) and _url_typeguard((url := attrs['url'])) and _global_pos_typeguard((global_pos := token['global_pos'])):
                if referer_url:
                    url = _to_absolute_path(path=url, referer=referer_url)
                children: object | None = token.get('children')
                if _children_typeguard(children):
                    text = _extract_text(children)
                else:
                    text = ''
                if 'title' in attrs:
                    title = str(attrs['title'])
                else:
                    title = None
                start, end = global_pos
                results.append(cls(type, url, text, title, start, end))
            if 'children' in token and _children_typeguard((children := token['children'])):
                results.extend(cls._extract_links(children, referer_url))
        return results
ImageDataAndReferences = dict[Optional[str], list[MarkdownLink]]
ImageDescriptionAndReferences = NewType('ImageDescriptionAndReferences', ImageDataAndReferences)

def caption_markdown_images(markdown_text: str, headers: dict[str, str], image_processing_config: ImageProcessingConfig, description_format: str, image_description_instruction: str, chatterer: Chatterer, img_bytes_fetcher: Optional[Callable[[str, dict[str, str]], bytes]]=None) -> str:
    """
    Replace image URLs in Markdown text with their alt text and generate descriptions using a language model.
    """
    image_url_and_markdown_links: dict[Optional[Base64Image], list[MarkdownLink]] = _get_image_url_and_markdown_links(markdown_text=markdown_text, headers=headers, config=image_processing_config, img_bytes_fetcher=img_bytes_fetcher)
    image_description_and_references: ImageDescriptionAndReferences = ImageDescriptionAndReferences({})
    for image_url, markdown_links in image_url_and_markdown_links.items():
        if image_url is not None:
            try:
                image_summary: str = chatterer.describe_image(image_url=image_url.data_uri, instruction=image_description_instruction)
            except Exception:
                print_exc()
                continue
            image_description_and_references[image_summary] = markdown_links
        else:
            image_description_and_references[None] = markdown_links
    return _replace_images(markdown_text=markdown_text, image_description_and_references=image_description_and_references, description_format=description_format)

async def acaption_markdown_images(markdown_text: str, headers: dict[str, str], image_processing_config: ImageProcessingConfig, description_format: str, image_description_instruction: str, chatterer: Chatterer, img_bytes_fetcher: Optional[Callable[[str, dict[str, str]], Awaitable[bytes]]]=None) -> str:
    """
    Replace image URLs in Markdown text with their alt text and generate descriptions using a language model.
    """
    image_url_and_markdown_links: dict[Optional[Base64Image], list[MarkdownLink]] = await _aget_image_url_and_markdown_links(markdown_text=markdown_text, headers=headers, config=image_processing_config, img_bytes_fetcher=img_bytes_fetcher)

    async def dummy() -> None:
        pass

    def _handle_exception(e: Optional[str | BaseException]) -> TypeGuard[Optional[str]]:
        if isinstance(e, BaseException):
            print(format_exception_only(type(e), e))
            return False
        return True
    coros: list[Awaitable[Optional[str]]] = [chatterer.adescribe_image(image_url=image_url.data_uri, instruction=image_description_instruction) if image_url is not None else dummy() for image_url in image_url_and_markdown_links.keys()]
    return _replace_images(markdown_text=markdown_text, image_description_and_references=ImageDescriptionAndReferences({image_summary: markdown_links for markdown_links, image_summary in zip(image_url_and_markdown_links.values(), await gather(*coros, return_exceptions=True)) if _handle_exception(image_summary)}), description_format=description_format)

def _children_typeguard(obj: object) -> TypeGuard[list[dict[str, object]]]:
    if not isinstance(obj, list):
        return False
    return all((isinstance(i, dict) for i in cast(list[object], obj)))

def _attrs_typeguard(obj: object) -> TypeGuard[dict[str, object]]:
    if not isinstance(obj, dict):
        return False
    return all((isinstance(k, str) for k in cast(dict[object, object], obj)))

def _global_pos_typeguard(obj: object) -> TypeGuard[tuple[int, int]]:
    if not isinstance(obj, tuple):
        return False
    obj = cast(tuple[object, ...], obj)
    if len(obj) != 2:
        return False
    return all((isinstance(i, int) for i in obj))

def _url_typeguard(obj: object) -> TypeGuard[str]:
    return isinstance(obj, str)

def _extract_text(tokens: list[dict[str, object]]) -> str:
    parts: list[str] = []
    for t in tokens:
        if t.get('type') == 'text':
            parts.append(str(t.get('raw', '')))
        elif 'children' in t:
            children: object = t['children']
            if not _children_typeguard(children):
                continue
            parts.append(_extract_text(children))
    return ''.join(parts)

def _to_absolute_path(path: str, referer: str) -> str:
    """
    path     : 변환할 경로(상대/절대 경로 혹은 URL일 수도 있음)
    referer  : 기준이 되는 절대경로(혹은 URL)
    """
    ref_parsed = urlparse(referer)
    is_referer_url = bool(ref_parsed.scheme and ref_parsed.netloc)
    if is_referer_url:
        parsed = urlparse(path)
        if parsed.scheme and parsed.netloc:
            return path
        else:
            return urljoin(referer, path)
    elif os.path.isabs(path):
        return path
    else:
        if not os.path.isdir(referer):
            referer_dir = os.path.dirname(referer)
        else:
            referer_dir = referer
        combined = os.path.join(referer_dir, path)
        return os.path.abspath(combined)

def _get_image_url_and_markdown_links(markdown_text: str, headers: dict[str, str], config: ImageProcessingConfig, img_bytes_fetcher: Optional[Callable[[str, dict[str, str]], bytes]]=None) -> dict[Optional[Base64Image], list[MarkdownLink]]:
    image_matches: dict[Optional[Base64Image], list[MarkdownLink]] = {}
    for markdown_link in MarkdownLink.from_markdown(markdown_text=markdown_text, referer_url=headers.get('Referer')):
        if markdown_link.type == 'link':
            image_matches.setdefault(None, []).append(markdown_link)
            continue
        image_data = Base64Image.from_url_or_path(markdown_link.url, headers=headers, config=config, img_bytes_fetcher=img_bytes_fetcher)
        if not image_data:
            image_matches.setdefault(None, []).append(markdown_link)
            continue
        image_matches.setdefault(image_data, []).append(markdown_link)
    return image_matches

async def _aget_image_url_and_markdown_links(markdown_text: str, headers: dict[str, str], config: ImageProcessingConfig, img_bytes_fetcher: Optional[Callable[[str, dict[str, str]], Awaitable[bytes]]]=None) -> dict[Optional[Base64Image], list[MarkdownLink]]:
    image_matches: dict[Optional[Base64Image], list[MarkdownLink]] = {}
    for markdown_link in MarkdownLink.from_markdown(markdown_text=markdown_text, referer_url=headers.get('Referer')):
        if markdown_link.type == 'link':
            image_matches.setdefault(None, []).append(markdown_link)
            continue
        image_data = await Base64Image.afrom_url_or_path(markdown_link.url, headers=headers, config=config, img_bytes_fetcher=img_bytes_fetcher)
        if not image_data:
            image_matches.setdefault(None, []).append(markdown_link)
            continue
        image_matches.setdefault(image_data, []).append(markdown_link)
    return image_matches

def _replace_images(markdown_text: str, image_description_and_references: ImageDescriptionAndReferences, description_format: str) -> str:
    replacements: list[tuple[MarkdownLink, str]] = []
    for image_description, markdown_links in image_description_and_references.items():
        for markdown_link in markdown_links:
            if image_description is None:
                if markdown_link.type == 'link':
                    replacements.append((markdown_link, markdown_link.link_markdown))
                elif markdown_link.type == 'image':
                    replacements.append((markdown_link, f'![{markdown_link.inline_text}](...)'))
            else:
                replacements.append((markdown_link, description_format.format(image_summary=image_description.replace('\n', ' '), inline_text=markdown_link.inline_text, **markdown_link._asdict())))
    return MarkdownLink.replace(markdown_text, replacements)
```

```C:\Users\cosogi\chatterer\chatterer\tools\convert_pdf_to_markdown.py
from __future__ import annotations
import logging
import re
from contextlib import contextmanager
from dataclasses import dataclass
from typing import TYPE_CHECKING, Callable, Iterable, List, Literal, Optional, Union
from ..language_model import Chatterer, HumanMessage
from ..utils.base64_image import Base64Image
from ..utils.bytesio import PathOrReadable, read_bytes_stream
if TYPE_CHECKING:
    from pymupdf import Document
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
MARKDOWN_PATTERN: re.Pattern[str] = re.compile('```(?:markdown\\s*\\n)?(.*?)```', re.DOTALL)

@dataclass
class PdfToMarkdown:
    """
    Converts PDF documents to Markdown using a multimodal LLM (Chatterer).
    Processes PDFs page by page, providing the LLM with both the extracted raw
    text and a rendered image of the page to handle complex layouts. It maintains
    context between pages by feeding the *tail end* of the previously generated
    Markdown back into the prompt for the next page to ensure smooth transitions.
    """
    chatterer: Chatterer
    'An instance of the Chatterer class configured with a vision-capable model.'
    image_zoom: float = 2.0
    'Zoom factor for rendering PDF pages as images (higher zoom = higher resolution).'
    image_format: Literal['jpg', 'jpeg', 'png'] = 'png'
    "The format for the rendered image ('png', 'jpeg', 'jpg'.)."
    image_jpg_quality: int = 95
    'Quality for JPEG images (if used).'
    context_tail_lines: int = 10
    "Number of lines from the end of the previous page's Markdown to use as context."

    def _get_context_tail(self, markdown_text: Optional[str]) -> Optional[str]:
        """Extracts the last N lines from the given markdown text."""
        if not markdown_text or self.context_tail_lines <= 0:
            return None
        lines = markdown_text.strip().splitlines()
        if not lines:
            return None
        tail_lines = lines[-self.context_tail_lines:]
        return '\n'.join(tail_lines)

    def _format_prompt_content(self, page_text: str, page_image_b64: Base64Image, previous_markdown_context_tail: Optional[str]=None, page_number: int=0, total_pages: int=1) -> HumanMessage:
        """
        Formats the content list for the HumanMessage input to the LLM.
        Uses only the tail end of the previous page's markdown for context.
        """
        instruction = f"You are an expert PDF to Markdown converter. Your task is to convert the content of the provided PDF page (Page {page_number + 1} of {total_pages}) into accurate and well-formatted Markdown. You are given:\n1.  The raw text extracted from the page ([Raw Text]).\n2.  A rendered image of the page ([Rendered Image]) showing its visual layout.\n3.  (Optional) The *ending portion* of the Markdown generated from the previous page ([End of Previous Page Markdown]) for context continuity.\n\n**Conversion Requirements:**\n*   **Text:** Reconstruct paragraphs, headings, lists, etc., naturally based on the visual layout. Correct OCR/formatting issues from [Raw Text] using the image. Minimize unnecessary whitespace.\n*   **Tables:** Convert tables accurately into Markdown table format (`| ... |`). Use image for text if [Raw Text] is garbled.\n*   **Images/Diagrams:** Describe significant visual elements (charts, graphs) within `<details>` tags. Example: `<details><summary>Figure 1: Description</summary>Detailed textual description from the image.</details>`. Ignore simple decorative images. Do **not** use `![alt](...)`.\n*   **Layout:** Respect columns, code blocks (``` ```), footnotes, etc., using standard Markdown.\n*   **Continuity (Crucial):**\n    *   Examine the [End of Previous Page Markdown] if provided.\n    *   If the current page's content *continues* a sentence, paragraph, list, or code block from the previous page, ensure your generated Markdown for *this page* starts seamlessly from that continuation point.\n    *   For example, if the previous page ended mid-sentence, the Markdown for *this page* should begin with the rest of that sentence.\n    *   **Do NOT repeat the content already present in [End of Previous Page Markdown] in your output.**\n    *   If the current page starts a new section (e.g., with a heading), begin the Markdown output fresh, ignoring the previous context tail unless necessary for list numbering, etc.\n\n**Input Data:**\n[Raw Text]\n```\n{(page_text if page_text else 'No text extracted from this page.')}\n```\n[Rendered Image]\n(See attached image)\n"
        if previous_markdown_context_tail:
            instruction += f'[End of Previous Page Markdown]\n```markdown\n... (content from previous page ends with) ...\n{previous_markdown_context_tail}\n```\n**Task:** Generate the Markdown for the *current* page (Page {page_number + 1}), ensuring it correctly continues from or follows the [End of Previous Page Markdown]. Start the output *only* with the content belonging to the current page.'
        else:
            instruction += '**Task:** Generate the Markdown for the *current* page (Page {page_number + 1}). This is the first page being processed in this batch.'
        instruction += '\n\n**Output only the Markdown content for the current page.** Ensure your output starts correctly based on the continuity rules.'
        return HumanMessage(content=[instruction, page_image_b64.data_uri_content])

    def convert(self, pdf_input: Union[str, 'Document'], page_indices: Optional[Union[Iterable[int], int]]=None, progress_callback: Optional[Callable[[int, int], None]]=None) -> str:
        """
        Converts a PDF document (or specific pages) to Markdown synchronously.
        Args:
            pdf_input: Path to the PDF file or a pymupdf.Document object.
            page_indices: Specific 0-based page indices to convert. If None, converts all pages.
                          Can be a single int or an iterable of ints.
            progress_callback: An optional function to call with (current_page_index, total_pages_to_process)
                               after each page is processed.
        Returns:
            A single string containing the concatenated Markdown output for the processed pages.
        """
        with open_pdf(pdf_input) as doc:
            target_page_indices = list(_get_page_indices(page_indices, len(doc)))
            total_pages_to_process = len(target_page_indices)
            if total_pages_to_process == 0:
                logger.warning('No pages selected for processing.')
                return ''
            full_markdown_output: List[str] = []
            previous_page_markdown: Optional[str] = None
            logger.info('Extracting text and rendering images for selected pages...')
            page_text_dict = extract_text_from_pdf(doc, target_page_indices)
            page_image_dict = render_pdf_as_image(doc, page_indices=target_page_indices, zoom=self.image_zoom, output=self.image_format, jpg_quality=self.image_jpg_quality)
            logger.info(f'Starting Markdown conversion for {total_pages_to_process} pages...')
            page_idx: int = target_page_indices.pop(0)
            i: int = 1
            while True:
                logger.info(f'Processing page {i}/{total_pages_to_process} (Index: {page_idx})...')
                try:
                    context_tail = self._get_context_tail(previous_page_markdown)
                    message = self._format_prompt_content(page_text=page_text_dict.get(page_idx, ''), page_image_b64=Base64Image.from_bytes(page_image_dict[page_idx], ext=self.image_format), previous_markdown_context_tail=context_tail, page_number=page_idx, total_pages=len(doc))
                    logger.debug(f'Sending request to LLM for page index {page_idx}...')
                    response = self.chatterer([message])
                    markdowns: list[str] = [match.group(1).strip() for match in MARKDOWN_PATTERN.finditer(response)]
                    if markdowns:
                        current_page_markdown = '\n'.join(markdowns)
                    else:
                        current_page_markdown = response.strip()
                        if current_page_markdown.startswith('```') and current_page_markdown.endswith('```'):
                            current_page_markdown = current_page_markdown[3:-3].strip()
                        elif '```' in current_page_markdown:
                            logger.warning(f"Page {page_idx + 1}: Response contains '```' but not in expected format. Using raw response.")
                    logger.debug(f'Received response from LLM for page index {page_idx}.')
                    full_markdown_output.append(current_page_markdown)
                    previous_page_markdown = current_page_markdown
                except Exception as e:
                    logger.error(f'Failed to process page index {page_idx}: {e}', exc_info=True)
                    continue
                if progress_callback:
                    try:
                        progress_callback(i, total_pages_to_process)
                    except Exception as cb_err:
                        logger.warning(f'Progress callback failed: {cb_err}')
                if not target_page_indices:
                    break
                page_idx = target_page_indices.pop(0)
                i += 1
        return '\n\n'.join(full_markdown_output).strip()

def render_pdf_as_image(doc: 'Document', zoom: float=2.0, output: Literal['png', 'pnm', 'pgm', 'ppm', 'pbm', 'pam', 'tga', 'tpic', 'psd', 'ps', 'jpg', 'jpeg']='png', jpg_quality: int=100, page_indices: Iterable[int] | int | None=None) -> dict[int, bytes]:
    """
    Convert PDF pages to images in bytes.

    Args:
        doc (Document): The PDF document to convert.
        zoom (float): Zoom factor for the image resolution. Default is 2.0.
        output (str): Output format for the image. Default is 'png'.
        jpg_quality (int): Quality of JPEG images (1-100). Default is 100.
        page_indices (Iterable[int] | int | None): Specific pages to convert. If None, all pages are converted.
            If an int is provided, only that page is converted.

    Returns:
        dict[int, bytes]: A dictionary mapping page numbers to image bytes.
    """
    from pymupdf import Matrix
    from pymupdf.utils import get_pixmap
    images_bytes: dict[int, bytes] = {}
    matrix = Matrix(zoom, zoom)
    for page_idx in _get_page_indices(page_indices, len(doc)):
        img_bytes = bytes(get_pixmap(page=doc[page_idx], matrix=matrix).tobytes(output=output, jpg_quality=jpg_quality))
        images_bytes[page_idx] = img_bytes
    return images_bytes

def extract_text_from_pdf(doc: 'Document', page_indices: Iterable[int] | int | None=None) -> dict[int, str]:
    """Convert a PDF file to plain text.

    Extracts text from each page of a PDF file and formats it with page markers.

    Args:
        doc (Document): The PDF document to convert.
        page_indices (Iterable[int] | int | None): Specific pages to convert. If None, all pages are converted.
            If an int is provided, only that page is converted.

    Returns:
        dict[int, str]: A dictionary mapping page numbers to text content.
    """
    return {page_idx: doc[page_idx].get_textpage().extractText().strip() for page_idx in _get_page_indices(page_indices, len(doc))}

@contextmanager
def open_pdf(pdf_input: PathOrReadable | Document):
    """Open a PDF document from a file path or use an existing Document object.

    Args:
        pdf_input (PathOrReadable | Document): The PDF file path or a pymupdf.Document object.

    Returns:
        tuple[Document, bool]: A tuple containing the opened Document object and a boolean indicating if it was opened internally.
    """
    import pymupdf
    should_close = True
    if isinstance(pdf_input, pymupdf.Document):
        should_close = False
        doc = pdf_input
    else:
        with read_bytes_stream(pdf_input) as stream:
            if stream is None:
                raise FileNotFoundError(pdf_input)
            doc = pymupdf.Document(stream=stream.read())
    yield doc
    if should_close:
        doc.close()

def _get_page_indices(page_indices: Iterable[int] | int | None, max_doc_pages: int) -> Iterable[int]:
    """Helper function to handle page indices for PDF conversion."""
    if page_indices is None:
        return range(max_doc_pages)
    elif isinstance(page_indices, int):
        return [page_indices]
    else:
        return [i for i in page_indices if 0 <= i < max_doc_pages]
```

```C:\Users\cosogi\chatterer\chatterer\tools\convert_to_text.py
import ast
import importlib
import os
import re
import site
from fnmatch import fnmatch
from pathlib import Path
from typing import TYPE_CHECKING, Callable, Iterable, NamedTuple, NotRequired, Optional, Self, Sequence, TypeAlias, TypedDict
from ..common_types.io import PathOrReadable
from ..utils.bytesio import read_bytes_stream
from .convert_pdf_to_markdown import extract_text_from_pdf
if TYPE_CHECKING:
    from bs4 import Tag
    from openai import OpenAI
    from requests import Response, Session
try:
    from tiktoken import get_encoding, list_encoding_names
    enc = get_encoding(list_encoding_names()[-1])
except ImportError:
    enc = None
type FileTree = dict[str, Optional[FileTree]]
CodeLanguageCallback: TypeAlias = Callable[['Tag'], Optional[str]]

class HtmlToMarkdownOptions(TypedDict):
    """
    TypedDict for options used in HTML to Markdown conversion.

    Contains various configuration options for controlling how HTML is converted to Markdown,
    including formatting preferences, escape behaviors, and styling options.
    """
    autolinks: NotRequired[bool]
    bullets: NotRequired[str]
    code_language: NotRequired[str]
    code_language_callback: NotRequired[CodeLanguageCallback]
    convert: NotRequired[Sequence[str]]
    default_title: NotRequired[bool]
    escape_asterisks: NotRequired[bool]
    escape_underscores: NotRequired[bool]
    escape_misc: NotRequired[bool]
    heading_style: NotRequired[str]
    keep_inline_images_in: NotRequired[Sequence[str]]
    newline_style: NotRequired[str]
    strip: NotRequired[Sequence[str]]
    strip_document: NotRequired[str]
    strong_em_symbol: NotRequired[str]
    sub_symbol: NotRequired[str]
    sup_symbol: NotRequired[str]
    table_infer_header: NotRequired[bool]
    wrap: NotRequired[bool]
    wrap_width: NotRequired[int]

def get_default_html_to_markdown_options() -> HtmlToMarkdownOptions:
    """
    Returns the default options for HTML to Markdown conversion.

    This function provides a set of sensible defaults for the markdownify library,
    including settings for bullets, escaping, heading styles, and other formatting options.

    Returns:
        HtmlToMarkdownOptions: A dictionary of default conversion options.
    """
    from markdownify import ASTERISK, SPACES, STRIP, UNDERLINED
    return {'autolinks': True, 'bullets': '*+-', 'code_language': '', 'default_title': False, 'escape_asterisks': True, 'escape_underscores': True, 'escape_misc': False, 'heading_style': UNDERLINED, 'keep_inline_images_in': [], 'newline_style': SPACES, 'strip_document': STRIP, 'strong_em_symbol': ASTERISK, 'sub_symbol': '', 'sup_symbol': '', 'table_infer_header': False, 'wrap': False, 'wrap_width': 80}

class CodeSnippets(NamedTuple):
    """
    A named tuple that represents code snippets extracted from Python files.

    Contains the paths to the files, the concatenated text of all snippets,
    and the base directory of the files.
    """
    paths: list[Path]
    snippets_text: str
    base_dir: Path

    @classmethod
    def from_path_or_pkgname(cls, path_or_pkgname: str, glob_patterns: str | list[str]='*.py', case_sensitive: bool=False, ban_file_patterns: Optional[list[str]]=None) -> Self:
        """
        Creates a CodeSnippets instance from a file path or package name.

        Args:
            path_or_pkgname: Path to a file/directory or a Python package name.
            ban_file_patterns: Optional list of patterns to exclude files.

        Returns:
            A new CodeSnippets instance with extracted code snippets.
        """
        paths: list[Path] = _get_filepaths(path_or_pkgname=path_or_pkgname, glob_patterns=glob_patterns, case_sensitive=case_sensitive, ban_fn_patterns=ban_file_patterns)
        snippets_text: str = ''.join((_get_a_snippet(p) for p in paths))
        return cls(paths=paths, snippets_text=snippets_text, base_dir=_get_base_dir(paths))

    @property
    def metadata(self) -> str:
        """
        Generates metadata about the code snippets.

        Returns a string containing information about the file tree structure,
        total number of files, tokens (if tiktoken is available), and lines.

        Returns:
            str: Formatted metadata string.
        """
        file_paths: list[Path] = self.paths
        text: str = self.snippets_text
        base_dir: Path = _get_base_dir(file_paths)
        results: list[str] = [base_dir.as_posix()]
        file_tree: FileTree = {}
        for file_path in sorted(file_paths):
            rel_path = file_path.relative_to(base_dir)
            subtree: Optional[FileTree] = file_tree
            for part in rel_path.parts[:-1]:
                if subtree is not None:
                    subtree = subtree.setdefault(part, {})
            if subtree is not None:
                subtree[rel_path.parts[-1]] = None

        def _display_tree(tree: FileTree, prefix: str='') -> None:
            """
            Helper function to recursively display a file tree structure.

            Args:
                tree: The file tree dictionary to display.
                prefix: Current line prefix for proper indentation.
            """
            items: list[tuple[str, Optional[FileTree]]] = sorted(tree.items())
            count: int = len(items)
            for idx, (name, subtree) in enumerate(items):
                branch: str = '└── ' if idx == count - 1 else '├── '
                results.append(f'{prefix}{branch}{name}')
                if subtree is not None:
                    extension: str = '    ' if idx == count - 1 else '│   '
                    _display_tree(tree=subtree, prefix=prefix + extension)
        _display_tree(file_tree)
        results.append(f'- Total files: {len(file_paths)}')
        if enc is not None:
            num_tokens: int = len(enc.encode(text, disallowed_special=()))
            results.append(f'- Total tokens: {num_tokens}')
        results.append(f"- Total lines: {text.count('\n') + 1}")
        return '\n'.join(results)

def html_to_markdown(html: str, options: Optional[HtmlToMarkdownOptions]) -> str:
    """
    Convert HTML content to Markdown using the provided options.

    Args:
        html (str): HTML content to convert.
        options (HtmlToMarkdownOptions): Options for the conversion.

    Returns:
        str: The Markdown content.
    """
    from markdownify import markdownify
    return str(markdownify(html, **options or {}))

def pdf_to_text(path_or_file: PathOrReadable, page_indices: Iterable[int] | int | None=None) -> str:
    """
    Convert a PDF file to plain text.

    Extracts text from each page of a PDF file and formats it with page markers.

    Args:
        path_or_file: Path to a PDF file or a readable object containing PDF data.
        page_indices: Optional list of page indices to extract. If None, all pages are extracted.
            If an integer is provided, it extracts that specific page.
            If a list is provided, it extracts the specified pages.

    Returns:
        str: Extracted text with page markers.

    Raises:
        FileNotFoundError: If the file cannot be found or opened.
    """
    from pymupdf import Document
    with read_bytes_stream(path_or_file) as stream:
        if stream is None:
            raise FileNotFoundError(path_or_file)
        with Document(stream=stream.read()) as doc:
            return '\n'.join((f'<!-- Page {page_no} -->\n{text}\n' for page_no, text in extract_text_from_pdf(doc, page_indices).items()))

def anything_to_markdown(source: 'str | Response | Path', requests_session: Optional['Session']=None, llm_client: Optional['OpenAI']=None, llm_model: Optional[str]=None, style_map: Optional[str]=None, exiftool_path: Optional[str]=None, docintel_endpoint: Optional[str]=None) -> str:
    """
    Convert various types of content to Markdown format.

    Uses the MarkItDown library to convert different types of content (URLs, files, API responses)
    to Markdown format.

    Args:
        source: The source content to convert (URL string, Response object, or Path).
        requests_session: Optional requests Session for HTTP requests.
        llm_client: Optional OpenAI client for LLM-based conversions.
        llm_model: Optional model name for the LLM.
        style_map: Optional style mapping configuration.
        exiftool_path: Optional path to exiftool for metadata extraction.
        docintel_endpoint: Optional Document Intelligence API endpoint.

    Returns:
        str: The converted Markdown content.
    """
    from markitdown import MarkItDown
    result = MarkItDown(requests_session=requests_session, llm_client=llm_client, llm_model=llm_model, style_map=style_map, exiftool_path=exiftool_path, docintel_endpoint=docintel_endpoint).convert(source)
    return result.text_content
pyscripts_to_snippets = CodeSnippets.from_path_or_pkgname

def _pattern_to_regex(pattern: str) -> re.Pattern[str]:
    """
    Converts an fnmatch pattern to a regular expression.

    In this function, '**' is converted to match any character including directory separators.
    The remaining '*' matches any character except directory separators, and '?' matches a single character.

    Args:
        pattern: The fnmatch pattern to convert.

    Returns:
        A compiled regular expression pattern.
    """
    pattern = re.escape(pattern)
    pattern = pattern.replace('\\*\\*', '.*')
    pattern = pattern.replace('\\*', '[^/]*')
    pattern = pattern.replace('\\?', '.')
    pattern = '^' + pattern + '$'
    return re.compile(pattern)

def _is_banned(p: Path, ban_patterns: list[str]) -> bool:
    """
    Checks if a given path matches any of the ban patterns.

    Determines if the path p matches any pattern in ban_patterns using either
    fnmatch-based or recursive patterns (i.e., containing '**').

    Note: Patterns should use POSIX-style paths (i.e., '/' separators).

    Args:
        p: The path to check.
        ban_patterns: List of patterns to match against.

    Returns:
        bool: True if the path matches any ban pattern, False otherwise.
    """
    p_str = p.as_posix()
    for pattern in ban_patterns:
        if '**' in pattern:
            regex = _pattern_to_regex(pattern)
            if regex.match(p_str):
                return True
        elif fnmatch(p_str, pattern):
            return True
    return False

def _get_a_snippet(fpath: Path) -> str:
    """
    Extracts a code snippet from a Python file.

    Reads the file, parses it as Python code, and returns a formatted code snippet
    with the relative path as a header in markdown code block format.

    Args:
        fpath: Path to the Python file.

    Returns:
        str: Formatted code snippet or empty string if the file doesn't exist.
    """
    if not fpath.is_file():
        return ''
    cleaned_code: str = '\n'.join((line for line in ast.unparse(ast.parse(fpath.read_text(encoding='utf-8'))).splitlines()))
    if (site_dir := next((d for d in reversed(site.getsitepackages()) if fpath.is_relative_to(d)), None)):
        display_path = fpath.relative_to(site_dir)
    elif fpath.is_relative_to((cwd := Path.cwd())):
        display_path = fpath.relative_to(cwd)
    else:
        display_path = fpath.absolute()
    return f'```{display_path}\n{cleaned_code}\n```\n\n'

def _get_base_dir(target_files: Sequence[Path]) -> Path:
    """
    Determines the common base directory for a sequence of file paths.

    Finds the directory with the shortest path that is a parent to at least one file.

    Args:
        target_files: Sequence of file paths.

    Returns:
        Path: The common base directory.
    """
    return Path(os.path.commonpath(target_files))

def _get_filepaths(path_or_pkgname: str, glob_patterns: str | list[str]='*.py', case_sensitive: bool=False, ban_fn_patterns: Optional[list[str]]=None) -> list[Path]:
    """
    Gets paths to files from a directory, file, or Python package name.

    If path_or_pkgname is a directory, finds all `glob_pattern` matching files recursively.
    If it's a file, returns just that file.
    If it's a package name, imports the package and finds all .py files in its directory.

    Args:
        path_or_pkgname: Path to directory/file or package name.
        glob_pattern: Pattern to match files.
        case_sensitive: Whether to match files case-sensitively.
        ban_fn_patterns: Optional list of patterns to exclude files.

    Returns:
        list[Path]: List of paths to Python files.
    """
    path = Path(path_or_pkgname)
    pypaths: list[Path]
    if path.is_dir():
        glob_patterns = glob_patterns if isinstance(glob_patterns, (tuple, list)) else [glob_patterns]
        pypaths = []
        for pattern in glob_patterns:
            if '**' in pattern:
                regex = _pattern_to_regex(pattern)
                pypaths.extend((p for p in path.rglob('**/*', case_sensitive=case_sensitive) if regex.match(p.as_posix())))
            else:
                pypaths += list(path.rglob(pattern, case_sensitive=case_sensitive))
    elif path.is_file():
        pypaths = [path]
    else:
        pypaths = [p for p in Path(next(iter(importlib.import_module(path_or_pkgname).__path__))).rglob('*.py', case_sensitive=False) if p.is_file()]
    return [p for p in pypaths if not ban_fn_patterns or not _is_banned(p, ban_fn_patterns)]
```

```C:\Users\cosogi\chatterer\chatterer\tools\upstage_document_parser.py
"""Adopted from `langchain_upstage.document_parse`"""
from __future__ import annotations
import base64
import binascii
import io
import json
import logging
import os
import uuid
from typing import TYPE_CHECKING, Dict, Iterator, Literal, Optional, TypedDict, cast
import requests
from langchain_core.document_loaders import BaseBlobParser, Blob
from langchain_core.documents import Document
from pydantic import BaseModel, Field
from ..common_types.io import BytesReadable
from ..language_model import DEFAULT_IMAGE_DESCRIPTION_INSTRUCTION, Chatterer
from ..utils.base64_image import Base64Image
from ..utils.imghdr import what
if TYPE_CHECKING:
    from pypdf import PdfReader
logger = logging.getLogger('pypdf')
logger.setLevel(logging.ERROR)
parser_logger = logging.getLogger(__name__)
DOCUMENT_PARSE_BASE_URL = 'https://api.upstage.ai/v1/document-ai/document-parse'
DEFAULT_NUM_PAGES = 10
DOCUMENT_PARSE_DEFAULT_MODEL = 'document-parse'
DEFAULT_IMAGE_DIR = 'images'
OutputFormat = Literal['text', 'html', 'markdown']
OCR = Literal['auto', 'force']
SplitType = Literal['none', 'page', 'element']
Category = Literal['paragraph', 'table', 'figure', 'header', 'footer', 'caption', 'equation', 'heading1', 'list', 'index', 'footnote', 'chart']

class Content(BaseModel):
    text: Optional[str] = None
    html: Optional[str] = None
    markdown: Optional[str] = None

class Coordinate(BaseModel):
    x: float
    y: float

class Element(BaseModel):
    category: Category
    content: Content
    coordinates: list[Coordinate] = Field(default_factory=list)
    base64_encoding: str = ''
    id: int
    page: int

    def parse_text(self, parser: 'UpstageDocumentParseParser') -> str:
        """
        Generates the text representation of the element.

        If the element is a figure with base64 encoding and no chatterer is provided,
        it generates a markdown link to a uniquely named image file and stores the
        image data in the parser's image_data dictionary. Otherwise, it uses the
        chatterer for description or returns the standard text/html/markdown.
        """
        output_format: OutputFormat = parser.output_format
        chatterer: Optional[Chatterer] = parser.chatterer
        image_description_instruction: str = parser.image_description_instruction
        output: Optional[str] = None
        if output_format == 'text':
            output = self.content.text
        elif output_format == 'html':
            output = self.content.html
        elif output_format == 'markdown':
            output = self.content.markdown
        if output is None:
            output = self.content.text or ''
        if self.category == 'figure' and self.base64_encoding:
            if chatterer is not None:
                try:
                    img_type = what(self.base64_encoding)
                    if not img_type:
                        parser_logger.warning(f'Could not determine image type for figure element {self.id} (page {self.page}).')
                        return output
                    image = Base64Image.from_string(f'data:image/{img_type};base64,{self.base64_encoding}')
                except (binascii.Error, ValueError) as e:
                    parser_logger.warning(f'Could not decode base64 for figure element {self.id} (page {self.page}): {e}. Falling back to original output.')
                    return output
                if image is None:
                    parser_logger.warning(f'Invalid base64 encoding format for image element {self.id}, cannot create Base64Image object.')
                    return output
                ocr_content = ''
                if output_format == 'markdown':
                    ocr_content = output.removeprefix('![image](/image/placeholder)\n')
                elif output_format == 'text':
                    ocr_content = output
                image_description = chatterer.describe_image(image.data_uri, image_description_instruction + f'\nHint: The OCR detected the following text:\n```\n{ocr_content}\n```')
                output = f'\n\n<details>\n<summary>Image Description</summary>\n{image_description}\n</details>\n\n'
            elif parser.image_dir is not None:
                try:
                    img_type = what(self.base64_encoding)
                    if not img_type:
                        parser_logger.warning(f'Could not determine image type for figure element {self.id} (page {self.page}).')
                        return output
                    image_bytes = base64.b64decode(self.base64_encoding)
                    filename = f'{uuid.uuid4().hex}.{img_type}'
                    relative_path = os.path.join(parser.image_dir, filename).replace('\\', '/')
                    parser.image_data[relative_path] = image_bytes
                    ocr_content = ''
                    if output_format == 'markdown' and output.startswith('![image]'):
                        ocr_content = output.split('\n', 1)[1] if '\n' in output else ''
                    elif output_format == 'text':
                        ocr_content = output
                    output = f'![image]({relative_path})\n{ocr_content}'.strip()
                except (binascii.Error, ValueError) as e:
                    parser_logger.warning(f'Could not decode base64 for figure element {self.id} (page {self.page}): {e}. Falling back to original output.')
                    pass
        return output

class Coordinates(TypedDict):
    id: int
    category: Category
    coordinates: list[Coordinate]

class PageCoordinates(Coordinates):
    page: int

def get_from_param_or_env(key: str, param: Optional[str]=None, env_key: Optional[str]=None, default: Optional[str]=None) -> str:
    """Get a value from a param or an environment variable."""
    if param is not None:
        return param
    elif env_key and env_key in os.environ and os.environ[env_key]:
        return os.environ[env_key]
    elif default is not None:
        return default
    else:
        raise ValueError(f'Did not find {key}, please add an environment variable `{env_key}` which contains it, or pass `{key}` as a named parameter.')

class UpstageDocumentParseParser(BaseBlobParser):
    """Upstage Document Parse Parser.

    Parses documents using the Upstage Document AI API. Can optionally extract
    images and return their data alongside the parsed documents.

    If a `chatterer` is provided, it will be used to generate descriptions for
    images (figures with base64 encoding).

    If `chatterer` is NOT provided, for figure elements with `base64_encoding`,
    this parser will:
    1. Generate a unique relative file path (e.g., "images/uuid.jpeg").
       The base directory can be configured with `image_dir`.
    2. Replace the element's content with a markdown image link pointing to this path.
    3. Store the actual image bytes in the `image_data` attribute dictionary,
       mapping the generated relative path to the bytes.

    The user is responsible for saving the files from the `image_data` dictionary
    after processing the documents yielded by `lazy_parse`.

    To use, you should have the environment variable `UPSTAGE_API_KEY`
    set with your API key or pass it as a named parameter to the constructor.

    Example:
        .. code-block:: python

            from langchain_upstage import UpstageDocumentParseParser
            from langchain_core.documents import Blob
            import os

            # --- Setup ---
            # Ensure UPSTAGE_API_KEY is set in environment or passed as api_key
            # Create a dummy PDF or image file 'my_document.pdf' / 'my_image.png'

            # --- Parsing without chatterer (extracts images) ---
            parser = UpstageDocumentParseParser(
                split="page",
                output_format="markdown",
                base64_encoding=["figure"], # Important: Request base64 for figures
                image_dir="extracted_images" # Optional: specify image dir
            )
            blob = Blob.from_path("my_document.pdf") # Or your image file path
            documents = []
            for doc in parser.lazy_parse(blob):
                print("--- Document ---")
                print(f"Page: {get_metadata_from_document(doc).get('page')}")
                print(doc.page_content)
                documents.append(doc)

            print("\\n--- Extracted Image Data ---")
            if parser.image_data:
                # User saves the images
                for img_path, img_bytes in parser.image_data.items():
                    # Create directories if they don't exist
                    os.makedirs(os.path.dirname(img_path), exist_ok=True)
                    try:
                        with open(img_path, "wb") as f:
                            f.write(img_bytes)
                        print(f"Saved image: {img_path}")
                    except IOError as e:
                        print(f"Error saving image {img_path}: {e}")
            else:
                print("No images extracted.")

            # --- Parsing with chatterer (generates descriptions) ---
            # from langchain_upstage import UpstageChatter # Assuming this exists
            # chatterer = UpstageChatter() # Initialize your chatterer
            # parser_with_desc = UpstageDocumentParseParser(
            #     split="page",
            #     output_format="markdown",
            #     base64_encoding=["figure"], # Still need base64 for description
            #     chatterer=chatterer
            # )
            # documents_with_desc = list(parser_with_desc.lazy_parse(blob))
            # print("\\n--- Documents with Descriptions ---")
            # for doc in documents_with_desc:
            #     print(f"Page: {get_metadata_from_document(doc).get('page')}")
            #     print(doc.page_content)

    """

    def __init__(self, api_key: Optional[str]=None, base_url: str=DOCUMENT_PARSE_BASE_URL, model: str=DOCUMENT_PARSE_DEFAULT_MODEL, split: SplitType='none', ocr: OCR='auto', output_format: OutputFormat='markdown', coordinates: bool=True, base64_encoding: list[Category]=[], chatterer: Optional[Chatterer]=None, image_description_instruction: str=DEFAULT_IMAGE_DESCRIPTION_INSTRUCTION, image_dir: Optional[str]=None) -> None:
        """
        Initializes an instance of the UpstageDocumentParseParser.

        Args:
            api_key (str, optional): Upstage API key. Defaults to env `UPSTAGE_API_KEY`.
            base_url (str, optional): Base URL for the Upstage API.
            model (str): Model for document parse. Defaults to "document-parse".
            split (SplitType, optional): Splitting type ("none", "page", "element").
                                          Defaults to "none".
            ocr (OCR, optional): OCR mode ("auto", "force"). Defaults to "auto".
            output_format (OutputFormat, optional): Output format ("text", "html", "markdown").
                                                     Defaults to "markdown".
            coordinates (bool, optional): Include coordinates in metadata. Defaults to True.
            base64_encoding (List[Category], optional): Categories to return as base64.
                                                       Crucial for image extraction/description.
                                                       Set to `["figure"]` to process images.
                                                       Defaults to [].
            chatterer (Chatterer, optional): Chatterer instance for image description.
                                             If None, images will be extracted to files.
                                             Defaults to None.
            image_description_instruction (str, optional): Instruction for image description.
                                                            Defaults to a standard instruction.
            image_dir (str, optional): The directory name to use when constructing
                                        relative paths for extracted images.
                                        Defaults to "images". This directory
                                        is NOT created by the parser.
        """
        self.api_key = get_from_param_or_env('UPSTAGE_API_KEY', api_key, 'UPSTAGE_API_KEY', os.environ.get('UPSTAGE_API_KEY'))
        self.base_url = base_url
        self.model = model
        self.split: SplitType = split
        self.ocr: OCR = ocr
        self.output_format: OutputFormat = output_format
        self.coordinates = coordinates
        self.base64_encoding: list[Category] = base64_encoding
        self.chatterer = chatterer
        self.image_description_instruction = image_description_instruction
        self.image_dir = image_dir
        self.image_data: Dict[str, bytes] = {}

    def _get_response(self, files: dict[str, tuple[str, BytesReadable]]) -> list[Element]:
        """
        Sends a POST request to the API endpoint with the provided files and
        returns the parsed elements.
        """
        response: Optional[requests.Response] = None
        try:
            headers = {'Authorization': f'Bearer {self.api_key}'}
            base64_encoding_str = str(self.base64_encoding) if self.base64_encoding else '[]'
            output_formats_str = f"['{self.output_format}']"
            response = requests.post(self.base_url, headers=headers, files=files, data={'ocr': self.ocr, 'model': self.model, 'output_formats': output_formats_str, 'coordinates': str(self.coordinates).lower(), 'base64_encoding': base64_encoding_str})
            response.raise_for_status()
            content_type = response.headers.get('Content-Type', '')
            if 'application/json' not in content_type:
                raise ValueError(f'Unexpected content type: {content_type}. Response body: {response.text}')
            response_data = response.json()
            result: object = response_data.get('elements', [])
            if not isinstance(result, list):
                raise ValueError(f"API response 'elements' is not a list: {result}")
            result = cast(list[object], result)
            validated_elements: list[Element] = []
            for i, element_data in enumerate(result):
                try:
                    validated_elements.append(Element.model_validate(element_data))
                except Exception as e:
                    parser_logger.error(f'Failed to validate element {i}: {element_data}. Error: {e}')
                    raise ValueError(f'Failed to validate element {i}: {e}') from e
            return validated_elements
        except requests.HTTPError as e:
            error_message = f'HTTP error: {e.response.status_code} {e.response.reason}'
            try:
                error_details = e.response.json()
                error_message += f' - {error_details}'
            except json.JSONDecodeError:
                error_message += f' - Response body: {e.response.text}'
            raise ValueError(error_message) from e
        except requests.RequestException as e:
            raise ValueError(f'Failed to send request: {e}') from e
        except json.JSONDecodeError as e:
            raise ValueError(f"Failed to decode JSON response: {e}. Response text starts with: {(response.text[:200] if response else 'No response')}") from e
        except Exception as e:
            raise ValueError(f'An unexpected error occurred during API call: {e}') from e

    def _split_and_request(self, full_docs: PdfReader, start_page: int, num_pages: int=DEFAULT_NUM_PAGES) -> list[Element]:
        """
        Splits the full pdf document into partial pages and sends a request.
        """
        try:
            from pypdf import PdfWriter
        except ImportError:
            raise ImportError('pypdf is required for PDF splitting. Please install it with `pip install pypdf`.')
        merger = PdfWriter()
        total_pages = len(full_docs.pages)
        end_page = min(start_page + num_pages, total_pages)
        if start_page >= total_pages:
            parser_logger.warning(f'Start page {start_page} is out of bounds for document with {total_pages} pages.')
            return []
        for i in range(start_page, end_page):
            merger.add_page(full_docs.pages[i])
        with io.BytesIO() as buffer:
            merger.write(buffer)
            buffer.seek(0)
            return self._get_response({'document': ('partial_doc.pdf', buffer)})

    def _element_document(self, element: Element, start_page: int=0) -> Document:
        """Converts an element into a Document object."""
        page_content = element.parse_text(self)
        metadata: dict[str, object] = element.model_dump(exclude={'content', 'base64_encoding'}, exclude_none=True)
        metadata['page'] = element.page + start_page
        if not self.coordinates:
            metadata.pop('coordinates', None)
        return Document(page_content=page_content, metadata=metadata)

    def _page_document(self, elements: list[Element], start_page: int=0) -> list[Document]:
        """Combines elements with the same page number into a single Document object."""
        documents: list[Document] = []
        if not elements:
            return documents
        pages: list[int] = sorted(list(set(map(lambda x: x.page, elements))))
        page_groups: Dict[int, list[Element]] = {page: [] for page in pages}
        for element in elements:
            page_groups[element.page].append(element)
        for page_num, group in page_groups.items():
            actual_page_num = page_num + start_page
            page_content_parts: list[str] = []
            page_coordinates: list[Coordinates] = []
            for element in sorted(group, key=lambda x: x.id):
                page_content_parts.append(element.parse_text(self))
                if self.coordinates and element.coordinates:
                    page_coordinates.append({'id': element.id, 'category': element.category, 'coordinates': element.coordinates})
            metadata: dict[str, object] = {'page': actual_page_num}
            if self.coordinates and page_coordinates:
                metadata['element_coordinates'] = page_coordinates
            combined_page_content = '\n\n'.join((part for part in page_content_parts if part))
            documents.append(Document(page_content=combined_page_content, metadata=metadata))
        return documents

    def lazy_parse(self, blob: Blob, is_batch: bool=False) -> Iterator[Document]:
        """
        Lazily parses a document blob.

        Yields Document objects based on the specified split type.
        If images are extracted (chatterer=None, base64_encoding=["figure"]),
        the image data will be available in `self.image_data` after iteration.

        Args:
            blob (Blob): The input document blob to parse. Requires `blob.path`.
            is_batch (bool, optional): Currently affects PDF page batch size.
                                       Defaults to False (process 1 page batch for PDF).
                                       *Note: API might have limits regardless.*

        Yields:
            Document: The parsed document object(s).

        Raises:
            ValueError: If blob.path is not set, API error occurs, or invalid config.
            ImportError: If pypdf is needed but not installed.
        """
        self.image_data = {}
        if not blob.path:
            raise ValueError('Blob path is required for UpstageDocumentParseParser.')
        PdfReader = None
        PdfReadError = None
        try:
            from pypdf import PdfReader as PyPdfReader
            from pypdf.errors import PdfReadError as PyPdfReadError
            PdfReader = PyPdfReader
            PdfReadError = PyPdfReadError
        except ImportError:
            pass
        full_docs: Optional[PdfReader] = None
        is_pdf = False
        number_of_pages = 1
        try:
            if PdfReader and PdfReadError:
                try:
                    full_docs = PdfReader(str(blob.path), strict=False)
                    number_of_pages = len(full_docs.pages)
                    is_pdf = True
                except (PdfReadError, FileNotFoundError, IsADirectoryError) as e:
                    parser_logger.warning(f"Could not read '{blob.path}' as PDF: {e}. Assuming non-PDF format.")
                except Exception as e:
                    parser_logger.error(f"Unexpected error reading PDF '{blob.path}': {e}")
                    raise ValueError(f'Failed to process PDF file: {e}') from e
            else:
                parser_logger.info('pypdf not installed. Treating input as a single non-PDF document for the API.')
        except Exception as e:
            raise ValueError(f'Failed to access or identify file type for: {blob.path}. Error: {e}') from e
        if self.split == 'none':
            combined_result = ''
            all_coordinates: list[PageCoordinates] = []
            if is_pdf and full_docs and PdfReader:
                start_page = 0
                batch_num_pages = DEFAULT_NUM_PAGES
                while start_page < number_of_pages:
                    elements = self._split_and_request(full_docs, start_page, batch_num_pages)
                    for element in sorted(elements, key=lambda x: (x.page, x.id)):
                        combined_result += element.parse_text(self) + '\n\n'
                        if self.coordinates and element.coordinates:
                            coords_with_page: PageCoordinates = {'id': element.id, 'category': element.category, 'page': element.page + start_page, 'coordinates': element.coordinates}
                            all_coordinates.append(coords_with_page)
                    start_page += batch_num_pages
            else:
                with open(blob.path, 'rb') as f:
                    filename = os.path.basename(blob.path)
                    elements = self._get_response({'document': (filename, f)})
                for element in sorted(elements, key=lambda x: x.id):
                    combined_result += element.parse_text(self) + '\n\n'
                    if self.coordinates and element.coordinates:
                        all_coordinates.append({'id': element.id, 'category': element.category, 'page': element.page, 'coordinates': element.coordinates})
            metadata: dict[str, object] = {'source': blob.path, 'total_pages': number_of_pages}
            if self.coordinates and all_coordinates:
                metadata['element_coordinates'] = all_coordinates
            yield Document(page_content=combined_result.strip(), metadata=metadata)
        elif self.split == 'element':
            if is_pdf and full_docs and PdfReader:
                start_page = 0
                batch_num_pages = DEFAULT_NUM_PAGES if is_batch else 1
                while start_page < number_of_pages:
                    elements = self._split_and_request(full_docs, start_page, batch_num_pages)
                    for element in sorted(elements, key=lambda x: (x.page, x.id)):
                        doc = self._element_document(element, start_page)
                        _get_metadata_from_document(doc)['source'] = blob.path
                        yield doc
                    start_page += batch_num_pages
            else:
                with open(blob.path, 'rb') as f:
                    filename = os.path.basename(blob.path)
                    elements = self._get_response({'document': (filename, f)})
                for element in sorted(elements, key=lambda x: x.id):
                    doc = self._element_document(element, 0)
                    _get_metadata_from_document(doc)['source'] = blob.path
                    yield doc
        elif self.split == 'page':
            if is_pdf and full_docs and PdfReader:
                start_page = 0
                batch_num_pages = DEFAULT_NUM_PAGES if is_batch else 1
                while start_page < number_of_pages:
                    elements = self._split_and_request(full_docs, start_page, batch_num_pages)
                    page_docs = self._page_document(elements, start_page)
                    for doc in page_docs:
                        _get_metadata_from_document(doc)['source'] = blob.path
                        yield doc
                    start_page += batch_num_pages
            else:
                with open(blob.path, 'rb') as f:
                    filename = os.path.basename(blob.path)
                    elements = self._get_response({'document': (filename, f)})
                page_docs = self._page_document(elements, 0)
                for doc in page_docs:
                    _get_metadata_from_document(doc)['source'] = blob.path
                    yield doc
        else:
            raise ValueError(f'Invalid split type: {self.split}')

def _get_metadata_from_document(doc: Document) -> dict[object, object]:
    """
    Helper function to extract metadata from a Document object.
    This is a placeholder and should be adjusted based on actual metadata structure.
    """
    metadata: dict[object, object] = doc.metadata
    return metadata
```

```C:\Users\cosogi\chatterer\chatterer\tools\webpage_to_markdown.py
"""
PlaywrightBot

This module provides a single class that uses Playwright to:
  - Fetch and render HTML pages (with JavaScript execution),
  - Optionally scroll down or reload pages,
  - Convert rendered HTML into Markdown,
  - Extract specific elements using CSS selectors,
  - Filter key information from a page via integration with a language model (Chatterer).

Both synchronous and asynchronous methods are available in this unified class.
Use the synchronous methods (without the "a" prefix) in a normal context manager,
or use the asynchronous methods (prefixed with "a") within an async context manager.
"""
from __future__ import annotations
from dataclasses import dataclass, field
from pathlib import Path
from types import TracebackType
from typing import TYPE_CHECKING, Literal, NotRequired, Optional, Self, Sequence, Type, TypeAlias, TypedDict, Union
from pydantic import BaseModel, Field
from ..language_model import DEFAULT_IMAGE_DESCRIPTION_INSTRUCTION, Chatterer
from ..utils.base64_image import ImageProcessingConfig, get_default_image_processing_config, is_remote_url
from .caption_markdown_images import acaption_markdown_images, caption_markdown_images
from .convert_to_text import HtmlToMarkdownOptions, get_default_html_to_markdown_options, html_to_markdown
if TYPE_CHECKING:
    import playwright.async_api
    import playwright.sync_api
WaitUntil: TypeAlias = Literal['commit', 'domcontentloaded', 'load', 'networkidle']
DEFAULT_UA: str = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36'

class SelectedLineRanges(BaseModel):
    line_ranges: list[str] = Field(description="List of inclusive line ranges, e.g., ['1-3', '5-5', '7-10']")

class PlaywrightLaunchOptions(TypedDict):
    executable_path: NotRequired[str | Path]
    channel: NotRequired[str]
    args: NotRequired[Sequence[str]]
    ignore_default_args: NotRequired[bool | Sequence[str]]
    handle_sigint: NotRequired[bool]
    handle_sigterm: NotRequired[bool]
    handle_sighup: NotRequired[bool]
    timeout: NotRequired[float]
    env: NotRequired[dict[str, str | float | bool]]
    headless: NotRequired[bool]
    devtools: NotRequired[bool]
    proxy: NotRequired[playwright.sync_api.ProxySettings]
    downloads_path: NotRequired[str | Path]
    slow_mo: NotRequired[float]
    traces_dir: NotRequired[str | Path]
    chromium_sandbox: NotRequired[bool]
    firefox_user_prefs: NotRequired[dict[str, str | float | bool]]

class PlaywrightPersistencyOptions(TypedDict):
    user_data_dir: NotRequired[str | Path]
    storage_state: NotRequired[playwright.sync_api.StorageState]

class PlaywrightOptions(PlaywrightLaunchOptions, PlaywrightPersistencyOptions):
    ...

def get_default_playwright_launch_options() -> PlaywrightLaunchOptions:
    return {'headless': True}

@dataclass
class PlayWrightBot:
    """
    A unified bot that leverages Playwright to render web pages, convert them to Markdown,
    extract elements, and filter key information using a language model.

    This class exposes both synchronous and asynchronous methods.

    Synchronous usage:
      with UnifiedPlaywrightBot() as bot:
          md = bot.url_to_md("https://example.com")
          headings = bot.select_and_extract("https://example.com", "h2")
          filtered_md = bot.url_to_md_with_llm("https://example.com")

    Asynchronous usage:
      async with UnifiedPlaywrightBot() as bot:
          md = await bot.aurl_to_md("https://example.com")
          headings = await bot.aselect_and_extract("https://example.com", "h2")
          filtered_md = await bot.aurl_to_md_with_llm("https://example.com")

    Attributes:
        headless (bool): Whether to run the browser in headless mode (default True).
        chatterer (Chatterer): An instance of the language model interface for processing text.
    """
    engine: Literal['firefox', 'chromium', 'webkit'] = 'firefox'
    chatterer: Optional[Chatterer] = field(default_factory=Chatterer.openai)
    playwright_launch_options: PlaywrightLaunchOptions = field(default_factory=get_default_playwright_launch_options)
    playwright_persistency_options: PlaywrightPersistencyOptions = field(default_factory=PlaywrightPersistencyOptions)
    html_to_markdown_options: HtmlToMarkdownOptions = field(default_factory=get_default_html_to_markdown_options)
    image_processing_config: ImageProcessingConfig = field(default_factory=get_default_image_processing_config)
    headers: dict[str, str] = field(default_factory=lambda: {'User-Agent': DEFAULT_UA})
    markdown_filtering_instruction: str = "You are a web parser bot, an AI agent that filters out redundant fields from a webpage.\n\nYou excel at the following tasks:\n1. Identifying the main article content of a webpage.\n2. Filtering out ads, navigation links, and other irrelevant information.\n3. Selecting the line number ranges that correspond to the article content.\n4. Providing these inclusive ranges in the format 'start-end' or 'single_line_number'.\n\nHowever, there are a few rules you must follow:\n1. Do not remove the title of the article, if present.\n2. Do not remove the author's name or the publication date, if present.\n3. Include only images that are part of the article.\n\nNow, return a valid JSON object, for example: {'line_ranges': ['1-3', '5-5', '7-10']}.\n\nMarkdown-formatted webpage content is provided below for your reference:\n---\n".strip()
    description_format: str = "<details><summary>{image_summary}</summary><img src='{url}' alt='{inline_text}'></details>"
    image_description_instruction: str = DEFAULT_IMAGE_DESCRIPTION_INSTRUCTION
    sync_playwright: Optional[playwright.sync_api.Playwright] = None
    sync_browser_context: Optional[playwright.sync_api.BrowserContext] = None
    async_playwright: Optional[playwright.async_api.Playwright] = None
    async_browser_context: Optional[playwright.async_api.BrowserContext] = None

    def get_sync_playwright(self) -> playwright.sync_api.Playwright:
        if self.sync_playwright is None:
            from playwright.sync_api import sync_playwright
            self.sync_playwright = sync_playwright().start()
        return self.sync_playwright

    async def get_async_playwright(self) -> playwright.async_api.Playwright:
        if self.async_playwright is None:
            from playwright.async_api import async_playwright
            self.async_playwright = await async_playwright().start()
        return self.async_playwright

    def get_sync_browser(self) -> playwright.sync_api.BrowserContext:
        if self.sync_browser_context is not None:
            return self.sync_browser_context

        def get_browser() -> playwright.sync_api.BrowserType:
            playwright = self.get_sync_playwright()
            if self.engine == 'firefox':
                return playwright.firefox
            elif self.engine == 'chromium':
                return playwright.chromium
            elif self.engine == 'webkit':
                return playwright.webkit
            else:
                raise ValueError(f'Unsupported engine: {self.engine}')
        user_data_dir = self.playwright_persistency_options.get('user_data_dir')
        if user_data_dir:
            self.sync_browser_context = get_browser().launch_persistent_context(user_data_dir=user_data_dir, **self.playwright_launch_options)
            return self.sync_browser_context
        browser = get_browser().launch(**self.playwright_launch_options)
        storage_state = self.playwright_persistency_options.get('storage_state')
        if storage_state:
            self.sync_browser_context = browser.new_context(storage_state=storage_state)
        else:
            self.sync_browser_context = browser.new_context()
        return self.sync_browser_context

    async def get_async_browser(self) -> playwright.async_api.BrowserContext:
        if self.async_browser_context is not None:
            return self.async_browser_context

        async def get_browser() -> playwright.async_api.BrowserType:
            playwright = await self.get_async_playwright()
            if self.engine == 'firefox':
                return playwright.firefox
            elif self.engine == 'chromium':
                return playwright.chromium
            elif self.engine == 'webkit':
                return playwright.webkit
            else:
                raise ValueError(f'Unsupported engine: {self.engine}')
        user_data_dir = self.playwright_persistency_options.get('user_data_dir')
        if user_data_dir:
            self.async_browser_context = await (await get_browser()).launch_persistent_context(user_data_dir=user_data_dir, **self.playwright_launch_options)
            return self.async_browser_context
        browser = await (await get_browser()).launch(**self.playwright_launch_options)
        storage_state = self.playwright_persistency_options.get('storage_state')
        if storage_state:
            self.async_browser_context = await browser.new_context(storage_state=storage_state)
        else:
            self.async_browser_context = await browser.new_context()
        return self.async_browser_context

    def get_page(self, url: str, timeout: float=10.0, wait_until: Optional[WaitUntil]='domcontentloaded', referer: Optional[str]=None) -> playwright.sync_api.Page:
        """
        Create a new page and navigate to the given URL synchronously.

        Args:
            url (str): URL to navigate to.
            timeout (float): Maximum navigation time in seconds.
            wait_until (str): Load state to wait for (e.g., "domcontentloaded").
            referer (Optional[str]): Referer URL to set.

        Returns:
            Page: The Playwright page object.
        """
        page = self.get_sync_browser().new_page()
        page.goto(url, timeout=int(timeout * 1000), wait_until=wait_until, referer=referer)
        return page

    async def aget_page(self, url: str, timeout: float=8, wait_until: Optional[WaitUntil]='domcontentloaded', referer: Optional[str]=None) -> playwright.async_api.Page:
        """
        Create a new page and navigate to the given URL asynchronously.

        Args:
            url (str): URL to navigate to.
            timeout (float): Maximum navigation time in seconds.
            wait_until (str): Load state to wait for.
            referer (Optional[str]): Referer URL to set.

        Returns:
            AsyncPage: The Playwright asynchronous page object.
        """
        page = await (await self.get_async_browser()).new_page()
        await page.goto(url, timeout=int(timeout * 1000), wait_until=wait_until, referer=referer)
        return page

    def url_to_md(self, url: str, wait: float=0.2, scrolldown: bool=False, sleep: int=0, reload: bool=True, timeout: Union[float, int]=8, keep_page: bool=False, referer: Optional[str]=None) -> str:
        """
        Navigate to a URL, optionally wait, scroll, or reload the page, and convert the rendered HTML to Markdown.

        Args:
            url (str): URL of the page.
            wait (float): Time to wait after navigation (in seconds).
            scrolldown (bool): If True, scroll to the bottom of the page.
            sleep (int): Time to wait after scrolling (in seconds).
            reload (bool): If True, reload the page.
            timeout (float | int): Navigation timeout in seconds.
            keep_page (bool): If True, do not close the page after processing.
            referer (Optional[str]): Referer URL to set.

        Returns:
            str: The page content converted to Markdown.
        """
        page: Optional[playwright.sync_api.Page] = None
        if not is_remote_url(url) and Path(url).is_file() and (Path(url).suffix.lower() == '.html'):
            with open(url, 'r', encoding='utf-8') as f:
                html = f.read()
        else:
            page = self.get_page(url, timeout=timeout, referer=referer)
            if wait:
                page.wait_for_timeout(wait * 1000)
            if scrolldown:
                page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                if sleep:
                    page.wait_for_timeout(sleep * 1000)
            if reload:
                page.reload(timeout=int(timeout * 1000))
            html = page.content()
        md = html_to_markdown(html=html, options=self.html_to_markdown_options)
        if not keep_page and page is not None:
            page.close()
        return md

    async def aurl_to_md(self, url: str, wait: float=0.2, scrolldown: bool=False, sleep: int=0, reload: bool=True, timeout: Union[float, int]=8, keep_page: bool=False, referer: Optional[str]=None) -> str:
        """
        Asynchronously navigate to a URL, wait, scroll or reload if specified,
        and convert the rendered HTML to Markdown.

        Args:
            url (str): URL of the page.
            wait (float): Time to wait after navigation (in seconds).
            scrolldown (bool): If True, scroll the page.
            sleep (int): Time to wait after scrolling (in seconds).
            reload (bool): If True, reload the page.
            timeout (float | int): Navigation timeout (in seconds).
            keep_page (bool): If True, do not close the page after processing.
            referer (Optional[str]): Referer URL to set.

        Returns:
            str: The page content converted to Markdown.
        """
        page: Optional[playwright.async_api.Page] = None
        if not is_remote_url(url) and Path(url).is_file() and (Path(url).suffix.lower() == '.html'):
            with open(url, 'r', encoding='utf-8') as f:
                html = f.read()
        else:
            page = await self.aget_page(url, timeout=timeout, referer=referer)
            if wait:
                await page.wait_for_timeout(wait * 1000)
            if scrolldown:
                await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                if sleep:
                    await page.wait_for_timeout(sleep * 1000)
            if reload:
                await page.reload(timeout=int(timeout * 1000))
            html = await page.content()
        md = html_to_markdown(html=html, options=self.html_to_markdown_options)
        if not keep_page and page is not None:
            await page.close()
        return md

    def select_and_extract(self, url: str, css_selector: str, wait: float=0.2, scrolldown: bool=False, sleep: int=0, reload: bool=True, timeout: Union[float, int]=8, keep_page: bool=False, referer: Optional[str]=None) -> list[str]:
        """
        Navigate to a URL, render the page, and extract text from elements matching the given CSS selector.

        Args:
            url (str): URL of the page.
            css_selector (str): CSS selector to locate elements.
            wait (float): Time to wait after navigation (in seconds).
            scrolldown (bool): If True, scroll the page.
            sleep (int): Time to wait after scrolling (in seconds).
            reload (bool): If True, reload the page.
            timeout (float | int): Maximum navigation time (in seconds).
            keep_page (bool): If True, do not close the page after processing.
            referer (Optional[str]): Referer URL to set.

        Returns:
            List[str]: A list of text contents from the matching elements.
        """
        page = self.get_page(url, timeout=timeout, referer=referer)
        if wait:
            page.wait_for_timeout(wait * 1000)
        if scrolldown:
            page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
            if sleep:
                page.wait_for_timeout(sleep * 1000)
        if reload:
            page.reload(timeout=int(timeout * 1000))
        elements = page.query_selector_all(css_selector)
        texts = [element.inner_text() for element in elements]
        if not keep_page:
            page.close()
        return texts

    async def aselect_and_extract(self, url: str, css_selector: str, wait: float=0.2, scrolldown: bool=False, sleep: int=0, reload: bool=True, timeout: Union[float, int]=8, keep_page: bool=False, referer: Optional[str]=None) -> list[str]:
        """
        Asynchronously navigate to a URL, render the page, and extract text from elements matching the CSS selector.

        Args:
            url (str): URL of the page.
            css_selector (str): CSS selector to locate elements.
            wait (float): Time to wait after navigation (in seconds).
            scrolldown (bool): If True, scroll the page.
            sleep (int): Time to wait after scrolling (in seconds).
            reload (bool): If True, reload the page.
            timeout (float | int): Navigation timeout (in seconds).
            keep_page (bool): If True, do not close the page after processing.
            referer (Optional[str]): Referer URL to set.

        Returns:
            List[str]: A list of text contents from the matching elements.
        """
        page = await self.aget_page(url, timeout=timeout, referer=referer)
        if wait:
            await page.wait_for_timeout(wait * 1000)
        if scrolldown:
            await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
            if sleep:
                await page.wait_for_timeout(sleep * 1000)
        if reload:
            await page.reload(timeout=int(timeout * 1000))
        elements = await page.query_selector_all(css_selector)
        texts: list[str] = []
        for element in elements:
            text = await element.inner_text()
            texts.append(text)
        if not keep_page:
            await page.close()
        return texts

    def url_to_md_with_llm(self, url: str, chunk_size: Optional[int]=None, wait: float=0.2, scrolldown: bool=False, sleep: int=0, reload: bool=True, timeout: Union[float, int]=8, keep_page: bool=False, referer: Optional[str]=None, describe_images: bool=True, filter: bool=True) -> str:
        """
        Convert a URL's page to Markdown and use a language model (Chatterer) to filter out unimportant lines.

        The method splits the Markdown text into chunks, prepends line numbers, and prompts the LLM
        to select the important line ranges. It then reconstructs the filtered Markdown.

        Args:
            url (str): URL of the page.
            chunk_size (Optional[int]): Number of lines per chunk. Defaults to the full content.
            wait (float): Time to wait after navigation (in seconds).
            scrolldown (bool): If True, scroll down the page.
            sleep (int): Time to wait after scrolling (in seconds).
            reload (bool): If True, reload the page.
            timeout (float | int): Navigation timeout (in seconds).
            keep_page (bool): If True, do not close the page after processing.
            referer (Optional[str]): Referer URL to set.
            describe_images (bool): If True, describe images in the Markdown text.
            filter (bool): If True, filter the important lines using the language model.

        Returns:
            str: Filtered Markdown containing only the important lines.
        """
        if self.chatterer is None:
            raise ValueError('Chatterer instance is not set. Please provide a valid Chatterer instance.')
        markdown_content = self.url_to_md(url, wait=wait, scrolldown=scrolldown, sleep=sleep, reload=reload, timeout=timeout, keep_page=keep_page, referer=referer)
        if describe_images:
            markdown_content = self.describe_images(markdown_text=markdown_content, referer_url=url)
        if not filter:
            return markdown_content
        lines = markdown_content.split('\n')
        line_length = len(lines)
        important_lines: set[int] = set()

        def _into_safe_range(value: int) -> int:
            """Ensure the line index stays within bounds."""
            return min(max(value, 0), line_length - 1)
        if chunk_size is None:
            chunk_size = line_length
        for i in range(0, len(lines), chunk_size):
            chunk_lines = lines[i:i + chunk_size]
            numbered_markdown = '\n'.join((f'[Ln {line_no}] {line}' for line_no, line in enumerate(chunk_lines, start=1)))
            result: SelectedLineRanges = self.chatterer.generate_pydantic(response_model=SelectedLineRanges, messages=f'{self.markdown_filtering_instruction}\n{numbered_markdown}')
            for range_str in result.line_ranges:
                if '-' in range_str:
                    start, end = map(int, range_str.split('-'))
                    important_lines.update(range(_into_safe_range(start + i - 1), _into_safe_range(end + i)))
                else:
                    important_lines.add(_into_safe_range(int(range_str) + i - 1))
        return '\n'.join((lines[line_no] for line_no in sorted(important_lines)))

    async def aurl_to_md_with_llm(self, url: str, chunk_size: Optional[int]=None, wait: float=0.2, scrolldown: bool=False, sleep: int=0, reload: bool=True, timeout: Union[float, int]=8, keep_page: bool=False, referer: Optional[str]=None, describe_images: bool=True, filter: bool=True) -> str:
        """
        Asynchronously convert a URL's page to Markdown and use the language model (Chatterer)
        to filter out unimportant lines.

        The method splits the Markdown text into chunks, prepends line numbers, and prompts the LLM
        to select the important line ranges. It then reconstructs the filtered Markdown.

        Args:
            url (str): URL of the page.
            chunk_size (Optional[int]): Number of lines per chunk; defaults to the full content.
            wait (float): Time to wait after navigation (in seconds).
            scrolldown (bool): If True, scroll the page.
            sleep (int): Time to wait after scrolling (in seconds).
            reload (bool): If True, reload the page.
            timeout (float | int): Navigation timeout (in seconds).
            keep_page (bool): If True, do not close the page after processing.
            referer (Optional[str]): Referer URL to set.
            describe_images (bool): If True, describe images in the Markdown text.
            filter (bool): If True, filter the important lines using the language model.

        Returns:
            str: Filtered Markdown containing only the important lines.
        """
        if self.chatterer is None:
            raise ValueError('Chatterer instance is not set. Please provide a valid Chatterer instance.')
        markdown_content = await self.aurl_to_md(url, wait=wait, scrolldown=scrolldown, sleep=sleep, reload=reload, timeout=timeout, keep_page=keep_page, referer=referer)
        if describe_images:
            markdown_content = await self.adescribe_images(markdown_text=markdown_content, referer_url=url)
        if not filter:
            return markdown_content
        lines = markdown_content.split('\n')
        line_length = len(lines)
        important_lines: set[int] = set()

        def _into_safe_range(value: int) -> int:
            """Ensure the line index is within valid bounds."""
            return min(max(value, 0), line_length - 1)
        if chunk_size is None:
            chunk_size = line_length
        for i in range(0, len(lines), chunk_size):
            chunk_lines = lines[i:i + chunk_size]
            numbered_markdown = '\n'.join((f'[Ln {line_no}] {line}' for line_no, line in enumerate(chunk_lines, start=1)))
            result: SelectedLineRanges = await self.chatterer.agenerate_pydantic(response_model=SelectedLineRanges, messages=f'{self.markdown_filtering_instruction}\n{numbered_markdown}')
            for range_str in result.line_ranges:
                if '-' in range_str:
                    start, end = map(int, range_str.split('-'))
                    important_lines.update(range(_into_safe_range(start + i - 1), _into_safe_range(end + i)))
                else:
                    important_lines.add(_into_safe_range(int(range_str) + i - 1))
        return '\n'.join((lines[line_no] for line_no in sorted(important_lines)))

    def describe_images(self, markdown_text: str, referer_url: str) -> str:
        """
        Replace image URLs in Markdown text with their alt text and generate descriptions using a language model.
        Using Playwright for fetching images to bypass CDN protections.
        """
        if self.chatterer is None:
            raise ValueError('Chatterer instance is not set. Please provide a valid Chatterer instance.')
        return caption_markdown_images(markdown_text=markdown_text, headers=self.headers | {'Referer': referer_url}, description_format=self.description_format, image_description_instruction=self.image_description_instruction, chatterer=self.chatterer, image_processing_config=self.image_processing_config, img_bytes_fetcher=self._playwright_fetch_image_bytes)

    async def adescribe_images(self, markdown_text: str, referer_url: str) -> str:
        """
        Replace image URLs in Markdown text with their alt text and generate descriptions using a language model.
        Using Playwright for fetching images to bypass CDN protections.
        """
        if self.chatterer is None:
            raise ValueError('Chatterer instance is not set. Please provide a valid Chatterer instance.')
        return await acaption_markdown_images(markdown_text=markdown_text, headers=self.headers | {'Referer': referer_url}, description_format=self.description_format, image_description_instruction=self.image_description_instruction, chatterer=self.chatterer, image_processing_config=self.image_processing_config, img_bytes_fetcher=self._aplaywright_fetch_image_bytes)

    def _playwright_fetch_image_bytes(self, image_url: str, headers: dict[str, str]) -> bytes:
        """Playwright를 사용하여 동기적으로 이미지 바이트를 가져옵니다."""
        page: Optional[playwright.sync_api.Page] = None
        try:
            page = self.get_sync_browser().new_page()
            page.set_extra_http_headers(headers)
            response = page.goto(image_url, wait_until='load', timeout=15000)
            if response and response.ok:
                return response.body()
            else:
                return b''
        except Exception as e:
            print(f'Playwright exception fetching image: {image_url}, Error: {e}')
            return b''
        finally:
            if page:
                page.close()

    async def _aplaywright_fetch_image_bytes(self, image_url: str, headers: dict[str, str]) -> bytes:
        """Playwright를 사용하여 비동기적으로 이미지 바이트를 가져옵니다."""
        page: Optional[playwright.async_api.Page] = None
        try:
            page = await (await self.get_async_browser()).new_page()
            await page.set_extra_http_headers(headers)
            response = await page.goto(image_url, wait_until='load', timeout=15000)
            if response and response.ok:
                return await response.body()
            else:
                print(f"Playwright failed to fetch image: {image_url}, Status: {(response.status if response else 'No Response')}")
                return b''
        except Exception as e:
            print(f'Playwright exception fetching image: {image_url}, Error: {e}')
            return b''
        finally:
            if page:
                await page.close()

    def __enter__(self) -> Self:
        return self

    async def __aenter__(self) -> Self:
        return self

    def __exit__(self, exc_type: Optional[Type[BaseException]], exc_val: Optional[BaseException], exc_tb: Optional[TracebackType]) -> None:
        """
        Exit the synchronous context.

        Closes the browser and stops Playwright.
        """
        if self.sync_browser_context is not None:
            self.sync_browser_context.close()
            self.sync_browser_context = None
        if self.sync_playwright:
            self.sync_playwright.stop()
            self.sync_playwright = None

    async def __aexit__(self, exc_type: Optional[Type[BaseException]], exc_val: Optional[BaseException], exc_tb: Optional[TracebackType]) -> None:
        """
        Asynchronously exit the context.

        Closes the asynchronous browser and stops Playwright.
        """
        if self.async_browser_context is not None:
            await self.async_browser_context.close()
            self.async_browser_context = None
        if self.async_playwright:
            await self.async_playwright.stop()
            self.async_playwright = None
```

```C:\Users\cosogi\chatterer\chatterer\tools\youtube.py
import json
import unicodedata
import urllib.parse
from dataclasses import dataclass
from typing import Any, Optional, Self, cast
import requests

def get_youtube_video_details(query: str) -> list[dict[str, Optional[str]]]:
    """Search for video metadata on YouTube using the given query. Returns a list of dictionaries containing `video_id`, `title`, `channel`, `duration`, `views`, `publish_time`, and `long_desc`."""
    return [{'video_id': video_id, 'title': video.title, 'channel': video.channel, 'duration': video.duration, 'views': video.views, 'publish_time': video.publish_time, 'long_desc': video.long_desc} for video in YoutubeSearchResult.from_query(base_url='https://youtube.com', query=query, max_results=10) if (video_id := _get_video_id(video.url_suffix))]

def get_youtube_video_subtitle(video_id: str) -> str:
    """Get the transcript of a YouTube video using the given video ID."""
    from youtube_transcript_api import YouTubeTranscriptApi
    get_transcript = YouTubeTranscriptApi.get_transcript
    list_transcripts = YouTubeTranscriptApi.list_transcripts
    result: str = ''
    buffer_timestamp: str = '0s'
    buffer_texts: list[str] = []
    for entry in get_transcript(video_id, languages=(next(iter(list_transcripts(video_id))).language_code,)):
        entry = cast(dict[object, object], entry)
        text: str = str(entry.get('text', '')).strip().replace('\n', ' ')
        if not text:
            continue
        if len(buffer_texts) >= 10 or _is_special_char(text) or (buffer_texts and _is_special_char(buffer_texts[-1])):
            result += f"[{buffer_timestamp}] {'. '.join(buffer_texts)}\n"
            start = entry.get('start', 0)
            if start:
                buffer_timestamp = f'{start:.0f}s'
            buffer_texts = [text]
        else:
            buffer_texts.append(text)
    if buffer_texts:
        result += f"[{buffer_timestamp}] {' '.join(buffer_texts)}"
    return result

def _get_video_id(suffix: str) -> str:
    urllib_parse_result = urllib.parse.urlparse(suffix)
    if urllib_parse_result.path.startswith('/shorts/'):
        parts = urllib_parse_result.path.split('/')
        if len(parts) < 3:
            print(f'Failed to get video ID from {suffix}')
            return ''
        return parts[2]
    query: str = urllib.parse.urlparse(suffix).query
    query_strings = urllib.parse.parse_qs(query)
    if 'v' not in query_strings:
        print(f'Failed to get video ID from {suffix}')
        return ''
    return next(iter(query_strings['v']), '')

def _is_special_char(text: str) -> bool:
    if not text:
        return False
    return not unicodedata.category(text[0]).startswith('L')

@dataclass
class YoutubeSearchResult:
    url_suffix: str
    id: Optional[str]
    thumbnails: list[str]
    title: Optional[str]
    long_desc: Optional[str]
    channel: Optional[str]
    duration: Optional[str]
    views: Optional[str]
    publish_time: Optional[str]

    @classmethod
    def from_query(cls, base_url: str, query: str, max_results: int) -> list[Self]:
        url: str = f'{base_url}/results?search_query={urllib.parse.quote_plus(query)}'
        response: str = requests.get(url).text
        while 'ytInitialData' not in response:
            response = requests.get(url).text
        results: list[Self] = cls.parse_html(response)
        return results[:max_results]

    @classmethod
    def parse_html(cls, html: str) -> list[Self]:
        results: list[Self] = []
        start: int = html.index('ytInitialData') + len('ytInitialData') + 3
        end: int = html.index('};', start) + 1
        data: Any = json.loads(html[start:end])
        for contents in data['contents']['twoColumnSearchResultsRenderer']['primaryContents']['sectionListRenderer']['contents']:
            for video in contents['itemSectionRenderer']['contents']:
                if 'videoRenderer' in video.keys():
                    video_data = video.get('videoRenderer', {})
                    suffix = video_data.get('navigationEndpoint', {}).get('commandMetadata', {}).get('webCommandMetadata', {}).get('url', None)
                    if not suffix:
                        continue
                    res = cls(id=video_data.get('videoId', None), thumbnails=[thumb.get('url', None) for thumb in video_data.get('thumbnail', {}).get('thumbnails', [{}])], title=video_data.get('title', {}).get('runs', [[{}]])[0].get('text', None), long_desc=video_data.get('descriptionSnippet', {}).get('runs', [{}])[0].get('text', None), channel=video_data.get('longBylineText', {}).get('runs', [[{}]])[0].get('text', None), duration=video_data.get('lengthText', {}).get('simpleText', 0), views=video_data.get('viewCountText', {}).get('simpleText', 0), publish_time=video_data.get('publishedTimeText', {}).get('simpleText', 0), url_suffix=suffix)
                    results.append(res)
            if results:
                break
        return results
if __name__ == '__main__':
    print(get_youtube_video_details('BTS'))
```

```C:\Users\cosogi\chatterer\chatterer\tools\__init__.py
from .caption_markdown_images import MarkdownLink, acaption_markdown_images, caption_markdown_images
from .citation_chunking import citation_chunker
from .convert_pdf_to_markdown import PdfToMarkdown, extract_text_from_pdf, open_pdf, render_pdf_as_image
from .convert_to_text import CodeSnippets, anything_to_markdown, get_default_html_to_markdown_options, html_to_markdown, pdf_to_text, pyscripts_to_snippets
from .upstage_document_parser import UpstageDocumentParseParser
from .webpage_to_markdown import PlayWrightBot, PlaywrightLaunchOptions, PlaywrightOptions, PlaywrightPersistencyOptions, get_default_playwright_launch_options
from .youtube import get_youtube_video_details, get_youtube_video_subtitle
__all__ = ['html_to_markdown', 'anything_to_markdown', 'pdf_to_text', 'get_default_html_to_markdown_options', 'pyscripts_to_snippets', 'citation_chunker', 'webpage_to_markdown', 'get_youtube_video_subtitle', 'get_youtube_video_details', 'CodeSnippets', 'PlayWrightBot', 'PlaywrightLaunchOptions', 'PlaywrightOptions', 'PlaywrightPersistencyOptions', 'get_default_playwright_launch_options', 'UpstageDocumentParseParser', 'acaption_markdown_images', 'caption_markdown_images', 'MarkdownLink', 'PdfToMarkdown', 'extract_text_from_pdf', 'open_pdf', 'render_pdf_as_image']
```

```C:\Users\cosogi\chatterer\chatterer\utils\base64_image.py
from __future__ import annotations
import re
from base64 import b64encode
from io import BytesIO
from logging import getLogger
from pathlib import Path
from typing import Awaitable, Callable, ClassVar, Literal, NotRequired, Optional, Self, Sequence, TypeAlias, TypedDict, TypeGuard, cast, get_args
from urllib.parse import urlparse
import requests
from aiohttp import ClientSession
from PIL.Image import Resampling
from PIL.Image import open as image_open
from pydantic import BaseModel
logger = getLogger(__name__)
ImageType: TypeAlias = Literal['jpeg', 'jpg', 'png', 'gif', 'webp', 'bmp']

class ImageProcessingConfig(TypedDict):
    """
    이미지 필터링/변환 시 사용할 설정.
      - formats: (Sequence[str]) 허용할 이미지 포맷(소문자, 예: ["jpeg", "png", "webp"]).
      - max_size_mb: (float) 이미지 용량 상한(MB). 초과 시 제외.
      - min_largest_side: (int) 가로나 세로 중 가장 큰 변의 최소 크기. 미만 시 제외.
      - resize_if_min_side_exceeds: (int) 가로나 세로 중 작은 변이 이 값 이상이면 리스케일.
      - resize_target_for_min_side: (int) 리스케일시, '가장 작은 변'을 이 값으로 줄임(비율 유지는 Lanczos).
    """
    formats: Sequence[ImageType]
    max_size_mb: NotRequired[float]
    min_largest_side: NotRequired[int]
    resize_if_min_side_exceeds: NotRequired[int]
    resize_target_for_min_side: NotRequired[int]

def get_default_image_processing_config() -> ImageProcessingConfig:
    return {'max_size_mb': 5, 'min_largest_side': 200, 'resize_if_min_side_exceeds': 2000, 'resize_target_for_min_side': 1000, 'formats': ['png', 'jpeg', 'jpg', 'gif', 'bmp', 'webp']}

class Base64Image(BaseModel):
    ext: ImageType
    data: str
    IMAGE_TYPES: ClassVar[tuple[str, ...]] = tuple(map(str, get_args(ImageType)))
    IMAGE_PATTERN: ClassVar[re.Pattern[str]] = re.compile('data:image/(' + '|'.join(IMAGE_TYPES) + ');base64,([A-Za-z0-9+/]+={0,2})')

    def __hash__(self) -> int:
        return hash((self.ext, self.data))

    def model_post_init(self, __context: object) -> None:
        if self.ext == 'jpg':
            self.ext = 'jpeg'

    @classmethod
    def from_string(cls, data: str) -> Optional[Self]:
        match = cls.IMAGE_PATTERN.fullmatch(data)
        if not match:
            return None
        return cls(ext=cast(ImageType, match.group(1)), data=match.group(2))

    @classmethod
    def from_bytes(cls, data: bytes, ext: ImageType) -> Self:
        return cls(ext=ext, data=b64encode(data).decode('utf-8'))

    @classmethod
    def from_url_or_path(cls, url_or_path: str, *, headers: dict[str, str]={}, config: ImageProcessingConfig=get_default_image_processing_config(), img_bytes_fetcher: Optional[Callable[[str, dict[str, str]], bytes]]=None) -> Optional[Self]:
        """Return a Base64Image instance from a URL or local file path."""
        if (maybe_base64 := cls.from_string(url_or_path)):
            return maybe_base64
        elif is_remote_url(url_or_path):
            if img_bytes_fetcher:
                img_bytes = img_bytes_fetcher(url_or_path, headers)
            else:
                img_bytes = cls._fetch_remote_image(url_or_path, headers)
            if not img_bytes:
                return None
            return cls._convert_image_into_base64(img_bytes, config)
        try:
            return cls._process_local_image(Path(url_or_path), config)
        except Exception:
            return None

    @classmethod
    async def afrom_url_or_path(cls, url_or_path: str, *, headers: dict[str, str]={}, config: ImageProcessingConfig=get_default_image_processing_config(), img_bytes_fetcher: Optional[Callable[[str, dict[str, str]], Awaitable[bytes]]]=None) -> Optional[Self]:
        """Return a Base64Image instance from a URL or local file path."""
        if (maybe_base64 := cls.from_string(url_or_path)):
            return maybe_base64
        elif is_remote_url(url_or_path):
            if img_bytes_fetcher:
                img_bytes = await img_bytes_fetcher(url_or_path, headers)
            else:
                img_bytes = await cls._afetch_remote_image(url_or_path, headers)
            if not img_bytes:
                return None
            return cls._convert_image_into_base64(img_bytes, config)
        try:
            return cls._process_local_image(Path(url_or_path), config)
        except Exception:
            return None

    @property
    def data_uri(self) -> str:
        return f"data:image/{self.ext.replace('jpg', 'jpeg')};base64,{self.data}"

    @property
    def data_uri_content(self) -> dict[Literal['type', 'image_url'], Literal['image_url'] | dict[Literal['url'], str]]:
        return {'type': 'image_url', 'image_url': {'url': self.data_uri}}

    @staticmethod
    def _verify_ext(ext: str, allowed_types: Sequence[ImageType]) -> TypeGuard[ImageType]:
        return ext in allowed_types

    @classmethod
    def _fetch_remote_image(cls, url: str, headers: dict[str, str]) -> bytes:
        try:
            with requests.Session() as session:
                response = session.get(url.strip(), headers={k: str(v) for k, v in headers.items()})
                response.raise_for_status()
                image_bytes = bytes(response.content or b'')
                if not image_bytes:
                    return b''
                return image_bytes
        except Exception:
            return b''

    @classmethod
    async def _afetch_remote_image(cls, url: str, headers: dict[str, str]) -> bytes:
        try:
            async with ClientSession() as session:
                async with session.get(url.strip(), headers={k: str(v) for k, v in headers.items()}) as response:
                    response.raise_for_status()
                    return await response.read()
        except Exception:
            return b''

    @classmethod
    def _convert_image_into_base64(cls, image_data: bytes, config: Optional[ImageProcessingConfig]) -> Optional[Self]:
        """
        Retrieve an image in bytes and return a base64-encoded data URL,
        applying dynamic rules from 'config'.
        """
        if not config:
            return cls._simple_base64_encode(image_data)
        max_size_mb = config.get('max_size_mb', float('inf'))
        image_size_mb = len(image_data) / (1024 * 1024)
        if image_size_mb > max_size_mb:
            logger.error(f'Image too large: {image_size_mb:.2f} MB > {max_size_mb} MB')
            return None
        try:
            with image_open(BytesIO(image_data)) as im:
                w, h = im.size
                largest_side = max(w, h)
                smallest_side = min(w, h)
                min_largest_side = config.get('min_largest_side', 1)
                if largest_side < min_largest_side:
                    logger.error(f'Image too small: {largest_side} < {min_largest_side}')
                    return None
                resize_if_min_side_exceeds = config.get('resize_if_min_side_exceeds', float('inf'))
                if smallest_side >= resize_if_min_side_exceeds:
                    resize_target = config.get('resize_target_for_min_side', 1000)
                    ratio = resize_target / float(smallest_side)
                    new_w = int(w * ratio)
                    new_h = int(h * ratio)
                    im = im.resize((new_w, new_h), Resampling.LANCZOS)
                pil_format: str = (im.format or '').lower()
                allowed_formats: Sequence[ImageType] = config.get('formats', [])
                if not cls._verify_ext(pil_format, allowed_formats):
                    logger.error(f'Invalid format: {pil_format} not in {allowed_formats}')
                    return None
                output_buffer = BytesIO()
                im.save(output_buffer, format=pil_format.upper())
                output_buffer.seek(0)
                final_bytes = output_buffer.read()
        except Exception:
            return None
        encoded_data = b64encode(final_bytes).decode('utf-8')
        return cls(ext=pil_format, data=encoded_data)

    @classmethod
    def _simple_base64_encode(cls, image_data: bytes) -> Optional[Self]:
        """
        Retrieve an image URL and return a base64-encoded data URL.
        """
        ext = detect_image_type(image_data)
        if not ext:
            return
        return cls(ext=ext, data=b64encode(image_data).decode('utf-8'))

    @classmethod
    def _process_local_image(cls, path: Path, config: ImageProcessingConfig) -> Optional[Self]:
        """로컬 파일이 존재하고 유효한 이미지 포맷이면 Base64 데이터 URL을 반환, 아니면 None."""
        if not path.is_file():
            return None
        ext = path.suffix.lower().removeprefix('.')
        if not cls._verify_ext(ext, config['formats']):
            return None
        return cls(ext=ext, data=b64encode(path.read_bytes()).decode('ascii'))

def is_remote_url(path: str) -> bool:
    parsed = urlparse(path)
    return bool(parsed.scheme and parsed.netloc)

def detect_image_type(image_data: bytes) -> Optional[ImageType]:
    """
    Detect the image format based on the image binary signature (header).
    Only JPEG, PNG, GIF, WEBP, and BMP are handled as examples.
    If the format is not recognized, return None.
    """
    if image_data.startswith(b'\xff\xd8\xff'):
        return 'jpeg'
    elif image_data.startswith(b'\x89PNG\r\n\x1a\n'):
        return 'png'
    elif image_data.startswith(b'GIF87a') or image_data.startswith(b'GIF89a'):
        return 'gif'
    elif image_data.startswith(b'RIFF') and image_data[8:12] == b'WEBP':
        return 'webp'
    elif image_data.startswith(b'BM'):
        return 'bmp'
```

```C:\Users\cosogi\chatterer\chatterer\utils\bytesio.py
import os
from contextlib import contextmanager, suppress
from io import BytesIO
from typing import Iterator, Optional
from ..common_types.io import BytesReadable, PathOrReadable, StringReadable

@contextmanager
def read_bytes_stream(path_or_file: PathOrReadable, assume_pathlike_bytes_as_path: bool=False, assume_pathlike_string_as_path: bool=True) -> Iterator[Optional[BytesReadable]]:
    """
    Context manager for opening a file or using an existing stream.

    Handles different types of input (file paths, byte streams, string streams)
    and yields a BytesReadable object that can be used to read binary data.

    Args:
        path_or_file: File path or readable object.
        assume_pathlike_bytes_as_path: If True, assume bytes-like objects are file paths. Else, treat as data itself.
        assume_pathlike_string_as_path: If True, assume string-like objects are file paths. Else, treat as data itself.

    Yields:
        Optional[BytesReadable]: A readable binary stream or None if opening fails.
    """
    stream: Optional[BytesReadable] = None
    should_close: bool = True
    try:
        with suppress(BaseException):
            if isinstance(path_or_file, BytesReadable):
                stream = path_or_file
                should_close = False
            elif isinstance(path_or_file, StringReadable):
                stream = BytesIO(path_or_file.read().encode('utf-8'))
            elif isinstance(path_or_file, bytes):
                if assume_pathlike_bytes_as_path and os.path.exists(path_or_file):
                    stream = open(path_or_file, 'rb')
                else:
                    stream = BytesIO(path_or_file)
            elif isinstance(path_or_file, str):
                if assume_pathlike_string_as_path and os.path.exists(path_or_file):
                    stream = open(path_or_file, 'rb')
                else:
                    stream = BytesIO(path_or_file.encode('utf-8'))
            else:
                stream = open(path_or_file, 'rb')
        yield stream
    finally:
        if stream is not None and should_close:
            stream.close()
```

```C:\Users\cosogi\chatterer\chatterer\utils\code_agent.py
import inspect
import textwrap
from typing import TYPE_CHECKING, Callable, Iterable, NamedTuple, Optional, Self, Sequence
from langchain_core.runnables.config import RunnableConfig
from ..messages import LanguageModelInput, SystemMessage
if TYPE_CHECKING:
    from langchain_experimental.tools import PythonAstREPLTool
DEFAULT_CODE_GENERATION_PROMPT = 'You are equipped with a Python code execution tool.\nYour primary goal is to generate Python code that effectively solves the *specific, immediate sub-task* required to progress towards the overall user request. The generated code and its resulting output will be automatically added to our conversation history.\n\nGuidelines for Optimal Tool Use:\n- Conciseness and Efficiency: Write code that directly addresses the current need. Avoid unnecessary complexity, computations, or data loading. Tool execution has resource limits.\n- Targeted Action: Focus only on the code required for the *next logical step*. Do not attempt to solve the entire problem in one code block if it involves multiple steps.\n- Error Handling: Implement basic error handling (e.g., `try-except`) for operations that might fail (like file access or network requests, if applicable).\n- Context Awareness: Assume the code runs in a stateful environment where variables and imports might persist from previous executions (unless explicitly cleared).\n- Self-Contained Execution: Ensure the code block is runnable as provided. Define necessary variables within the block if they aren\'t guaranteed to exist from prior context.\n\nOutput Format:\nReturn *only* a JSON object containing the Python code:\n{\n  "code": "<your_python_code_here>"\n}\n\n'
DEFAULT_FUNCTION_REFERENCE_PREFIX_PROMPT = 'The following Python functions are available in the global scope for you to use directly in your code.\nYou do not need to define these functions; simply call them as needed.\nUse these functions only when they directly help in solving the current task. You are not obligated to use them.\n'
DEFAULT_FUNCTION_REFERENCE_SEPARATOR = '\n---\n'

class FunctionSignature(NamedTuple):
    name: str
    callable: Callable[..., object]
    signature: str

    @classmethod
    def from_callable(cls, callables: Optional[Callable[..., object] | Iterable[Callable[..., object]]]) -> list[Self]:
        if callables is None:
            return []
        if isinstance(callables, Callable) and (not isinstance(callables, type)):
            return [cls._from_callable(callables)]
        if isinstance(callables, Iterable):
            return [cls._from_callable(c) for c in callables]
        return []

    @classmethod
    def _from_callable(cls, callable_obj: Callable[..., object]) -> Self:
        """
        Get the name and signature of a function as a string.
        """
        is_async_func = inspect.iscoroutinefunction(callable_obj)
        function_def = 'async def' if is_async_func else 'def'
        if inspect.isfunction(callable_obj):
            function_name = callable_obj.__code__.co_name
        elif hasattr(callable_obj, 'name') and isinstance(getattr(callable_obj, 'name'), str):
            function_name = getattr(callable_obj, 'name')
        elif hasattr(callable_obj, '__name__'):
            function_name = callable_obj.__name__
        else:
            function_name = type(callable_obj).__name__
        try:
            signature_str = str(inspect.signature(callable_obj))
        except ValueError:
            signature_str = '(...)'
        signature = f'{function_def} {function_name}{signature_str}:'
        docstring = inspect.getdoc(callable_obj)
        if docstring:
            docstring = f'"""{docstring.strip()}"""'
            full_signature = f"{signature}\n{textwrap.indent(docstring, '    ')}"
        else:
            full_signature = signature
        return cls(name=function_name, callable=callable_obj, signature=full_signature)

    @classmethod
    def as_prompt(cls, function_signatures: Iterable[Self], prefix: Optional[str]=DEFAULT_FUNCTION_REFERENCE_PREFIX_PROMPT, sep: str=DEFAULT_FUNCTION_REFERENCE_SEPARATOR) -> str:
        """
        Generate a prompt string from a list of function signatures.
        """
        if not function_signatures:
            return ''
        body: str = sep.join((fsig.signature for fsig in function_signatures))
        if prefix:
            return f'{prefix}\n{body}'
        return body

class CodeExecutionResult(NamedTuple):
    code: str
    output: str

    @classmethod
    def from_code(cls, code: str, repl_tool: Optional['PythonAstREPLTool']=None, config: Optional[RunnableConfig]=None, function_signatures: Optional[Iterable[FunctionSignature]]=None, **kwargs: object) -> Self:
        """
        Execute code using the Python REPL tool.
        """
        if repl_tool is None:
            repl_tool = get_default_repl_tool()
        if function_signatures:
            insert_callables_into_global(function_signatures=function_signatures, repl_tool=repl_tool)
        output = str(repl_tool.invoke(code, config=config))
        return cls(code=code, output=output)

    @classmethod
    async def afrom_code(cls, code: str, repl_tool: Optional['PythonAstREPLTool']=None, config: Optional[RunnableConfig]=None, function_signatures: Optional[Iterable[FunctionSignature]]=None, **kwargs: object) -> Self:
        """
        Execute code using the Python REPL tool asynchronously.
        """
        if repl_tool is None:
            repl_tool = get_default_repl_tool()
        if function_signatures:
            insert_callables_into_global(function_signatures=function_signatures, repl_tool=repl_tool)
        output = str(await repl_tool.ainvoke(code, config=config))
        return cls(code=code, output=output)

def get_default_repl_tool() -> 'PythonAstREPLTool':
    """Initializes and returns a default PythonAstREPLTool instance."""
    try:
        from langchain_experimental.tools import PythonAstREPLTool
        return PythonAstREPLTool()
    except ImportError:
        raise ImportError('PythonAstREPLTool requires langchain_experimental. Install with: pip install langchain-experimental')

def insert_callables_into_global(function_signatures: Iterable[FunctionSignature], repl_tool: 'PythonAstREPLTool') -> None:
    """Insert callables into the REPL tool's globals."""
    if not hasattr(repl_tool, 'globals') or not isinstance(repl_tool.globals, dict):
        repl_tool.globals = {}
    current_globals: dict[object, object] = repl_tool.globals
    for fsig in function_signatures:
        current_globals[fsig.name] = fsig.callable

def _add_message_first(messages: LanguageModelInput, prompt_to_add: str) -> LanguageModelInput:
    """Prepends a SystemMessage to the beginning of the message list/string."""
    if not prompt_to_add:
        return messages
    if isinstance(messages, str):
        return f'{prompt_to_add}\n\n{messages}'
    elif isinstance(messages, Sequence):
        msg_list = list(messages)
        msg_list.insert(0, SystemMessage(content=prompt_to_add))
        return msg_list
    else:
        raise TypeError(f'Unsupported message input type: {type(messages)}')

def augment_prompt_for_toolcall(function_signatures: Iterable[FunctionSignature], messages: LanguageModelInput, prompt_for_code_invoke: Optional[str]=DEFAULT_CODE_GENERATION_PROMPT, function_reference_prefix: Optional[str]=DEFAULT_FUNCTION_REFERENCE_PREFIX_PROMPT, function_reference_seperator: str=DEFAULT_FUNCTION_REFERENCE_SEPARATOR) -> LanguageModelInput:
    """Adds function references and code invocation prompts to the messages."""
    func_prompt = FunctionSignature.as_prompt(function_signatures, function_reference_prefix, function_reference_seperator)
    if func_prompt:
        messages = _add_message_first(messages=messages, prompt_to_add=func_prompt)
    if prompt_for_code_invoke:
        messages = _add_message_first(messages=messages, prompt_to_add=prompt_for_code_invoke)
    return messages
```

```C:\Users\cosogi\chatterer\chatterer\utils\imghdr.py
"""
Recognize image file formats based on their first few bytes (base64-encoded).
Originally derived from Python's imghdr, modified for base64 inputs.
"""
import base64
import math
from typing import Callable, List, Literal, Optional
ImageType = Literal['jpeg', 'png', 'gif', 'tiff', 'rgb', 'pbm', 'pgm', 'ppm', 'rast', 'xbm', 'bmp', 'webp', 'exr']
tests: List[Callable[[bytes], Optional[ImageType]]] = []

def register_test(func: Callable[[bytes], Optional[ImageType]]) -> Callable[[bytes], Optional[ImageType]]:
    tests.append(func)
    return func

def decode_prefix(b64_data: str, prefix_bytes: int=32) -> bytes:
    needed_chars = math.ceil(prefix_bytes * 4 / 3)
    truncated_data = b64_data[:needed_chars]
    try:
        return base64.b64decode(truncated_data)
    except Exception:
        return base64.b64decode(b64_data)

def what(b64_data: str) -> Optional[ImageType]:
    """
    base64 인코딩된 문자열에 포함된 이미지의 타입을 반환한다.

    :param b64_data: 이미지 데이터를 담은 base64 문자열.
    :return: 이미지 포맷 문자열 (예: "jpeg", "png", "gif", 등) 또는 인식되지 않으면 None.
    """
    h: bytes = decode_prefix(b64_data, prefix_bytes=32)
    for tf in tests:
        res = tf(h)
        if res:
            return res
    return None

@register_test
def test_jpeg(h: bytes) -> Optional[ImageType]:
    if len(h) >= 10 and h[6:10] in (b'JFIF', b'Exif'):
        return 'jpeg'
    elif h.startswith(b'\xff\xd8\xff\xdb'):
        return 'jpeg'
    return None

@register_test
def test_png(h: bytes) -> Optional[ImageType]:
    if h.startswith(b'\x89PNG\r\n\x1a\n'):
        return 'png'
    return None

@register_test
def test_gif(h: bytes) -> Optional[ImageType]:
    if h.startswith(b'GIF87a') or h.startswith(b'GIF89a'):
        return 'gif'
    return None

@register_test
def test_tiff(h: bytes) -> Optional[ImageType]:
    if h[:2] in (b'MM', b'II'):
        return 'tiff'
    return None

@register_test
def test_rgb(h: bytes) -> Optional[ImageType]:
    if h.startswith(b'\x01\xda'):
        return 'rgb'
    return None

@register_test
def test_pbm(h: bytes) -> Optional[ImageType]:
    if len(h) >= 3 and h[0] == ord(b'P') and (h[1] in b'14') and (h[2] in b' \t\n\r'):
        return 'pbm'
    return None

@register_test
def test_pgm(h: bytes) -> Optional[ImageType]:
    if len(h) >= 3 and h[0] == ord(b'P') and (h[1] in b'25') and (h[2] in b' \t\n\r'):
        return 'pgm'
    return None

@register_test
def test_ppm(h: bytes) -> Optional[ImageType]:
    if len(h) >= 3 and h[0] == ord(b'P') and (h[1] in b'36') and (h[2] in b' \t\n\r'):
        return 'ppm'
    return None

@register_test
def test_rast(h: bytes) -> Optional[ImageType]:
    if h.startswith(b'Y\xa6j\x95'):
        return 'rast'
    return None

@register_test
def test_xbm(h: bytes) -> Optional[ImageType]:
    if h.startswith(b'#define '):
        return 'xbm'
    return None

@register_test
def test_bmp(h: bytes) -> Optional[ImageType]:
    if h.startswith(b'BM'):
        return 'bmp'
    return None

@register_test
def test_webp(h: bytes) -> Optional[ImageType]:
    if len(h) >= 12 and h.startswith(b'RIFF') and (h[8:12] == b'WEBP'):
        return 'webp'
    return None

@register_test
def test_exr(h: bytes) -> Optional[ImageType]:
    if h.startswith(b'v/1\x01'):
        return 'exr'
    return None
if __name__ == '__main__':
    example_png_base64 = 'iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mP8/5+BAQAE/wH+U6az4wAAAABJRU5ErkJggg=='
    fmt = what(example_png_base64)
    print(f'Detected format: {fmt}')
```

```C:\Users\cosogi\chatterer\chatterer\utils\__init__.py
from .base64_image import Base64Image
from .code_agent import CodeExecutionResult, FunctionSignature, get_default_repl_tool, insert_callables_into_global
__all__ = ['Base64Image', 'FunctionSignature', 'CodeExecutionResult', 'get_default_repl_tool', 'insert_callables_into_global']
```

```C:\Users\cosogi\chatterer\chatterer\tools\citation_chunking\chunks.py
import logging
from typing import Callable, Optional, Self
from pydantic import BaseModel, Field
from ...language_model import Chatterer
from ...messages import AIMessage, BaseMessage, HumanMessage
from .prompt import generate_fewshot_affirmative_response, generate_human_assistant_fewshot_examples, generate_instruction
from .reference import Reference
logger = logging.getLogger(__name__)

class CitationChunk(BaseModel):
    subject: str = Field(description='The main topic or subject that the citations capture.')
    references: list[Reference] = Field(description='A list of citation objects and/or regex patterns for the subject.')

class CitationChunks(BaseModel):
    citation_chunks: list[CitationChunk] = Field(description='A list of citation chunks, each capturing a specific topic in the document.')

    @classmethod
    def from_llm(cls, chatterer: Chatterer, document: str, fewshot_examples_generator: Optional[Callable[[], list[tuple[str, str]]]]=generate_human_assistant_fewshot_examples, instruction_generator: Optional[Callable[[], str]]=generate_instruction, fewshot_affirmative_response: Optional[Callable[[], str]]=generate_fewshot_affirmative_response) -> Self:
        messages: list[BaseMessage] = []
        if instruction_generator:
            messages.append(HumanMessage(content=instruction_generator()))
        if fewshot_examples_generator is not None:
            if fewshot_affirmative_response:
                messages.append(AIMessage(content=generate_fewshot_affirmative_response()))
            for human_ask, ai_answer in fewshot_examples_generator():
                messages.append(HumanMessage(content=human_ask))
                messages.append(AIMessage(content=ai_answer))
        messages.append(HumanMessage(content=document))
        try:
            return chatterer.generate_pydantic(response_model=cls, messages=messages)
        except Exception as e:
            logger.error(f'Error obtaining CitationChunks from LLM: {e}')
            raise e
```

```C:\Users\cosogi\chatterer\chatterer\tools\citation_chunking\citations.py
from __future__ import annotations
import difflib
import logging
from typing import NamedTuple, Optional, Self, TypeAlias
from pydantic import Field
from regex import DOTALL
from regex import compile as regex_compile
from regex import error as regex_error
from ...language_model import Chatterer
from ...messages import HumanMessage
from .chunks import CitationChunk
from .reference import MultiMatchRegex, Reference, SingleMatchCitation
from .utils import MatchedText
ModelAndSteps: TypeAlias = tuple[Chatterer, int]
logger = logging.getLogger(__name__)

class Citations(NamedTuple):
    """
    Holds the verified citation chunks and their matching information.
    """
    name: str
    references: dict[Reference, list[ReferencedTextMatch]]

    @classmethod
    def from_unverified(cls, unverified_chunk: CitationChunk, document: str, model_and_refinement_steps: Optional[ModelAndSteps]=None) -> Self:
        subject: str = unverified_chunk.subject
        self: Self = cls(name=subject, references={})
        for reference in unverified_chunk.references or ():
            if isinstance(reference, SingleMatchCitation):
                try:
                    mt: Optional[ReferencedTextMatch] = ReferencedTextMatch.from_citation(subject=subject, citation=reference, document=document, model_and_refinement_steps=model_and_refinement_steps)
                    if mt is None or not mt.text.strip():
                        logger.warning(f"Failed to extract text for citation {reference} in subject '{subject}'.")
                    else:
                        self.references[reference] = [mt]
                except Exception as e:
                    logger.error(f"Error processing citation {reference} for subject '{subject}': {e}")
            else:
                try:
                    regex_matches: list[ReferencedTextMatch] = ReferencedTextMatch.from_regex(regex=reference, subject=subject, document=document)
                    if regex_matches:
                        self.references[reference] = regex_matches
                except regex_error as e:
                    logger.error(f"Regex error for subject '{subject}' with pattern '{reference}': {e}")
        return self

class ReferencedTextMatch(MatchedText):

    @classmethod
    def from_citation(cls, subject: str, citation: SingleMatchCitation, document: str, model_and_refinement_steps: Optional[ModelAndSteps]=None) -> Optional[Self]:
        """
        Extract text from the document using the adjusted citation indices.
        Additionally, if a language model is provided, evaluate the extraction quality
        and refine it if needed.
        """
        citation_id: Optional[SingleMatchCitationWithIndex] = SingleMatchCitationWithIndex.from_indexless_citation(indexless_citation=citation, document=document, subject=subject, model_and_refinement_steps=model_and_refinement_steps)
        if citation_id is None:
            return
        return cls(start_idx=citation_id.start, end_idx=citation_id.end, text=citation_id.extracted_text)

    @classmethod
    def from_regex(cls, regex: MultiMatchRegex, subject: str, document: str) -> list[Self]:
        """
        Apply the given regex to the document and return all matching results as a list of MatchedText.
        """
        try:
            compiled_pattern = regex_compile(regex.regular_expression, flags=DOTALL)
        except regex_error as e:
            logger.error(f'Regex compilation error for pattern /{regex.regular_expression}/: {e}')
            raise e
        try:
            matches = list(compiled_pattern.finditer(document, timeout=1.0))
        except regex_error as e:
            logger.error(f'Regex matching error for pattern /{regex.regular_expression}/: {e}')
            raise e
        return [cls(start_idx=m.start(), end_idx=m.end(), text=m.group()) for m in matches]

class SingleMatchCitationWithIndex(SingleMatchCitation):
    start: int = Field(description='The computed start index of the citation in the document.')
    end: int = Field(description='The computed end index of the citation in the document.')
    extracted_text: str = Field(description='The extracted text from the document using the computed indices.')

    @classmethod
    def from_indexless_citation(cls, indexless_citation: SingleMatchCitation, document: str, subject: str, model_and_refinement_steps: Optional[ModelAndSteps]=None) -> Optional[Self]:
        """
        Compute the correct start and end indices for the citation based on the provided text snippets.
        This method ignores any indices provided by the LLM and computes them using a similarity-based search.
        If multiple high-scoring candidates are found, the one with the highest effective score is chosen.
        """
        if model_and_refinement_steps is None:
            model = None
            num_refinement_steps = 1
        else:
            model, num_refinement_steps = model_and_refinement_steps
        for _ in range(num_refinement_steps):
            result = cls.from_indexless_citation_with_refinement(indexless_citation=indexless_citation, document=document, subject=subject, chatterer=model)
            if result is None:
                continue
            return result

    @staticmethod
    def find_best_match_index(snippet: str, document: str, target_index: int) -> Optional[int]:
        """
        Extracts a candidate window centered around the specified target_index,
        with a size equal to the length of the snippet. Within this region,
        it calculates the similarity with the snippet using a sliding window approach.

        The index of the candidate with the highest effective_score is returned.
        If no suitable candidate is found, the target_index is returned.

        Note: If multiple high-scoring candidates are found, the one with the highest effective score is chosen.
        """
        snippet = snippet.strip()
        if not snippet:
            return
        snippet_len: int = len(snippet)
        best_index: int = -1
        best_effective_score = 0.0
        max_radius = max(target_index, len(document) - target_index)
        for offset in range(max_radius):
            for candidate_index in (target_index - offset, target_index + offset):
                if candidate_index < 0 or candidate_index + snippet_len > len(document):
                    continue
                candidate_segment = document[candidate_index:min(candidate_index + snippet_len, len(document))]
                if len(candidate_segment) < snippet_len:
                    continue
                local_best_similarity = 0.0
                local_best_offset = 0
                for i in range(0, len(candidate_segment) - snippet_len + 1):
                    candidate_window = candidate_segment[i:i + snippet_len]
                    similarity = difflib.SequenceMatcher(None, snippet, candidate_window).ratio()
                    if similarity > local_best_similarity:
                        local_best_similarity = similarity
                        local_best_offset = i
                candidate_final_index = candidate_index + local_best_offset
                if candidate_final_index + snippet_len > len(document):
                    candidate_final_index = len(document) - snippet_len
                if local_best_similarity > best_effective_score:
                    best_effective_score = local_best_similarity
                    best_index = candidate_final_index
        if not 0 <= best_index < len(document):
            logger.warning(f"Snippet '{snippet}' not found with sufficient similarity.")
            return
        else:
            logger.debug(f"Found best match for snippet '{snippet}' at index {best_index} with effective score {best_effective_score:.2f}.")
            return best_index

    @classmethod
    def from_indexless_citation_with_refinement(cls, indexless_citation: SingleMatchCitation, document: str, subject: str, chatterer: Optional[Chatterer]) -> Optional[Self]:
        if chatterer is None:
            logger.error('No LLM provided for indexless citation refinement.')
            new_indexless_citation = indexless_citation
        else:
            new_indexless_citation = chatterer.generate_pydantic(response_model=SingleMatchCitation, messages=[HumanMessage(content=f"I tried to find the `SNIPPET` in the `original-raw-document` to extract a text citation for the subject `subject-to-parse`, but I couldn't find it. Please provide `citation-start-from` and `citation-end-at` to help me locate the correct text span.\n---\n<original-raw-document>\n{document}\n</original-raw-document>\n---\n<subject-to-parse>\n{subject}\n</subject-to-parse>\n---\n<current-citation-start-from>\n{indexless_citation.start_from}\n</current-citation-start-from>\n---\n<current-citation-end-at>\n{indexless_citation.end_at}\n</current-citation-end-at>\n")])
        doc_len: int = len(document)
        start_snippet: str = new_indexless_citation.start_from.strip()
        if start_snippet:
            target_for_start = document.find(start_snippet)
            if target_for_start == -1:
                target_for_start = 0
            new_start: Optional[int] = cls.find_best_match_index(snippet=start_snippet, document=document, target_index=target_for_start)
            if new_start is None:
                return
        else:
            logger.warning('No start_text provided')
            return
        end_snippet: str = new_indexless_citation.end_at.strip()
        if end_snippet:
            target_for_end = document.find(end_snippet, new_start)
            if target_for_end == -1:
                target_for_end = new_start
            candidate_end: Optional[int] = cls.find_best_match_index(snippet=end_snippet, document=document, target_index=target_for_end)
            if candidate_end is None:
                return
            new_end: int = candidate_end + len(end_snippet)
        else:
            logger.warning('No end_text provided; defaulting end index to document length.')
            new_end = doc_len
        if not 0 <= new_start < new_end <= doc_len:
            logger.error(f'Adjusted citation indices invalid: start {new_start}, end {new_end}, doc_len {doc_len}.')
            return
        try:
            extracted_text = document[new_start:new_end]
        except IndexError as e:
            logger.error(f'Error extracting text using adjusted citation indices: {e}')
            return
        return cls(start=new_start, end=new_end, start_from=new_indexless_citation.start_from, end_at=new_indexless_citation.end_at, extracted_text=extracted_text)
```

```C:\Users\cosogi\chatterer\chatterer\tools\citation_chunking\citation_chunker.py
import logging
from typing import Callable, NamedTuple, Optional, Self
import colorama
from colorama import Fore
from ...language_model import Chatterer
from .chunks import CitationChunks
from .citations import Citations
from .prompt import generate_fewshot_affirmative_response, generate_human_assistant_fewshot_examples, generate_instruction
logger = logging.getLogger(__name__)
colorama.init()

class GlobalCoverage(NamedTuple):
    coverage: float
    matched_intervals: list[tuple[int, int]]

    @staticmethod
    def merge_intervals(intervals: list[tuple[int, int]]) -> list[tuple[int, int]]:
        if not intervals:
            return []
        sorted_intervals = sorted(intervals, key=lambda x: x[0])
        merged: list[tuple[int, int]] = [sorted_intervals[0]]
        for current in sorted_intervals[1:]:
            prev = merged[-1]
            if current[0] <= prev[1]:
                merged[-1] = (prev[0], max(prev[1], current[1]))
            else:
                merged.append(current)
        return merged

    @classmethod
    def from_verified_citations(cls, verified_chunks: list[Citations], document: str) -> Self:
        all_intervals: list[tuple[int, int]] = []
        for chunk in verified_chunks:
            for matches in chunk.references.values():
                for m in matches:
                    all_intervals.append((m.start_idx, m.end_idx))
        merged: list[tuple[int, int]] = cls.merge_intervals(all_intervals)
        doc_length: int = len(document)
        total_matched = sum((e - s for s, e in merged))
        coverage: float = total_matched / doc_length if doc_length > 0 else 0.0
        return cls(coverage=coverage, matched_intervals=merged)

def citation_chunker(document: str, chatterer: Chatterer, global_coverage_threshold: float=0.9, num_refinement_steps: int=3, fewshot_examples_generator: Optional[Callable[[], list[tuple[str, str]]]]=generate_human_assistant_fewshot_examples, instruction_generator: Optional[Callable[[], str]]=generate_instruction, fewshot_affirmative_response: Optional[Callable[[], str]]=generate_fewshot_affirmative_response, test_global_coverage: bool=False) -> list[Citations]:
    """
    1) Obtain CitationChunks via the LLM.
    2) Process each chunk to extract MatchedText using snippet-based index correction.
    3) Calculate overall document coverage and print results.
    """
    unverified_chunks: CitationChunks = CitationChunks.from_llm(chatterer=chatterer, document=document, fewshot_examples_generator=fewshot_examples_generator, instruction_generator=instruction_generator, fewshot_affirmative_response=fewshot_affirmative_response)
    verified_chunks: list[Citations] = []
    for chunk in unverified_chunks.citation_chunks:
        try:
            vc: Citations = Citations.from_unverified(unverified_chunk=chunk, document=document, model_and_refinement_steps=(chatterer, num_refinement_steps))
            verified_chunks.append(vc)
        except Exception as e:
            logger.error(f"Error processing chunk for subject '{chunk.subject}': {e}")
    if test_global_coverage:
        gc = GlobalCoverage.from_verified_citations(verified_chunks, document)
        logger.info(f'Global coverage: {gc.coverage * 100:.1f}%')
        if gc.coverage < global_coverage_threshold:
            logger.info(f'Global coverage {gc.coverage * 100:.1f}% is below the threshold {global_coverage_threshold * 100:.1f}%.')
        print('=== Final Global Coverage Check ===')
        print(f'Overall coverage: {gc.coverage * 100:.1f}% of the document.')
        if gc.matched_intervals:
            print('Merged matched intervals:')
            for interval in gc.matched_intervals:
                print(f' - {interval}')
        else:
            print('No matches found across all chunks.')
        print('\n=== Raw Semantic Chunking Result ===')
        for vc in verified_chunks:
            print(f'{Fore.LIGHTGREEN_EX}[SUBJECT] {Fore.GREEN}{vc.name}{Fore.RESET}')
            if vc.references:
                for source_key, matches in vc.references.items():
                    print(f'{Fore.LIGHTBLUE_EX}  [SOURCE] {Fore.BLUE}{source_key}{Fore.RESET}')
                    for mt in matches:
                        snippet = repr(mt.text)
                        print(f'    {Fore.LIGHTYELLOW_EX}[MATCH @ {mt.start_idx}~{mt.end_idx}] {Fore.YELLOW}{snippet}{Fore.RESET}')
            else:
                print(' - (No matches found even after refinement.)')
    return verified_chunks
```

```C:\Users\cosogi\chatterer\chatterer\tools\citation_chunking\prompt.py
"""
ragent/prompt/citation_chunking.py

This module defines prompt constants for citation chunking.
The LLM is expected to return JSON objects that include only the text snippets for the beginning and end of the citation span.
The character indices will be computed in a post‐processing step.
"""
from functools import cache

@cache
def generate_instruction() -> str:
    from .chunks import CitationChunk, CitationChunks
    from .reference import MultiMatchRegex, SingleMatchCitation
    return "You are an AI specialized in 'citation-based text chunking'.\nGiven a document, perform the following steps:\n1) Identify the major topics in the document.\n2) For each topic, provide a list of citation objects indicating the text snippets at the beginning and end of the relevant paragraph(s) for that topic.\n\nImportant:\n- Return citation objects with 'start_text' and 'end_text' fields to precisely capture the text span. Do NOT include character indices.\n- If a regular expression based matching is more appropriate for a topic (e.g. for multiple matches), you may include a regex object of type 'multi_match_regex'.\n\nReturn JSON strictly in the following format:\n{json_example}\n\n1) Return only valid JSON (no extra keys).\n2) Do NOT include any commentary.\n3) Ensure that the citations capture the entire relevant paragraph without overlap or omission.".format(json_example=CitationChunks(citation_chunks=[CitationChunk(subject='Quantum Advantage', references=[SingleMatchCitation(start_from='Starting snippet...', end_at='... Ending snippet'), MultiMatchRegex(type='multi_match_regex', regular_expression='Some.*?regex.*?pattern')])]).model_dump_json(indent=2))

@cache
def generate_human_assistant_fewshot_examples() -> list[tuple[str, str]]:
    from .chunks import CitationChunk, CitationChunks
    from .reference import SingleMatchCitation
    return [('Agent-Semantic Chunking of the following text:\n\nTitle: Revolutionary Breakthrough in Quantum Computing\n\nIn a landmark development, researchers at the National Quantum Laboratory unveiled a quantum computer that demonstrates clear quantum advantage by performing computations that are infeasible on classical systems.\n\nThe breakthrough is the result of years of rigorous research and international collaboration. The system leverages entanglement and superposition to process complex algorithms at unprecedented speeds.\n\nHowever, practical applications are still emerging, and experts caution about scalability challenges. Meanwhile, several tech giants are expressing keen interest in integrating quantum technology into future products.\n\nPlease classify the major topics and return the exact text snippets (for the start and end of the relevant paragraphs) for each topic.', CitationChunks(citation_chunks=[CitationChunk(subject='Quantum Advantage', references=[SingleMatchCitation(start_from='In a landmark development', end_at='on classical systems.')]), CitationChunk(subject='Research Collaboration', references=[SingleMatchCitation(start_from='The breakthrough is the result', end_at='unprecedented speeds.')]), CitationChunk(subject='Practical Challenges', references=[SingleMatchCitation(start_from='However, practical applications', end_at='scalability challenges.')]), CitationChunk(subject='Industry Interest', references=[SingleMatchCitation(start_from='Meanwhile, several tech giants', end_at='future products.')])]).model_dump_json(indent=2)), ('Agent-Semantic Chunking of the following text:\n\nTitle: Rising Seas and Coastal Erosion: A Global Crisis\n\nCommunities worldwide face the impacts of climate change as rising sea levels lead to accelerated coastal erosion, jeopardizing homes and critical infrastructure.\n\nIn a small coastal town, residents noted that "the encroaching sea" has already begun to claim beachfront properties, prompting local authorities to implement emergency measures.\n\nEnvironmental experts warn that without significant intervention, the frequency and severity of these events will increase, further exacerbating the global climate crisis.\n\nPlease classify the major topics and return the exact text snippets (for the start and end of the relevant paragraphs) for each topic.', CitationChunks(citation_chunks=[CitationChunk(subject='Coastal Erosion Impact', references=[SingleMatchCitation(start_from='Communities worldwide face the impacts', end_at='critical infrastructure.')]), CitationChunk(subject='Local Emergency Response', references=[SingleMatchCitation(start_from='In a small coastal town', end_at='emergency measures.')]), CitationChunk(subject='Expert Warning', references=[SingleMatchCitation(start_from='Environmental experts warn', end_at='global climate crisis.')])]).model_dump_json(indent=2))]

def generate_fewshot_affirmative_response() -> str:
    return 'Great! I will now perform the citation-based chunking. Please provide the document to process!'
```

```C:\Users\cosogi\chatterer\chatterer\tools\citation_chunking\reference.py
from typing import Literal, TypeAlias
from pydantic import BaseModel, Field

class MultiMatchRegex(BaseModel):
    type: Literal['multi_match_regex'] = Field(description='A regex pattern that should match multiple instances of the subject in the document.')
    regular_expression: str = Field(description='The regex pattern that should match multiple instances of the subject in the document.')

    def __hash__(self) -> int:
        return hash((self.type, self.regular_expression))

class SingleMatchCitation(BaseModel):
    start_from: str = Field(description='A snippet of text at the beginning of the cited section.')
    end_at: str = Field(description='A snippet of text at the end of the cited section.')

    def __hash__(self) -> int:
        return hash((self.start_from, self.end_at))
Reference: TypeAlias = SingleMatchCitation | MultiMatchRegex
```

```C:\Users\cosogi\chatterer\chatterer\tools\citation_chunking\utils.py
from typing import Callable, NamedTuple, Self, TypeVar
from pydantic import BaseModel
T = TypeVar('T', bound=BaseModel)

class MatchedText(NamedTuple):
    text: str
    start_idx: int
    end_idx: int

    @classmethod
    def from_text(cls, full_text: str, len_func: Callable[[str], int], chunk_size: int=2048, token_overlap: int=0, separator: str='\n') -> list[Self]:
        """
        토큰 수 제한과 선택적 오버랩을 기준으로 텍스트를 청크로 분할합니다.
        각 청크는 원본 텍스트 내의 위치 정보 (start_idx, end_idx)와 함께 반환됩니다.
        텍스트는 separator 문자열로 분할하며, 토큰 수는 len_func 함수를 통해 계산합니다.

        Args:
            full_text: 분할할 전체 텍스트.
            len_func: 주어진 텍스트의 토큰 수를 반환하는 함수.
            chunk_size: 각 청크의 최대 토큰 수. 기본값은 2048.
            token_overlap: 청크 간 중첩할 토큰 수. 기본값은 0.
            separator: 텍스트를 분할할 구분자 문자열. 기본값은 "
".

        Returns:
            각 요소가 (chunk_text, start_idx, end_idx)인 튜플의 리스트.
            chunk_text는 whole_text 내에서 whole_text[start_idx:end_idx]와 동일한 부분 문자열입니다.
        """
        text_chunks: list[Self] = []
        sep_token_count: int = len_func(separator)
        sep_len = len(separator)
        piece_infos: list[Self] = []
        start_idx = 0
        while True:
            idx = full_text.find(separator, start_idx)
            if idx == -1:
                piece_infos.append(cls(text=full_text[start_idx:], start_idx=start_idx, end_idx=len(full_text)))
                break
            else:
                piece_infos.append(cls(text=full_text[start_idx:idx], start_idx=start_idx, end_idx=idx))
                start_idx = idx + sep_len
        current_chunk: list[Self] = []
        current_token_count: int = 0
        i = 0
        while i < len(piece_infos):
            piece_info = piece_infos[i]
            piece = piece_info.text
            piece_start = piece_info.start_idx
            piece_end = piece_info.end_idx
            piece_token_count: int = len_func(piece) + sep_token_count
            if current_token_count + piece_token_count > chunk_size:
                if not current_chunk:
                    current_chunk.append(cls(text=piece, start_idx=piece_start, end_idx=piece_end))
                    current_token_count += piece_token_count
                    i += 1
                chunk_start = current_chunk[0].start_idx
                chunk_end = current_chunk[-1].end_idx
                chunk_text = full_text[chunk_start:chunk_end]
                text_chunks.append(cls(text=chunk_text, start_idx=chunk_start, end_idx=chunk_end))
                if token_overlap > 0:
                    overlap_chunk: list[Self] = []
                    overlap_count: int = 0
                    for j in range(len(current_chunk) - 1, -1, -1):
                        p_text = current_chunk[j].text
                        p_token_count = len_func(p_text) + sep_token_count
                        if overlap_count + p_token_count <= token_overlap or not overlap_chunk:
                            overlap_chunk.insert(0, current_chunk[j])
                            overlap_count += p_token_count
                        else:
                            break
                    current_chunk = overlap_chunk.copy()
                    current_token_count = overlap_count
                else:
                    current_chunk.clear()
                    current_token_count = 0
            else:
                current_chunk.append(cls(text=piece, start_idx=piece_start, end_idx=piece_end))
                current_token_count += piece_token_count
                i += 1
        if current_chunk:
            chunk_start = current_chunk[0].start_idx
            chunk_end = current_chunk[-1].end_idx
            chunk_text = full_text[chunk_start:chunk_end]
            text_chunks.append(cls(text=chunk_text, start_idx=chunk_start, end_idx=chunk_end))
        return text_chunks
```

```C:\Users\cosogi\chatterer\chatterer\tools\citation_chunking\__init__.py
from .citation_chunker import citation_chunker
__all__ = ['citation_chunker']
```

